chunker:
  section_labeling: |
    You are given a list of section titles from a single research paper. Your task is to categorize each title into one of the following categories:

    `["Abstract", "Introduction", "Problem Definition", "Methodology", "Related Work", "Experiment", "Conclusion", "References", "Acknowledgement", "Appendix"]`

    **Instructions:**

    * Assign each title to exactly one category.
    * If a title does not clearly fit any category, assign it to the most likely one based on typical academic paper structure.
    * Consecutive sections in the input list are highly likely to belong to the same category ‚Äî use this as an additional cue when making assignments.
    * Multiple titles may belong to the same category, but no title should appear in more than one category.
    * Output only the raw JSON object without any explanation, commentary, or extra formatting. Do not wrap the output in triple backticks or add a language label.

    **Output format:**

    ```
    {
      "Abstract": [titles assigned to Abstract],
      "Introduction": [titles assigned to Introduction],
      "Problem Definition": [titles assigned to Problem Definition],
      "Methodology": [titles assigned to Methodology],
      "Related Work": [titles assigned to Related Work],
      "Experiment": [titles assigned to Experiment],
      "Conclusion": [titles assigned to Conclusion],
      "References": [titles assigned to References],
      "Acknowledgement": [titles assigned to Acknowledgement],
      "Appendix": [titles assigned to Appendix]
    }
    ```

    **Input:**
    [Title List]

    **Output:**

reference_taxonomy_builder: 
  method_taxonomy:
    [ROLE]
    You are an expert research taxonomist. Your task is to generate a **hierarchical taxonomy** of methodologies for a given topic. Each node should include a **name**, a **short description**, and a balanced view of **pros and cons**.

    [TASK]
    * Given a topic, generate a **hierarchical taxonomy of methodologies** in JSON format.
    * Organize methodologies into **broad categories at the top level**, with **subcategories and specific approaches** as children.
    * Each node must contain:
      - "name": the category/method name.
      - "description": a concise explanation of that category/method.
      - "pros": a short list of key advantages.
      - "cons": a short list of key limitations.
    * Ensure the taxonomy covers both **established approaches** and **emerging ones** where applicable.
    * Output must be **valid JSON only** (no commentary, no markdown).  
    * The JSON must represent a **taxonomy of methodologies**, not of tasks, datasets, or other concepts.

    [OUTPUT FORMAT]
    {
      "name": "{{topic}} Methodologies",
      "description": "A taxonomy of methodologies applied to {{topic}}, organized hierarchically from broad categories to specific techniques.",
      "pros": ["Provides structured overview", "Captures established and emerging approaches"],
      "cons": ["May oversimplify relationships", "Requires refinement to ensure accuracy"],
      "children": [
        {
          "name": "Category 1",
          "description": "Brief explanation of what Category 1 covers in the context of {{topic}}.",
          "pros": ["Widely applied", "Broadly adaptable"],
          "cons": ["May be rigid", "Limited scalability"],
          "children": [
            {
              "name": "Subcategory A",
              "description": "Explanation of Subcategory A.",
              "pros": ["Focused scope", "Strong theoretical basis"],
              "cons": ["May lack flexibility", "Dependent on assumptions"],
              "children": [
                {
                  "name": "Approach X",
                  "description": "Explanation of Approach X.",
                  "pros": ["Efficient in small-scale cases", "Interpretability"],
                  "cons": ["Struggles with large-scale problems", "Limited generalization"]
                },
                {
                  "name": "Approach Y",
                  "description": "Explanation of Approach Y.",
                  "pros": ["Handles diverse inputs", "Robust outcomes"],
                  "cons": ["Complex implementation", "Resource-intensive"]
                }
              ]
            }
          ]
        },
        {
          "name": "Category 2",
          "description": "Brief explanation of what Category 2 covers.",
          "pros": ["Flexible approaches", "High adaptability"],
          "cons": ["Less validated", "May lack benchmarks"],
          "children": [
            {
              "name": "Subcategory B",
              "description": "Explanation of Subcategory B.",
              "pros": ["Innovative techniques", "Promising direction"],
              "cons": ["Unstable performance", "Limited interpretability"]
            }
          ]
        }
      ]
    }

    [INSTRUCTIONS]
    1. Each "description" should be 2‚Äì3 sentences, concise but informative.
    2. Each "pros" and "cons" list should have 2‚Äì3 items, short and precise.
    3. Maintain 3‚Äì4 hierarchical levels if meaningful.
    4. Use clear and academically standard terminology.
    5. Return only JSON (no extra text, no markdown).
    6. Ensure the JSON explicitly represents a **taxonomy of methodologies**.

    [INPUT PLACEHOLDER]
    Below is the input topic:
    {{topic}}
  
  task_taxonomy:
    [ROLE]
    You are an expert research taxonomist. Your task is to generate a **hierarchical taxonomy of tasks** for a given topic. Each node must include a **name**, a **short description**, and detailed **input** and **output** specifications.

    [TASK]
    * Generate a **hierarchical taxonomy of tasks** in JSON format for the provided topic.
    * Organize tasks into **broad categories at the top level**, with **subcategories and specific tasks** as children.
    * Each node must contain:
      - "name": the category/subcategory/task name.
      - "description": a concise 2‚Äì3 sentence explanation of that node.
      - "input": an object describing expected data **formats** and additional **properties**/**constraints**.
      - "output": an object describing produced data **formats** and additional **properties**/**constraints**.
    * Use general, domain‚Äëneutral terminology. Avoid tool/library/dataset names.
    * Output must be **valid JSON only** (no commentary, no markdown).
    * The JSON must represent a **taxonomy of tasks** (not methodologies, datasets, metrics, or systems).

    [INPUT/OUTPUT FIELD CONVENTIONS]
    * Each "input" and "output" object MUST include:
      - "data_formats": an array of **generic labels** only (no domain qualifiers, no parentheses), chosen from a neutral vocabulary such as:
        ["text", "structured table", "key‚Äìvalue JSON", "graphs", "images", "audio", "video", "time series", "spatial data", "multimodal bundle"].
      - **One or both** of the following arrays (at least one must be non‚Äëempty):
        - "properties": concise **descriptive qualities** of the data (e.g., "consistent encoding", "schema present", "low noise", "traceable lineage").
        - "constraints": concise **requirements** the data must meet (e.g., "deduplicated", "balanced classes", "time‚Äësynchronized", "privacy‚Äëcompliant").
    * If both "properties" and "constraints" are present, **do not duplicate items** across them:
      - Put prescriptive/mandatory items under "constraints".
      - Put descriptive/preferable qualities under "properties".
    * Non‚Äëleaf nodes: summarize **typical** inputs/outputs by selecting the most frequent 3‚Äì6 formats/items across descendants (deduplicated; ordered by frequency). Do not leave these arrays empty.
    * Leaf nodes: specify **task‚Äëspecific** formats (1‚Äì3) and items (3‚Äì6).
    * Keep wording general and reusable across topics.

    [CONSTRUCTION RULES]
    * Root Node:
      - "name": "{{topic}} Task Taxonomy".
      - "description": 2‚Äì3 sentences summarizing the task space for the topic.
      - Include "input" and "output" summarizing common formats and items across the entire tree (per the conventions above).
    * Hierarchy:
      - Create 3‚Äì4 levels when meaningful. Children must be strict semantic subsets of their parent.
      - Use concise, non‚Äëoverlapping labels for categories and subcategories.
      - Sort each node‚Äôs "children" **alphabetically by "name"**.
    * Completeness & Consistency:
      - Every node includes: "name", "description", "input", "output", and "children" (leaves use `"children": []`).
      - Deduplicate entries within each "data_formats", "properties", and "constraints" array.
      - The taxonomy must remain **task‚Äëfocused**.

    [OUTPUT REQUIREMENTS]
    * Output must be **valid JSON only** (no commentary, no markdown).
    * Ensure all strings are plain text.

    [OUTPUT FORMAT]  (example includes both "properties" and "constraints")
    {
      "name": "{{topic}} Task Taxonomy",
      "description": "A hierarchical taxonomy of tasks related to {{topic}}, organized from broad categories to specific tasks. Each node specifies inputs and outputs with formats plus descriptive properties and requirements.",
      "input": {
        "data_formats": ["text", "structured table"],
        "properties": ["consistent encoding", "schema present"],
        "constraints": ["deduplicated"]
      },
      "output": {
        "data_formats": ["structured table", "key‚Äìvalue JSON"],
        "properties": ["machine‚Äëreadable", "traceable lineage"],
        "constraints": ["evaluation‚Äëready"]
      },
      "children": [
        {
          "name": "Category A",
          "description": "What this category of tasks covers and how it is distinguished at a high level within {{topic}}.",
          "input": {
            "data_formats": ["text", "structured table"],
            "properties": ["standardized units", "low noise"],
            "constraints": ["minimal missingness"]
          },
          "output": {
            "data_formats": ["structured table", "key‚Äìvalue JSON"],
            "properties": ["schema‚Äëaligned", "reproducible derivations"],
            "constraints": ["alignment with input IDs"]
          },
          "children": [
            {
              "name": "Subcategory A1",
              "description": "How these tasks differ from others in Category A and typical workflow characteristics.",
              "input": {
                "data_formats": ["structured table"],
                "properties": ["clearly typed fields"],
                "constraints": ["representative sampling"]
              },
              "output": {
                "data_formats": ["structured table"],
                "properties": ["validated fields"],
                "constraints": ["deterministic schema"]
              },
              "children": [
                {
                  "name": "Task X",
                  "description": "Concise summary of the task objective, expected operations, and scope boundaries.",
                  "input": {
                    "data_formats": ["structured table"],
                    "properties": ["normalized units"],
                    "constraints": ["unique identifiers present", "no duplicates"]
                  },
                  "output": {
                    "data_formats": ["structured table", "key‚Äìvalue JSON"],
                    "properties": ["well‚Äëdefined schema"],
                    "constraints": ["confidence/score where applicable", "consistent ordering rules"]
                  },
                  "children": []
                },
                {
                  "name": "Task Y",
                  "description": "Concise summary of the task objective, expected operations, and scope boundaries.",
                  "input": {
                    "data_formats": ["text"],
                    "properties": ["language specified", "clean tokenization"],
                    "constraints": ["sufficient context length"]
                  },
                  "output": {
                    "data_formats": ["text", "key‚Äìvalue JSON"],
                    "properties": ["faithful to source", "concise"],
                    "constraints": ["machine‚Äëparseable when structured"]
                  },
                  "children": []
                }
              ]
            }
          ]
        },
        {
          "name": "Category B",
          "description": "What this category of tasks covers and how it complements Category A.",
          "input": {
            "data_formats": ["multimodal bundle", "time series"],
            "properties": ["clear metadata"],
            "constraints": ["synchronized modalities", "time‚Äëaligned sequences"]
          },
          "output": {
            "data_formats": ["structured table", "graphs"],
            "properties": ["traceable to inputs"],
            "constraints": ["explicit scoring criteria", "evaluation‚Äëready"]
          },
          "children": [
            {
              "name": "Subcategory B1",
              "description": "Characteristic task patterns within this subcategory.",
              "input": {
                "data_formats": ["graphs", "structured table"],
                "properties": ["unique identifiers"],
                "constraints": ["well‚Äëdefined nodes/edges", "consistent relation semantics"]
              },
              "output": {
                "data_formats": ["graphs", "key‚Äìvalue JSON"],
                "properties": ["documented assumptions"],
                "constraints": ["validated connectivity", "no orphan nodes"]
              },
              "children": []
            }
          ]
        }
      ]
    }

    [INPUT PLACEHOLDER]
    Below is the input topic:
    {{topic}}

taxonomy_generator:
  task_taxonomy:
    extract_problem_definition: |
      You are acting as a **research assistant** tasked with analyzing a research paper to extract its **core research problem** in a **structured**, **methodology-agnostic** format.

      Your objective is to **identify or infer the problem definition** and decompose it into precise components that describe **what the problem is**, **without referencing how it is solved**. Do not include any mention of algorithms, models, frameworks, training strategies, or implementation details.

      If the paper uses shorthand (e.g., acronyms, variables, or mathematical notation), refer to the preliminaries or notation sections to rewrite the problem in a **clear and unambiguous** manner.

      ---

      ### ‚úÖ Output Format

      ```yaml
      paper_id: "<Paper ID>"
      paper_title: "<Title of the paper>"

      problem_formulation:
        simple_description: >
          "<One-sentence summary of the problem, or `None` if not found.>"

        formal_definition:
          input: >
            "<Describe the input, including its type, structure, and properties. Mention hardware constraints only if they are explicitly part of the problem setting‚Äîdo NOT infer hardware requirements from experimental setups.>"
          output: >
            "<Describe the desired output, including type, structure, and expected characteristics.>"
      ```

      ---

      ### üß≠ Step-by-Step Instructions

      #### üîπ Step 1: Generate `simple_description`

      Create a concise, self-contained summary of the problem using this template:

      > **"Given <input>, the goal is to obtain <output> by achieving <objective>."**

      Follow this order when searching for the problem statement:

      1. Dedicated **Problem Definition** section
      2. **Introduction**
      3. **Abstract**
      4. Relevant parts of the **Methodology** *(only for high-level task description‚Äînot implementation details)*
      5. Any other section as necessary

      > If no coherent one-sentence summary can be extracted, return:

      ```yaml
      simple_description: None
      ```

      > ‚úÖ When a paper describes multiple related problems, prioritize the **primary** or **most general** problem formulation unless others are clearly independent.

      ---

      #### üîπ Step 2: Generate `formal_definition`

      Decompose the problem into the following structured components:

      | Field      | What to Describe                                                                                                                                                                                                                                                                         |
      | ---------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | **input**  | The data or contextual information provided to the system. Include type, format, and any explicitly stated constraints (e.g., memory, disk). **Do not treat the hardware used in experimental settings as input constraints unless they are explicitly part of the problem definition.** |
      | **output** | The expected result, product, or prediction. Include type and structure. Do not include how it is computed.                                                                                                                                                                              |

      > Leave any field blank (`""`) if it cannot be inferred from the paper.

      ---

      ### üö´ Do Not Include:

      * Specific methods, algorithms, architectures, or procedural details
      * Training objectives, loss functions, or optimization strategies
      * System or model names
      * Hardware specifications from experiments (e.g., GPUs, clusters), **unless explicitly part of the problem setting**

      ---

      ### ‚ö†Ô∏è Common Pitfalls to Avoid

      | Case                                               | Guideline                                                                                                                                                  |
      | -------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | **Hardware in experiments**                        | Never assume experimental hardware is part of the problem input unless **explicitly described** (e.g., ‚Äúthe task must run on edge devices with ‚â§2GB RAM‚Äù). |
      | **Vague or entangled problem-method descriptions** | Isolate the **task definition** only. If it's mixed with solution details, extract only the input‚Äìoutput‚Äìgoal relationship.                                |
      | **Multiple sub-problems**                          | Focus on the **primary problem** or provide a clearly separated list if multiple are independently defined.                                                |
      | **No clear problem statement**                     | Use context from preliminaries, task descriptions, or dataset definitions to infer a possible formulation. If still ambiguous, return `"None"` as needed.  |

      Below is the paper information:
      Paper ID: [Paper ID]
      Paper Title: [Paper Title]
      Paper Content:[Paper Content]

    aspect_classification: |
      You are given a list of problem definitions extracted from a collection of computer science research papers. Each definition includes structured aspects such as:

      * **Input**
      * **Output**

      ---

      ## ‚úÖ Task: Aspect-Wise Classification Process

      Focus on the following two aspects:

      * **Input**
      * **Output**

      For **each aspect**, process its corresponding content independently and repeat the following steps:

      ---

      ### üî• Step 1: Extract Key Entities

      Iterate over each paper. From the content of the current aspect:

      * Identify the main research-related entities (key concepts) that **describe the authors‚Äô formulation of this aspect** and are **explicitly emphasized in `problem_formulation` as central to the problem definition**.
      * If `problem_formulation` lacks sufficient information, refer to the corresponding structured field (**input**, **output**) for clarification.

      Focus on extracting noun phrases that match the aspect‚Äôs definition:

      * **Input**: Describe the input, including its type, structure, and properties. Mention hardware constraints only if they are explicitly part of the problem setting‚Äîdo NOT infer hardware requirements from experimental setups.
      * **Output**: Describe the desired output, including type, structure, and expected characteristics.

      ---

      ### üî• Step 2: Group Entities Within Sublists

      Group entities within the same paper and aspect into **local entity groups**. These groups reflect strong correlations between entities (they appear together in the same problem definition). Build connections among entities in each group to capture their co-occurrence relationships.

      ---

      ### üî• Step 3: Align and Merge Entities Across Sublists

      * **Identify Related Entities:** Merge entities across papers if they are exact matches, semantically equivalent, or containment relationships.
      * **Merge into Unified Nodes:** Example: {"user-item matrix", "rating matrix"} ‚Üí `Interaction Matrix`.
      * **Build Global Entity Groups:** Connected components of unified nodes form candidate classes.

      ---

      ### üî• Step 4: Refine Classes and Classify Papers

      1. **Provide Class Explanations:** For each resulting class, provide a short description of its scope and relevance.
      2. **Drop or Merge Small Classes:** Merge or drop classes with <2 papers.
      3. **Align Class Names:** Use formal research terminology.
      4. **Assign Papers:** Assign each paper to one or more classes per aspect. Use the paper ID to denote the paper.

      ---

      ### üî• Step 5: Identify Unassigned or Misaligned Papers

      * Detect **unassigned papers** (entities extracted but no class assignment).
      * Detect **misalignments** (entities poorly matched to class descriptions).
      * Suggest corrections (reassignment or new class).

      ---

      ### üî• Step 6: Implement Corrections

      * For each **unassigned or misaligned paper** identified in Step 5, update the classification by:

        * Assigning it to the most appropriate existing class if suggested.
        * Or creating a new class if no suitable class exists.
      * Ensure that the **final classification output incorporates these corrections directly**, so that all papers have appropriate assignments.

      ---

      ## üìã Output Format

      Return the results **strictly in valid JSON format** with the following structure:

      ```json
      {
        "input_classification": {
          "classes": [
            {
              "class_name": "User-Item Matrix",
              "class_description": "Matrix representation of user-item interactions, commonly used in recommendation tasks.",
              "papers": ["Paper A", "Paper C"]
            },
            {
              "class_name": "Knowledge Graph",
              "class_description": "Graph-based structured input capturing relational facts.",
              "papers": ["Paper B", "Paper F"]
            },
            {
              "class_name": "Temporal Sequence Data",
              "class_description": "Sequential data used for modeling time-dependent behaviors.",
              "papers": ["Paper G"]   // Correction applied here
            }
          ]
        },
        "output_classification": {
          "classes": [
            {
              "class_name": "Hash Codes",
              "class_description": "Compact binary representations of input data used for efficient retrieval.",
              "papers": ["Paper A", "Paper D"]
            },
            {
              "class_name": "Ranked List",
              "class_description": "Ordered set of relevant results returned for a given query.",
              "papers": ["Paper B", "Paper E"]
            }
          ]
        }
      }
      ```
      Below is the problem definition list:
      [Problem Definitions]

    taxonomy_generation: |
      You are an expert research taxonomist. You are given one **JSON object**, containing the classification result of the problem aspects:

      * **Input**: The nature or structure of data the research problem takes as input
      * **Output**: The expected result, format, or structure produced by the method.

      ---

      ### üì• Input Format

      Each JSON object contains an array of class entries. Each entry has the following fields:

      ```json
      {
        "class_name": "string",
        "class_description": "string",
        "supporting_papers": ["PaperA", "PaperB", ...]
      }
      ```

      Below is the classification result:
      [Classification Result]

      ---

      ## ‚úÖ Goal

      Construct a **single hierarchical taxonomy of meaningful research tasks**, where each task is defined by a combination of **Input** and **Output** classes.

      Each node in the taxonomy must represent a **specific, expressive, and well-scoped research task**, not just a data structure or format.

      ---

      ### üî• Step 1: Normalize and Refine Class Labels

      1. **Normalize Class Names:** Use standard academic terminology where possible. Resolve informal, ambiguous, or overly generic names using their class descriptions and example papers.
      2. **Merge Equivalent or Contained Classes:** If two classes are synonyms or exhibit strict containment (e.g., ‚Äúuser-item matrix‚Äù ‚äÜ ‚Äúinteraction matrix‚Äù), merge them.
      3. **Drop or Collapse Weak Classes:** Drop classes with too few or overly vague supporting papers unless they can be merged into broader categories.

      **Output:** Refined JSON list of input and output classes.

      ---

      ### üî• Step 2: Form Meaningful Task Nodes by Input‚ÄìOutput Pairing

      1. **Pair Input and Output Classes:** For every (Input, Output) pair, check if it defines a **coherent, meaningful research task**.
      2. **Keep only valid task nodes** that meet all three criteria:

        * The pair appears in at least **2 different papers**
        * The pairing reflects a **common and recognizable research objective**
        * The task name can be made **expressive** (not just ‚ÄúMatrix ‚Üí Vector‚Äù).
      3. **Name Each Task:**
        Use the format:

        ```
        TASK:L{level}:{CANONICAL_NAME}
        ```

      ---

      ### üî• Step 3: Build a Hierarchy over the Tasks

      1. **Group Similar Tasks:**

        * Parent-child relationships based on task granularity and generalization.
        * Example:

          * Parent: `KNOWLEDGE_GRAPH_TASKS`

            * Child: `KNOWLEDGE_GRAPH_EMBEDDING`
            * Child: `KNOWLEDGE_GRAPH_COMPLETION`

      2. **Hierarchy Rules:**

        * Use only **IS-A** relationships.
        * Construct a **tree** (no DAGs or cycles).
        * Root node: `ROOT`.

      ---

      ## üìã Output Format (JSON Only)

      Return the taxonomy strictly as **valid JSON**. Each node must have:

      * `task_id`: Unique ID in the format `TASK:L{level}:{CANONICAL_NAME}`
      * `task_name`: Expressive descriptive name
      * `input_class`: Input class name
      * `output_class`: Output class name
      * `explanation`: 1‚Äì2 sentence explanation of the task
      * `papers`: List of supporting papers
      * `children`: Array of child task nodes

      Example:

      ```json
      {
        "taxonomy": {
          "task_id": "ROOT",
          "task_name": "ROOT",
          "children": [
            {
              "task_id": "TASK:L1:KNOWLEDGE_GRAPH_COMPLETION",
              "task_name": "Knowledge Graph Completion",
              "input_class": "Knowledge Graph",
              "output_class": "Completed Triples",
              "explanation": "Predict missing edges in a relational graph based on observed triples.",
              "papers": ["PaperA", "PaperD"],
              "children": []
            },
            {
              "task_id": "TASK:L1:HASH_CODE_GENERATION_FOR_RETRIEVAL",
              "task_name": "Hash Code Generation for Retrieval",
              "input_class": "User-Item Interaction Matrix",
              "output_class": "Hash Codes",
              "explanation": "Generate compact binary representations for fast retrieval in recommendation settings.",
              "papers": ["PaperB", "PaperF"],
              "children": []
            }
          ]
        }
      }
      ```

      * Ensure the JSON is **syntactically valid**.
      * Do not output Markdown, tables, or text outside the JSON block.

      ---
  method_taxonomy:
    extract_method_summary: |
      [ROLE]
      You are an AI research methodology analyst. Your job is to extract a precise, structured summary of a paper‚Äôs method from the method section text of the paper. You may parse free text and simple Markdown tables. You must be faithful to the source.

      [PRINCIPLES]
      - Grounded only in the method section text of the paper. No speculation or outside knowledge.
      - If a required value is missing, use [] or null (never "N/A").
      - Keep names consistent, concise, and unambiguous.
      - Prefer clarity over verbosity; avoid redundancy.

      [INPUT]
      The method section text of the paper may include method descriptions, equations, citations, figure/table mentions, and Markdown tables.

      [GOAL]
      Return ONE valid JSON object (no prose, no markdown) with five top-level keys:
      - "description"
      - "core_components"
      - "workflow_sequence"
      - "objectives"
      - "technical_keywords"

      [DEFINITIONS]
      - Description: a concise summary of the method as presented in the text. Capture the central idea, not evaluation results.
      - Component: a stable building block of the method (e.g., a module, optimizer, evaluator, sampler). NOT a step.
      - Step: an ordered action in the overall procedure (e.g., initialize ‚Üí transform ‚Üí aggregate ‚Üí evaluate).
      - Objective: an intended outcome (e.g., accuracy, efficiency, robustness, interpretability).
      - Technical keyword: a compact technique/algorithm/training scheme/framework term (unigram/bigram), not generic words like ‚Äúmodel‚Äù or ‚Äúdata‚Äù.

      [WHAT TO INCLUDE / EXCLUDE]
      - Include only method-related content from the method section text of the paper.
      - Include md-table-derived insights if they inform design choices (ablations, parameters, variants). Ignore purely results tables unless they justify a method choice.
      - Do not include datasets, benchmarks, or results unless explicitly used to justify a component or step.
      - Remove citations, equation numbers, and footnotes; keep symbols as plain text if needed.

      [NORMALIZATION RULES]
      - Component names: Title Case (e.g., "Preprocessing Module", "Scoring Function"). Be consistent.
      - Step references: write exactly "Step X" (e.g., "Step 2") when referenced in `supported_by`.
      - `technical_keywords`: lowercase; max 10; deduplicate; avoid stopwords/generic terms; use unigrams/bigrams only.
      - `inputs` and `outputs`: lists of short noun phrases, no sentences.

      [DISAMBIGUATION & DEDUP]
      - If multiple names describe the same component, choose one canonical name (prefer the paper‚Äôs primary term) and use it everywhere.
      - Merge duplicate components; aggregate their roles in a single `function` if needed.
      - If the sequence is implied but not explicit, infer the most plausible order; mark concurrency with ‚Äúin parallel‚Äù or iteration with ‚Äúrepeat until convergence‚Äù.

      [MARKDOWN TABLE GUIDANCE]
      - If a Markdown table clarifies the method (e.g., variants, hyperparameters, component comparisons), summarize ONE key takeaway in `insights` for the relevant step.
      - When possible, include the table label text (e.g., "Table 2") as part of `insights`. If no label is available, omit it.
      - Do not copy entire tables; extract only what explains the method.

      [FIELD-BY-FIELD SPECIFICATION]
      1) description (string)
      - Provide a concise summary of the overall method.  
      - Capture the main design idea and what distinguishes it.  
      - Avoid discussion of results, datasets, or evaluation metrics.

      2) core_components (array of objects)
      - "component": short canonical name (Title Case).
      - "function": one-sentence purpose/role.
      - "inputs": array<string> of key inputs (short phrases).
      - "outputs": array<string> of key outputs (short phrases).

      3) workflow_sequence (array of objects)
      - "step_number": integer starting at 1, strictly increasing.
      - "action": concise description of what is done.
      - "components_used": array<string> whose entries EXACTLY match names in "core_components".
      - "insights": one short sentence summarizing method-relevant evidence from figures/tables OR null.

      4) objectives (array of objects)
      - "goal": specific aim of the method.
      - "supported_by": array<string> referencing component names and/or "Step X".

      5) technical_keywords (array<string>)
      - Up to 10 lowercased, domain-neutral method terms; no duplicates; avoid dataset names or generic words.

      [VALIDATION CHECKLIST]
      - All five top-level keys exist: "description", "core_components", "workflow_sequence", "objectives", "technical_keywords".
      - Every name in "components_used" appears in "core_components".
      - All strings use double quotes; no trailing commas; valid JSON.
      - No extra keys beyond the schema.
      - Use [] for empty lists, null where applicable.

      [OUTPUT ONLY]
      Return ONLY the JSON object. No explanations, no markdown, no schema.

      [OUTPUT FORMAT EXAMPLE (ILLUSTRATIVE‚ÄîDO NOT COPY VERBATIM)]
      {
      "paper_id": <paper_id>,
      "paper_title": <paper_title>,
      "description": "This method introduces a modular framework that preprocesses inputs, computes scores, and applies decision rules to generate outputs.",
      "core_components": [
          {
          "component": "Preprocessing Module",
          "function": "standardizes and validates raw inputs",
          "inputs": ["raw inputs"],
          "outputs": ["processed inputs"]
          },
          {
          "component": "Scoring Function",
          "function": "computes a score used for decision-making",
          "inputs": ["processed inputs", "parameters"],
          "outputs": ["scores"]
          }
      ],
      "workflow_sequence": [
          {
          "step_number": 1,
          "action": "prepare inputs using the Preprocessing Module",
          "components_used": ["Preprocessing Module"],
          "insights": "Table 1 indicates preprocessing improves stability."
          },
          {
          "step_number": 2,
          "action": "compute scores and apply selection",
          "components_used": ["Scoring Function"],
          "insights": null
          }
      ],
      "objectives": [
          {
          "goal": "improve reliability of decision-making",
          "supported_by": ["Preprocessing Module", "Step 1"]
          }
      ],
      "technical_keywords": ["preprocessing", "scoring", "thresholding", "parameter tuning"]
      }

      [INPUT PLACEHOLDER]
      Below is the input of section text:
      {{method_section_text}}

    extract_pros_and_cons: |
      [ROLE DESCRIPTION]  
      You are an expert research analyst specializing in cross-study methodology comparison. You excel at synthesizing structured data to identify commonalities and distinctions across research papers, and at inferring high-level abstract topics that unify them.

      [INPUT FORMAT]  
      Input will be strictly in JSON format, containing one or more methodology analyses.  
      - Each analysis corresponds to a single paper.  
      - Each analysis follows the structure:  
        {
          "paper_id": "...",
          "methodology": {
            "name": "...",
            "description": "...",
            "pros": ["..."],
            "cons": ["..."]
          }
        }

      [TASK DESCRIPTION]  
      Analyze the set of methodology analyses.  
      1. Identify **common advantages/limitations** (across ‚â•2 papers).  
      2. Identify **unique advantages/limitations** (per paper).  
      3. Infer a **high-level, abstract research topic** that best captures the overall theme of the input papers.  
      Then synthesize everything into a **single JSON object** following the schema defined below.

      [PROCESSING INSTRUCTIONS]  
      1. Collect all `pros` and `cons` across papers.  
      2. Identify **common pros/cons**: appear in ‚â•2 different papers.  
      3. Identify **unique pros/cons**: appear in only one paper.  
      4. For each paper, attach its unique pros/cons separately.  
      5. From all papers‚Äô methods, infer a concise, general, and expressive topic label that unifies them.  

      [OUTPUT CONSTRUCTION]  
      Produce a **single JSON object** with the following structure:  
        {
          "inferred_topic": "...",
          "key_commonalities": {
            "pros": ["..."],
            "cons": ["..."]
          },
          "paper_level_analysis": [
            {
              "paper_id": "...",
              "method_name": "...",
              "description": "...",
              "common_pros": ["..."],
              "unique_pros": ["..."],
              "common_cons": ["..."],
              "unique_cons": ["..."]
            }
          ],
          "notes": "Unique aspects highlight methodological distinctions"
        }

      [OUTPUT RULES]
      * Always output **valid JSON only** (no Markdown).  
      * If a list is empty, return an empty array `[]`.  
      * Sort `paper_level_analysis` entries alphabetically by `method_name`.  
      * If only one paper is given, set `"key_commonalities": { "pros": [], "cons": [] }`.  
      * Preserve exact wording of `pros`/`cons` from input. Do not paraphrase.  
      * Escape special characters to ensure valid JSON.  
      * `"inferred_topic"` must be concise, general, and expressive, avoiding dataset names, benchmarks, or overly specific jargon.  

      [SCHEMA SUMMARY]
      * `inferred_topic`: high-level abstract research topic inferred from the set of papers.  
      * `key_commonalities`: shared pros/cons across ‚â•2 papers.  
      * `paper_level_analysis`: per-paper breakdown with common and unique aspects.  
      * `notes`: static footer string highlighting the meaning of unique aspects.  

      [INPUT PLACEHOLDER]  
      Below is the input JSON of methodology summaries:  
      {{method_summaries}}

    taxonomy_generation: |
      [ROLE DESCRIPTION]
      You are an expert research methodology taxonomist. Your task is to synthesize information from multiple sources about a set of research papers and generate a single, hierarchical classification tree in a strict JSON format. You should use a reference taxonomy as a soft guide for structure and naming but must not override clear evidence from the input data.

      [REFERENCE TAXONOMY USAGE]
      Treat the reference taxonomy as a baseline guide (a soft reference). Align with existing categories when they clearly apply. Extend, merge, or refine categories if the evidence requires it. Introduce new paradigms when none of the reference categories fit. Do not force a method into an ill-fitting category just to preserve reference alignment.

      [TASK DESCRIPTION]
      Analyze the provided data blocks to build a multi-layered taxonomy of research methodologies. Use the detailed methodology summaries and the pros/cons report as the primarya basis for classification. The final output must be a valid JSON object representing the classification tree.

      [INPUT FORMAT]
      You will be provided with the following data blocks. Some blocks may be optional and may be empty.

      1. **Methodology Summaries:**  
        A JSON object mapping paper filenames to their detailed, structured methodology summaries.

      2. **Pros & Cons Report:**  
        A JSON object containing a comparative analysis of advantages and limitations found across the methods.

      3. **Reference Taxonomy:**  
        A JSON object representing an existing taxonomy. Treat this as a soft reference: align with it where possible, but extend, refine, or reorganize categories when the evidence requires.

      [CORE INSTRUCTIONS FOR TAXONOMY GENERATION]

      1. Identify Papers:
        - Use the `paper_id` field (a string) as the unique identifier for each paper across all inputs.
        - If any input block uses filenames or other keys in addition to `paper_id`, prefer the explicit `paper_id`.
        - In the final taxonomy, all node `index` arrays MUST list these `paper_id` strings. Each leaf node MUST correspond to exactly one `paper_id`.

      2. Build Taxonomy Tree (Hierarchical Classification):
        - Soft Guidance from Reference Taxonomy:
          - Throughout the classification, consult the Reference Taxonomy as a **soft guide** for candidate groupings and labels. Adopt its categories when they clearly fit; extend, merge, or refine them when the input evidence requires. **Do not force** a method into an ill-fitting category.
        
        - Root Node (Inferred Topic):
          - Name the root with a concise, general, expressive topic inferred from the collection (e.g., from recurring terms in method names/descriptions). Avoid dataset names, benchmarks, or overly specific jargon.
          - The root `index` must include **all** `paper_id`s.

        - Partitioning Strategy (Ordered Dimensions):
          1) **Primary Aim**: First, partition papers by their main objective or function (the purpose the approach primarily serves).
          2) **Core Technique Family**: Within each aim, group by the principal methodological family (e.g., rule-based, optimization-driven, learning-based, search/planning, hybrid). Keep labels short and general.
          3) **Operational Regime / Strategy**: Further refine by salient operational choices (e.g., data regime, supervision style, pipeline design, single- vs multi-stage, online vs offline decision flow).
          4) **Distinctive Features & Trade‚Äëoffs**: Use evidence from {{pros_cons_summary}} and method descriptions to introduce splits that capture meaningful differences (e.g., scalability, robustness, interpretability, efficiency, adaptability). Only add layers when they introduce a clear, distinguishing dimension.

        - Assignment & Exclusivity:
          ‚Ä¢ Assignment & Multi-Branch Handling
            - A paper may be relevant to multiple branches. In such cases, attach the paper as a leaf under **each applicable branch** using the same `paper_id`.
            - To avoid unnecessary duplication at very fine granularity, if a paper spans several sibling sub-branches within a parent, **attach it directly to the nearest higher-level node** (the lowest ancestor that faithfully covers all relevant sub-branches) instead of repeating it across grandchildren.
            - Parent node `index` arrays MUST include all `paper_id`s for leaves directly attached anywhere in their subtree.
            - Children must be strict semantic subsets of their parent. Multi-branch attachment is allowed for leaves only and does not violate subset semantics.

          ‚Ä¢ Validation Adjustments
            - Completeness: Every `paper_id` MUST appear as a leaf at least once.
            - Non-exclusivity for leaves: A `paper_id` MAY appear under multiple branches when justified by evidence.
            - No circular references: A leaf must not be an ancestor of itself. 
            - Depth economy: Prefer attaching to the **nearest** higher-level node that accurately reflects membership when a paper spans multiple fine-grained branches.


        - Node Payloads (All Levels):
          - `name`: Provide an expressive, accurate label.
          - `description`: One‚Äìtwo simple sentences describing the methods grouped at this node.
          - `pros` / `cons`: For intermediate nodes, include **deduplicated** items that characterize the group (i.e., appear across multiple member papers). For leaves, use the paper‚Äëspecific items.
          - `index`: List of `paper_id` strings belonging to the node.
          - `children`: Subnodes (empty array for leaves).

        - Depth Control & Merging:
          - Add deeper layers **only** if they introduce meaningful, evidence‚Äëbacked distinctions.
          - If an intermediate node would have only one child, **merge** the node with that child and assign a single, more specific, semantically accurate name.


        3. **Node Schema Requirements:**
          * **Intermediate Nodes (Categories):**
            ```json
            {
              "name": "<Category Label>",
              "description": "<Simple description of the methods grouped here>",
              "pros": ["..."],
              "cons": ["..."],
              "index": [<Paper ID 1>, <Paper ID 2>, ...],
              "children": [ ...subnodes... ]
            }
            ```
          * **Leaf Nodes (Papers):**
            ```json
            {
              "name": "<Exact Paper Filename>",
              "description": "<Concise summary of the paper's specific methodology>",
              "pros": ["..."],
              "cons": ["..."],
              "index": [<Paper ID>],
              "children": []
            }
            ```

        [VALIDATION RULES & CONSTRAINTS]
        * Every paper from the input must appear as a leaf node exactly once.
        * A paper cannot appear in more than one branch of the tree. The classification must be mutually exclusive at all levels.
        * Children must be strict semantic subsets of their parents.
        * Each new layer must introduce a more specific, distinguishing dimension.
        * Preserve original technical terms. Do not generalize or rename concepts without evidence.
        * Do not create vague categories like "Other" or "Miscellaneous". Every group must have a clear, defining characteristic.
        * If an intermediate node has only one child, merge them into a single, semantically meaningful node.

        [OUTPUT REQUIREMENTS]
        - Provide ONLY the final JSON object enclosed in a ```json code fence.
        - Do not include any additional text, notes, or explanations before or after the JSON block.
        - Ensure the output is a single, syntactically valid JSON object.

      [INPUT PLACEHOLDER]

      --- BEGIN METHOD SUMMARIES ---
      {{method_summaries}}
      --- END METHOD SUMMARIES ---

      --- BEGIN PROS & CONS SUMMARY ---
      {{pros_cons_summary}}
      --- END PROS & CONS SUMMARY ---

      --- BEGIN REFERENCE TAXONOMY ---
      {{reference_taxonomy}}
      --- END REFERENCE TAXONOMY ---

experiment_setting_extractor:
  extract_experiment_summary: |
    You are a highly precise information extractor for scientific papers.

    ## üéØ Task
    Given the **experiment-related sections** of a research paper, extract a structured JSON summary containing:

    1. **Paper Metadata**  
    2. **Used Datasets**  
    3. **Used Evaluation Metrics**  
    4. **Baselines Compared Against** (including citation numbers)  
    5. **Performance of the Proposed Method** on each dataset using each metric

    The goal is to document what **datasets** the paper uses, what **metrics** it evaluates with, what **baselines** it compares against (and how they are cited), and how well its **proposed method** performs ‚Äî including numeric values and supporting context.

    ---

    ## üßæ Input Format

    You will receive:
    - The **text from the experiment section(s)** of a single paper
    - The **paper title** and **paper_id** (or filename)

    The text may include:
    - Performance tables
    - Descriptions of datasets and baselines
    - Metric definitions
    - Numeric results (in sentences or tables)

    ---

    ## üì¶ Output Format (strict JSON only)

    ```json
    {
      "paper_id": "unique_id",
      "paper_title": "Full Paper Title",
      "proposed_method_name": "MethodXYZ",

      "datasets": [
        {
          "name": "DatasetName",
          "description": "Short description if available",
          "task_type": ["classification", "recommendation"],
        }
      ],

      "metrics": [
        {
          "name": "Accuracy",
          "description": "What the metric measures (if stated)",
          "formulation": "Optional LaTeX if present",
        }
      ],

      "baselines": [
        {
          "name": "BaselineModel",
          "description": "Short description if available",
          "category": "deep learning",   // optional (e.g., heuristic, classical ML, deep, graph, LLM)
          "citation_number": "[5]",      // e.g., citation reference [5] or [Smith et al., 2021]
        }
      ],

      "results": [
        {
          "dataset_name": "DatasetName",
          "metric_name": "Accuracy",
          "value": 0.942,
          "processing": "Details of how the dataset was processed",  // optional
        }
      ]
    }
    ```

    ---

    ## ‚úÖ Extraction Instructions

    - **Only include information that is explicitly stated** in the text provided.
    - You must fill in `"name"` for datasets, metrics, and baselines. Other fields are optional but encouraged.
    - The `proposed_method_name` is often referred to as "our method", "we", "proposed model", or bolded in tables. Use your best judgment from the context.
    - For each dataset and metric, list only once ‚Äî avoid duplicates.

    ### Datasets
    - Extract datasets that are used in main experiments.
    - Include descriptions or task types if stated.
    - If the dataset was preprocessed (e.g., resized, filtered, sampled), include the information in the `processing` field.

    ### Metrics
    - Extract the names of evaluation metrics used.
    - If the paper provides a definition or mathematical formulation, include it.

    ### Baselines
    - Extract all models or methods that the proposed method is compared against in experiments.
    - Use the full name of the baseline method if available.
    - Add a short description or category (e.g., graph-based, deep learning, LLM) if stated.
    - **Include the in-text citation number or reference tag** (e.g., ‚Äú[5]‚Äù or ‚Äú[Smith et al., 2021]‚Äù) if it appears near the baseline mention.

    ### Results
    - Include **only the main experiment results**.  
      ‚ùó **Do not include results from ablation studies, hyperparameter tuning, robustness tests, or other analysis sections.**
    - Include one entry per (dataset, metric) pair for the proposed method.
    - Normalize all metric values to the range **[0, 1]**.  
      ‚ùó If the paper reports values as percentages (e.g., 94.2%), convert to decimal (e.g., 0.942).
    - Remove any field referring to units, split types, or model variants.
    - If values appear as ‚Äúwe achieve 94.2% accuracy...‚Äù, extract the value and quote the phrase as provenance.
    - Use numeric types when possible.

    ---

    ## üîí Output Rules

    - Return strictly valid UTF-8 JSON.
    - Do not include explanations or reasoning.
    - Do not add content that is not grounded in the input text.
    - Omit fields if no reliable information is available.
    - Include a `"confidence"` field (between 0 and 1) for each dataset, metric, baseline, and result item.
    - Do not hallucinate URLs, references, or method names.

    ---

    ## üß† Controlled Vocabulary (optional)

    - `task_type` options (for datasets):  
      `"classification"`, `"regression"`, `"ranking"`, `"recommendation"`, `"entity matching"`, `"clustering"`, `"generation"`, `"qa"`, `"segmentation"`, `"summarization"`

    - Use metric names as-is if known (e.g., `"Accuracy"`, `"RMSE"`, `"NDCG@10"`)

    ---

    ## üß™ Example (abbreviated)

    **Input Text (snippet):**
    > We evaluate our model on CIFAR-10 and TinyImageNet. Our model achieves 94.2% accuracy on CIFAR-10 and 64.1% on TinyImageNet. We compare against ResNet-50 [5] and DeiT-Small [7].

    **Expected JSON Output:**
    ```json
    {
      "paper_id": "paper_001",
      "paper_title": "Image Classification via Self-Distillation",
      "proposed_method_name": "OurModel",

      "datasets": [
        {"name": "CIFAR-10", "processing": "images resized to 32x32", "confidence": 0.95},
        {"name": "TinyImageNet", "confidence": 0.92}
      ],

      "metrics": [
        {"name": "Accuracy", "description": "percentage of correct predictions", "confidence": 0.9}
      ],

      "baselines": [
        {"name": "ResNet-50", "category": "deep learning", "citation_number": "[5]", "confidence": 0.9},
        {"name": "DeiT-Small", "category": "transformer", "citation_number": "[7]", "confidence": 0.85}
      ],

      "results": [
        {
          "dataset_name": "CIFAR-10",
          "metric_name": "Accuracy",
          "value": 0.942,
          "provenance": {"quote": "achieves 94.2% accuracy on CIFAR-10", "source": "text"},
          "confidence": 0.95
        },
        {
          "dataset_name": "TinyImageNet",
          "metric_name": "Accuracy",
          "value": 0.641,
          "provenance": {"quote": "64.1% on TinyImageNet", "source": "text"},
          "confidence": 0.9
        }
      ]
    }
    ```

    ---

    ## üì• Now process the following:

    **paper_id**: `{paper_id}`  
    **paper_title**: `{paper_title}`

    **Experiment Sections**:
    ```
    {experiment_text}
    ```
  merge_baselines: |
    You are given a list of baseline method entries extracted from multiple research papers. Your task is to merge these entries into a **single unified list** of canonical baseline methods.

    ---

    ## üéØ Goal

    For each baseline method, create a unified entry with:

    * `canonical_name`
    * Collected `aliases`
    * Merged metadata (`category`, `description`)
    * `supporting_papers` for provenance
    * `usage_frequency` = number of distinct papers referencing it

    ---

    ## üßæ Input Format

    ```json
    [
      {
        "paper_id": "abc123",
        "baselines": [
          {
            "name": "XGBoost",
            "description": "Gradient boosting decision tree method.",
            "category": "classical ML"
          },
          {
            "name": "BERT",
            "description": "Transformer-based pretrained language model.",
            "category": "deep learning"
          }
        ]
      },
      {
        "paper_id": "xyz456",
        "baselines": [
          {
            "name": "xgb",
            "description": "Boosted tree method widely used for tabular tasks.",
            "category": "classical ML"
          }
        ]
      }
    ]
    ```

    Each block contains:

    * `paper_id` ‚Äì identifier of the paper
    * `baselines` ‚Äì list of baseline method entries

    Each baseline entry has at least a `name` and may include:

    * `description`
    * `category`

    ---

    ## üì¶ Output Format

    ```json
    {
      "baselines": [
        {
          "canonical_name": "xgboost",
          "aliases": ["XGBoost", "xgb"],
          "description": "Gradient boosting decision tree method.",
          "category": "classical ML",
          "supporting_papers": ["abc123", "xyz456"],
          "usage_frequency": 2
        },
        {
          "canonical_name": "bert",
          "aliases": ["BERT"],
          "description": "Transformer-based pretrained language model.",
          "category": "deep learning",
          "supporting_papers": ["abc123"],
          "usage_frequency": 1
        }
      ]
    }
    ```

    ---

    ## ‚úÖ Instructions

    * Normalize names to create `canonical_name`: lowercase, remove punctuation, unify hyphen/underscore.
    * Merge entries with the same `canonical_name`.
    * For conflicting descriptions or categories, keep the one supported by more papers.
    * Merge `supporting_papers` and count unique `paper_id`s for `usage_frequency`.
    * Preserve all observed surface forms in `aliases`.

    Return only valid JSON in the output:

    ```json
    {"baselines": [ ... ]}
    ```

    Below is the input:
    [Baseline_input]

  merge_datasets: |
    You are given a collection of dataset entries extracted from multiple sources. Each source provides a **`paper_id`** and its corresponding list of **`datasets`**. Your task is to merge all dataset entries across papers into a **single unified list**.

    ---

    ## üéØ Goal

    For each dataset, generate a single canonical entry by:

    * Unifying different surface names into one `canonical_name`
    * Collecting all observed `aliases`
    * Merging non-conflicting fields
    * Preserving provenance via `supporting_papers`
    * Computing `usage_frequency` as the number of distinct papers referencing it

    ---

    ## üßæ Input Format

    ```json
    [
      {
        "paper_id": "abc123",
        "datasets": [
          {
            "name": "Example-Dataset",
            "description": "A sample dataset used for testing.",
            "task_type": ["classification"],
            "processing": "Preprocessed with tokenization and normalization.",
            "confidence": 0.95
          },
          {
            "name": "SampleData",
            "description": "Another dataset for benchmarking purposes.",
            "task_type": ["regression"],
            "confidence": 0.9
          }
        ]
      },
      {
        "paper_id": "xyz456",
        "datasets": [
          {
            "name": "Ex_DS",
            "description": "Structured dataset commonly used for evaluation.",
            "task_type": ["classification"],
            "confidence": 0.92
          }
        ]
      }
    ]
    ```

    Each paper block contains:

    * `paper_id` ‚Äì unique identifier of the paper
    * `datasets` ‚Äì a list of dataset entries mentioned in that paper

    Each dataset entry has at least a `name` and may include:

    * `description`
    * `profile` (e.g., `size`, `attributes`, `categories`, etc.)
    * `task_type`
    * `url`
    * `processing`
    * `confidence`

    ---

    ## üì¶ Output Format

    ```json
    {
      "datasets": [
        {
          "canonical_name": "example_dataset",
          "aliases": ["Example-Dataset", "Ex_DS"],
          "description": "A sample dataset used for testing.",
          "profile": { ... },
          "task_type": ["classification"],
          "url": "...",
          "processing": "Preprocessed with tokenization and normalization.",
          "supporting_papers": ["abc123", "xyz456"],
          "usage_frequency": 2
        },
        {
          "canonical_name": "sampledata",
          "aliases": ["SampleData"],
          "description": "Another dataset for benchmarking purposes.",
          "task_type": ["regression"],
          "supporting_papers": ["abc123"],
          "usage_frequency": 1
        }
      ]
    }
    ```

    ---

    ## ‚úÖ Instructions

    * Normalize names to create `canonical_name`: lowercase, remove punctuation, collapse whitespace, unify hyphen/underscore.
    * Merge entries with the same `canonical_name`.
    * For conflicting descriptions, keep the one supported by more distinct papers.
    * Merge `supporting_papers` and count unique `paper_id`s for `usage_frequency`.
    * Preserve all observed surface forms in `aliases`.

    Return only valid JSON in the output.

    Below is the input:
    [Dataset_input]

  merge_metrics: |
    You are given a list of metric definitions extracted from multiple research papers. Your task is to merge them into a **single unified list** of metrics.

    ---

    ## üéØ Goal

    For each metric:

    * Normalize to a `canonical_name`
    * Collect all observed surface names as `aliases`
    * Merge non-conflicting fields (`description`, `formulation`, `task_scope`)
    * Preserve provenance in `supporting_papers`
    * Compute `usage_frequency` as the number of distinct papers mentioning it

    ---

    ## üßæ Input Format

    ```json
    [
      {
        "paper_id": "abc123",
        "metrics": [
          {
            "name": "RMSE",
            "description": "Root Mean Square Error, measures average magnitude of error.",
            "formulation": "sqrt(mean((y - y_hat)^2))",
            "task_scope": ["regression"]
          },
          {
            "name": "Accuracy",
            "description": "Proportion of correct predictions out of total predictions.",
            "formulation": "(TP + TN) / (TP + TN + FP + FN)",
            "task_scope": ["classification"]
          }
        ]
      },
      {
        "paper_id": "xyz456",
        "metrics": [
          {
            "name": "Root Mean Square Error",
            "description": "Measures deviation between predicted and observed values.",
            "formulation": "sqrt(Œ£(y - y_hat)^2 / n)",
            "task_scope": ["recommendation"]
          }
        ]
      }
    ]
    ```

    Each block contains:

    * `paper_id` ‚Äì unique identifier of the paper
    * `metrics` ‚Äì list of metric entries extracted from that paper

    Each metric entry has at least a `name` and may include:

    * `description`
    * `formulation`
    * `task_scope`

    ---

    ## üì¶ Output Format

    ```json
    {
      "metrics": [
        {
          "canonical_name": "rmse",
          "aliases": ["RMSE", "Root Mean Square Error"],
          "description": "Root Mean Square Error, measures average magnitude of error.",
          "formulation": "sqrt(mean((y - y_hat)^2))",
          "task_scope": ["regression", "recommendation"],
          "supporting_papers": ["abc123", "xyz456"],
          "usage_frequency": 2
        },
        {
          "canonical_name": "accuracy",
          "aliases": ["Accuracy"],
          "description": "Proportion of correct predictions out of total predictions.",
          "formulation": "(TP + TN) / (TP + TN + FP + FN)",
          "task_scope": ["classification"],
          "supporting_papers": ["abc123"],
          "usage_frequency": 1
        }
      ]
    }
    ```

    ---

    ## ‚úÖ Instructions

    * Normalize names to create `canonical_name`: lowercase, trim, remove punctuation, unify hyphen/underscore.
    * Merge entries with the same `canonical_name`.
    * For conflicting descriptions/formulations, keep the one supported by more distinct papers.
    * Merge `task_scope` lists and deduplicate.
    * Merge `supporting_papers` and count unique `paper_id`s for `usage_frequency`.
    * Preserve all observed surface names in `aliases`.

    Return only valid JSON in the output:

    ```json
    {"metrics": [ ... ]}
    ```

    Below is the input:
    [Metric_input]


              
summary_generator: |
  You are given the full content of a single section from a research paper. Your task is to **summarize the content of this section only**.

  **Instructions:**

  * Write a concise summary (2‚Äì4 sentences) that captures the main purpose, arguments, methods, or findings in this section.
  * If the section has subsections, integrate their content into one coherent summary.
  * Do not copy sentences verbatim; rephrase in clear academic language.
  * **Only output the summary text. Do not include labels, JSON, formatting, or any additional commentary.**

  **Input:**
  [Section Content]

  **Output:**
  [Summary of that section only]

KB_functions:
  Experiment_settings_recommendation: |
    \[ROLE DESCRIPTION]
    You are an expert research experiment designer. Your task is to analyze canonical experiment metadata (datasets, baselines, and metrics) collected from multiple papers in a given research topic and recommend the **most suitable experiment settings** for a new research paper in that topic.

    \[INPUTS]
    You will be given:

    1. **Canonical Datasets (`{canonical_datasets}`):** A structured list of datasets, their aliases, descriptions, processing details, usage frequency, and supporting papers.
    2. **Canonical Baselines (`{canonical_baselines}`):** A structured list of baselines, their aliases, descriptions, categories, usage frequency, and supporting papers.
    3. **Canonical Metrics (`{canonical_metrics}`):** A structured list of evaluation metrics, their aliases, descriptions, formulations, usage frequency, and supporting papers.

    \[PROCESSING INSTRUCTIONS]

    1. **Dataset Selection**

      * Recommend datasets that are **widely adopted** (higher usage frequency) and **representative** of the topic.
      * Ensure coverage across different dataset characteristics (e.g., scale, modality, or diversity).
      * Mark the **top three most recommended datasets** as `recommended = True`, others as `recommended = False`.

    2. **Baseline Selection**

      * Recommend a **balanced set** of baselines, including:

        * **Classical approaches** for traditional comparison.
        * **Recent state-of-the-art methods** for benchmarking.
        * **Alternative paradigms** (unsupervised, graph-based, LLM-based, etc.) if frequently used or highlighted.
      * Mark the **top three most recommended baselines** as `recommended = True`, others as `recommended = False`.

    3. **Metric Selection**

      * Always include **core evaluation metrics** commonly used in the topic.
      * Add additional metrics if they are **task- or setting-specific**.
      * Mark the **top three most recommended metrics** as `recommended = True`, others as `recommended = False`.

    4. **Output Formatting**

      * For each dataset, baseline, and metric, output objects in the following format:

        ```json
        {
          "name": "<canonical_name>",
          "description": "<description>",
          "usage_count": <integer>,
          "recommended": true/false
        }
        ```
      * Wrap all results in a single JSON object with the following structure:

        ```json
        {
          "recommended_datasets": [ ... ],
          "recommended_baselines": [ ... ],
          "recommended_metrics": [ ... ],
          "rationale": "Explanation of why the top 3 in each category were marked as recommended."
        }
        ```

    \[OUTPUT]
    Only output the JSON object, no extra explanation. **The size of the recommended datasets, baselines, and metrics should be less than 5.**

    Below is the input:
    [Experiment_settings_recommendation_input]








