chunker:
  section_labeling: |
    You are given a list of section titles from a single research paper. Your task is to categorize each title into one of the following categories:

    `["Abstract", "Introduction", "Problem Definition", "Methodology", "Related Work", "Experiment", "Conclusion", "References", "Acknowledgement", "Appendix"]`

    **Instructions:**

    * Assign each title to exactly one category.
    * If a title does not clearly fit any category, assign it to the most likely one based on typical academic paper structure.
    * Consecutive sections in the input list are highly likely to belong to the same category ‚Äî use this as an additional cue when making assignments.
    * Multiple titles may belong to the same category, but no title should appear in more than one category.
    * Output only the raw JSON object without any explanation, commentary, or extra formatting. Do not wrap the output in triple backticks or add a language label.

    **Output format:**

    ```
    {
      "Abstract": [titles assigned to Abstract],
      "Introduction": [titles assigned to Introduction],
      "Problem Definition": [titles assigned to Problem Definition],
      "Methodology": [titles assigned to Methodology],
      "Related Work": [titles assigned to Related Work],
      "Experiment": [titles assigned to Experiment],
      "Conclusion": [titles assigned to Conclusion],
      "References": [titles assigned to References],
      "Acknowledgement": [titles assigned to Acknowledgement],
      "Appendix": [titles assigned to Appendix]
    }
    ```

    **Input:**
    [Title List]

    **Output:**


taxonomy_generator:
  task_taxonomy:
    extract_problem_definition: |
      You are acting as a **research assistant** tasked with analyzing a research paper to extract its **core research problem** in a **structured**, **methodology-agnostic** format.

      Your objective is to **identify or infer the problem definition** and decompose it into precise components that describe **what the problem is**, **without referencing how it is solved**. Do not include any mention of algorithms, models, frameworks, training strategies, or implementation details.

      If the paper uses shorthand (e.g., acronyms, variables, or mathematical notation), refer to the preliminaries or notation sections to rewrite the problem in a **clear and unambiguous** manner.

      ---

      ### ‚úÖ Output Format

      ```yaml
      paper_id: "<Paper ID>"
      paper_title: "<Title of the paper>"

      problem_formulation:
        simple_description: >
          "<One-sentence summary of the problem, or `None` if not found.>"

        formal_definition:
          input: >
            "<Describe the input, including its type, structure, and properties. Mention hardware constraints only if they are explicitly part of the problem setting‚Äîdo NOT infer hardware requirements from experimental setups.>"
          output: >
            "<Describe the desired output, including type, structure, and expected characteristics.>"
      ```

      ---

      ### üß≠ Step-by-Step Instructions

      #### üîπ Step 1: Generate `simple_description`

      Create a concise, self-contained summary of the problem using this template:

      > **"Given <input>, the goal is to obtain <output> by achieving <objective>."**

      Follow this order when searching for the problem statement:

      1. Dedicated **Problem Definition** section
      2. **Introduction**
      3. **Abstract**
      4. Relevant parts of the **Methodology** *(only for high-level task description‚Äînot implementation details)*
      5. Any other section as necessary

      > If no coherent one-sentence summary can be extracted, return:

      ```yaml
      simple_description: None
      ```

      > ‚úÖ When a paper describes multiple related problems, prioritize the **primary** or **most general** problem formulation unless others are clearly independent.

      ---

      #### üîπ Step 2: Generate `formal_definition`

      Decompose the problem into the following structured components:

      | Field      | What to Describe                                                                                                                                                                                                                                                                         |
      | ---------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | **input**  | The data or contextual information provided to the system. Include type, format, and any explicitly stated constraints (e.g., memory, disk). **Do not treat the hardware used in experimental settings as input constraints unless they are explicitly part of the problem definition.** |
      | **output** | The expected result, product, or prediction. Include type and structure. Do not include how it is computed.                                                                                                                                                                              |

      > Leave any field blank (`""`) if it cannot be inferred from the paper.

      ---

      ### üö´ Do Not Include:

      * Specific methods, algorithms, architectures, or procedural details
      * Training objectives, loss functions, or optimization strategies
      * System or model names
      * Hardware specifications from experiments (e.g., GPUs, clusters), **unless explicitly part of the problem setting**

      ---

      ### ‚ö†Ô∏è Common Pitfalls to Avoid

      | Case                                               | Guideline                                                                                                                                                  |
      | -------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | **Hardware in experiments**                        | Never assume experimental hardware is part of the problem input unless **explicitly described** (e.g., ‚Äúthe task must run on edge devices with ‚â§2GB RAM‚Äù). |
      | **Vague or entangled problem-method descriptions** | Isolate the **task definition** only. If it's mixed with solution details, extract only the input‚Äìoutput‚Äìgoal relationship.                                |
      | **Multiple sub-problems**                          | Focus on the **primary problem** or provide a clearly separated list if multiple are independently defined.                                                |
      | **No clear problem statement**                     | Use context from preliminaries, task descriptions, or dataset definitions to infer a possible formulation. If still ambiguous, return `"None"` as needed.  |

      Below is the paper information:
      Paper ID: [Paper ID]
      Paper Title: [Paper Title]
      Paper Content:[Paper Content]

    aspect_classification: |
      You are given a list of problem definitions extracted from a collection of computer science research papers. Each definition includes structured aspects such as:

      * **Input**
      * **Output**

      ---

      ## ‚úÖ Task: Aspect-Wise Classification Process

      Focus on the following two aspects:

      * **Input**
      * **Output**

      For **each aspect**, process its corresponding content independently and repeat the following steps:

      ---

      ### üî• Step 1: Extract Key Entities

      Iterate over each paper. From the content of the current aspect:

      * Identify the main research-related entities (key concepts) that **describe the authors‚Äô formulation of this aspect** and are **explicitly emphasized in `problem_formulation` as central to the problem definition**.
      * If `problem_formulation` lacks sufficient information, refer to the corresponding structured field (**input**, **output**) for clarification.

      Focus on extracting noun phrases that match the aspect‚Äôs definition:

      * **Input**: Describe the input, including its type, structure, and properties. Mention hardware constraints only if they are explicitly part of the problem setting‚Äîdo NOT infer hardware requirements from experimental setups.
      * **Output**: Describe the desired output, including type, structure, and expected characteristics.

      ---

      ### üî• Step 2: Group Entities Within Sublists

      Group entities within the same paper and aspect into **local entity groups**. These groups reflect strong correlations between entities (they appear together in the same problem definition). Build connections among entities in each group to capture their co-occurrence relationships.

      ---

      ### üî• Step 3: Align and Merge Entities Across Sublists

      * **Identify Related Entities:** Merge entities across papers if they are exact matches, semantically equivalent, or containment relationships.
      * **Merge into Unified Nodes:** Example: {"user-item matrix", "rating matrix"} ‚Üí `Interaction Matrix`.
      * **Build Global Entity Groups:** Connected components of unified nodes form candidate classes.

      ---

      ### üî• Step 4: Refine Classes and Classify Papers

      1. **Provide Class Explanations:** For each resulting class, provide a short description of its scope and relevance.
      2. **Drop or Merge Small Classes:** Merge or drop classes with <2 papers.
      3. **Align Class Names:** Use formal research terminology.
      4. **Assign Papers:** Assign each paper to one or more classes per aspect. Use the paper ID to denote the paper.

      ---

      ### üî• Step 5: Identify Unassigned or Misaligned Papers

      * Detect **unassigned papers** (entities extracted but no class assignment).
      * Detect **misalignments** (entities poorly matched to class descriptions).
      * Suggest corrections (reassignment or new class).

      ---

      ### üî• Step 6: Implement Corrections

      * For each **unassigned or misaligned paper** identified in Step 5, update the classification by:

        * Assigning it to the most appropriate existing class if suggested.
        * Or creating a new class if no suitable class exists.
      * Ensure that the **final classification output incorporates these corrections directly**, so that all papers have appropriate assignments.

      ---

      ## üìã Output Format

      Return the results **strictly in valid JSON format** with the following structure:

      ```json
      {
        "input_classification": {
          "classes": [
            {
              "class_name": "User-Item Matrix",
              "class_description": "Matrix representation of user-item interactions, commonly used in recommendation tasks.",
              "papers": ["Paper A", "Paper C"]
            },
            {
              "class_name": "Knowledge Graph",
              "class_description": "Graph-based structured input capturing relational facts.",
              "papers": ["Paper B", "Paper F"]
            },
            {
              "class_name": "Temporal Sequence Data",
              "class_description": "Sequential data used for modeling time-dependent behaviors.",
              "papers": ["Paper G"]   // Correction applied here
            }
          ]
        },
        "output_classification": {
          "classes": [
            {
              "class_name": "Hash Codes",
              "class_description": "Compact binary representations of input data used for efficient retrieval.",
              "papers": ["Paper A", "Paper D"]
            },
            {
              "class_name": "Ranked List",
              "class_description": "Ordered set of relevant results returned for a given query.",
              "papers": ["Paper B", "Paper E"]
            }
          ]
        }
      }
      ```
      Below is the problem definition list:
      [Problem Definitions]

    taxonomy_generation: |
      You are an expert research taxonomist. You are given one **JSON object**, containing the classification result of the problem aspects:

      * **Input**: The nature or structure of data the research problem takes as input
      * **Output**: The expected result, format, or structure produced by the method.

      ---

      ### üì• Input Format

      Each JSON object contains an array of class entries. Each entry has the following fields:

      ```json
      {
        "class_name": "string",
        "class_description": "string",
        "supporting_papers": ["PaperA", "PaperB", ...]
      }
      ```

      Below is the classification result:
      [Classification Result]

      ---

      ## ‚úÖ Goal

      Construct a **single hierarchical taxonomy of meaningful research tasks**, where each task is defined by a combination of **Input** and **Output** classes.

      Each node in the taxonomy must represent a **specific, expressive, and well-scoped research task**, not just a data structure or format.

      ---

      ### üî• Step 1: Normalize and Refine Class Labels

      1. **Normalize Class Names:** Use standard academic terminology where possible. Resolve informal, ambiguous, or overly generic names using their class descriptions and example papers.
      2. **Merge Equivalent or Contained Classes:** If two classes are synonyms or exhibit strict containment (e.g., ‚Äúuser-item matrix‚Äù ‚äÜ ‚Äúinteraction matrix‚Äù), merge them.
      3. **Drop or Collapse Weak Classes:** Drop classes with too few or overly vague supporting papers unless they can be merged into broader categories.

      **Output:** Refined JSON list of input and output classes.

      ---

      ### üî• Step 2: Form Meaningful Task Nodes by Input‚ÄìOutput Pairing

      1. **Pair Input and Output Classes:** For every (Input, Output) pair, check if it defines a **coherent, meaningful research task**.
      2. **Keep only valid task nodes** that meet all three criteria:

        * The pair appears in at least **2 different papers**
        * The pairing reflects a **common and recognizable research objective**
        * The task name can be made **expressive** (not just ‚ÄúMatrix ‚Üí Vector‚Äù).
      3. **Name Each Task:**
        Use the format:

        ```
        TASK:L{level}:{CANONICAL_NAME}
        ```

      ---

      ### üî• Step 3: Build a Hierarchy over the Tasks

      1. **Group Similar Tasks:**

        * Parent-child relationships based on task granularity and generalization.
        * Example:

          * Parent: `KNOWLEDGE_GRAPH_TASKS`

            * Child: `KNOWLEDGE_GRAPH_EMBEDDING`
            * Child: `KNOWLEDGE_GRAPH_COMPLETION`

      2. **Hierarchy Rules:**

        * Use only **IS-A** relationships.
        * Construct a **tree** (no DAGs or cycles).
        * Root node: `ROOT`.

      ---

      ## üìã Output Format (JSON Only)

      Return the taxonomy strictly as **valid JSON**. Each node must have:

      * `task_id`: Unique ID in the format `TASK:L{level}:{CANONICAL_NAME}`
      * `task_name`: Expressive descriptive name
      * `input_class`: Input class name
      * `output_class`: Output class name
      * `explanation`: 1‚Äì2 sentence explanation of the task
      * `papers`: List of supporting papers
      * `children`: Array of child task nodes

      Example:

      ```json
      {
        "taxonomy": {
          "task_id": "ROOT",
          "task_name": "ROOT",
          "children": [
            {
              "task_id": "TASK:L1:KNOWLEDGE_GRAPH_COMPLETION",
              "task_name": "Knowledge Graph Completion",
              "input_class": "Knowledge Graph",
              "output_class": "Completed Triples",
              "explanation": "Predict missing edges in a relational graph based on observed triples.",
              "papers": ["PaperA", "PaperD"],
              "children": []
            },
            {
              "task_id": "TASK:L1:HASH_CODE_GENERATION_FOR_RETRIEVAL",
              "task_name": "Hash Code Generation for Retrieval",
              "input_class": "User-Item Interaction Matrix",
              "output_class": "Hash Codes",
              "explanation": "Generate compact binary representations for fast retrieval in recommendation settings.",
              "papers": ["PaperB", "PaperF"],
              "children": []
            }
          ]
        }
      }
      ```

      * Ensure the JSON is **syntactically valid**.
      * Do not output Markdown, tables, or text outside the JSON block.

      ---
  method_taxonomy:
    extract_method_summary: |
      [ROLE]
      You are an AI research methodology analyst specializing in decomposing scientific methods. You interpret textual descriptions and HTML table data associated with figures.

      [TASK]
      Analyze `{section_text}` (Paper ID: `{paper_id}`, Paper Title: `{paper_title}`) containing methodology descriptions and potential HTML tables. Generate an integrated Markdown summary using this workflow:

      1. **Method Decomposition**  
      - Extract core components, workflow sequence, and objectives
      - Represent components in a Markdown table with columns:  
          `Component | Function | Inputs | Outputs`
      - Map workflow steps in a numbered list showing transitions

      2. **Figure Analysis**  
      - Identify referenced figures (e.g., "Figure 1 shows...")
      - For each figure:  
          ‚Ä¢ Locate descriptive text  
          ‚Ä¢ Parse associated HTML tables if present  
          ‚Ä¢ Create analysis table with columns:  
          `Figure Ref | Element Type | Key Relationships | Data Insights`

      3. **Integration**  
      - Cross-reference figures with method components using bold tags (e.g., **ComponentX**)
      - Embed figure insights directly in workflow steps
      - Fuse objectives with supporting evidence from tables

      [OUTPUT RULES]
      ```markdown
      ### Core Components
      | Component | Function | Inputs | Outputs |
      |-----------|----------|--------|---------|
      | ...       | ...      | ...    | ...     |

      ### Workflow Sequence
      1. [Step 1] ‚Üí **(Figure X)**  
      - [Action] using [Component]  
      - *Table insight: [Value] from Table Y*

      ### Objectives
      - [Goal 1] achieved through [Component/Step]  
      *(Supported by Figure Z: [Observation])*

      ### Figure Analysis
      | Fig Ref | Element Type | Key Relationships | Data Insights |
      |---------|--------------|-------------------|---------------|
      | Fig 1   | [Type]       | [Connection]      | [Table Data]  |
      ```
      **Critical Constraints**  
      - Use ONLY standard Markdown (no LaTeX, HTML, or images)  
      - Represent mathematical concepts verbally (e.g., "loss minimization")  
      - Maximum 2 sentences per table cell  
      - Omit redundant explanations already covered in tables  
      - Bold method elements when linked to figures (**ComponentA**)  
      - Prioritize table representations over paragraphs
      - Use the paper ID to denote the paper in the summary

    extract_pros_and_cons: |
      [ROLE DESCRIPTION]
      You are an expert research analyst specializing in cross-study methodology comparison. You excel at synthesizing structured data to identify shared patterns and distinctive features across research papers.

      [TASK DESCRIPTION]
      Analyze methodology analyses from multiple research papers. Identify common/unique advantages and limitations, then synthesize findings into a structured Markdown comparison table. Prioritize visual clarity and information density.

      [INPUT]
      - **Methodology Data (MD Format):** Contains one or more methodology analyses in flexible markdown format.  
      **Each analysis corresponds to a single paper's methodology**.
      input: `{method_summaries}`

      [PROCESSING INSTRUCTIONS]
      1. **Data Extraction:**
        - Extract JSON from Markdown code blocks (prioritize ```json tagged blocks)
        - Parse into array of paper objects: `papers = [{{paper_id, methodology: {{name, description, pros, cons}}}}, ...]`

      2. **Cross-Analysis:**
        ```python
        # Aggregate metrics
        all_pros = flatten(paper.methodology.pros for paper in papers)
        all_cons = flatten(paper.methodology.cons for paper in papers)
        
        # Identify common elements (appearing in ‚â•2 papers)
        common_pros = [p for p in set(all_pros) if count_occurrences(p) >= 2]
        common_cons = [c for c in set(all_cons) if count_occurrences(c) >= 2]
        
        # Map unique elements per paper
        for paper in papers:
            paper["unique_pros"] = [p for p in paper.methodology.pros if p not in common_pros]
            paper["unique_cons"] = [c for c in paper.methodology.cons if c not in common_cons]
        ```

      3. **Output Structure:**
        ```markdown
        # Methodology Comparison Report

        ## Key Commonalities
        | Aspect       | Shared Findings                          |
        |--------------|------------------------------------------|
        | **Pros** | {{bullet_list(common_pros)}}               |
        | **Cons** | {{bullet_list(common_cons)}}               |
        *Note: Appear in ‚â•2 papers*

        ## Paper-Level Analysis
        | Paper ID | Method Name     | Description | Common Pros | Unique Pros | Common Cons | Unique Cons |
        |----------|-----------------|-------------|-------------|-------------|-------------|-------------|
        {{generate_table_rows}}
        ```

      4. **Table Generation Rules:**
        - **Header:** Use exact column names above
        - **Cell Formatting:**
          - `Description`: First 50 words + "..." if truncated
          - Pros/Cons columns: Bullet lists with line breaks (`<br>‚Ä¢ `)
          - Empty cells: "‚Äî" when no items
        - **Sorting:** Order papers by method name alphabetically
        - **Special Cases:**
          - Single paper input: Omit "Key Commonalities" section
          - Empty lists: Display "‚Äî" in table cells

      [OUTPUT REQUIREMENTS]
      1. Strict Markdown format within ```markdown code fence
      2. Table must contain all papers from input
      3. Preserve original text annotations in all pros/cons
      4. Escape Markdown special characters in source text
      5. Include footer note: "*Unique aspects highlight methodological distinctions*"
    taxonomy_generation: "[ROLE DESCRIPTION]\nYou are an expert research methodology
      taxonomist. Your task is to synthesize information from multiple sources about
      a set of research papers and generate a single, hierarchical classification
      tree in a strict JSON format.\n\n[TASK DESCRIPTION]\nAnalyze the provided data
      blocks to build a multi-layered taxonomy of research methodologies. Use the
      detailed methodology summaries and the pros/cons report as the primary basis
      for fine-grained technical classification. Use the introductions and related
      work sections (if provided) for broader, high-level contextual grouping. The
      final output must be a valid JSON object representing the classification tree.\n
      \n[INPUTS]\nYou will be provided with the following data blocks. Some blocks
      (like introductions and related works) are optional and may be empty.\n\n1.\
      \  **Methodology Summaries (`{method_summaries}`):** A JSON object mapping paper
      filenames to their detailed, structured methodology summaries. This is your
      primary source for technical details.\n2.  **Pros & Cons Report (`{pros_cons_summary}`):**
      A Markdown report containing a comparative analysis of the advantages and limitations
      found across the methods. Use this to identify key trade-offs and distinguishing
      features.\n3.  **Introductions (`{introductions}`):** (Optional) A JSON object
      mapping paper filenames to their introduction sections. Use this to understand
      the core problem, motivation, and high-level claims of each paper, which can
      help in forming top-level categories.\n4.  **Related Works (`{related_works}`):**
      (Optional) A JSON object mapping paper filenames to their related work sections.
      Use this to understand how each paper positions itself against others, which
      aids in determining its methodological paradigm.\n\n[CORE INSTRUCTIONS FOR TAXONOMY
      GENERATION]\n\n1.  **Identify Papers:** The paper filenames (e.g., \"paper1.md\"\
      ) are the keys in the `{method_summaries}` and other JSON objects. Use these
      filenames as the unique identifiers for each paper. For the `index` field in
      the output, convert these filenames to a \"P\" format (e.g., extract the number
      from \"paperX.md\" to create \"PX\").\n\n2.  **Build Taxonomy Tree (Hierarchical
      Classification):**\n    * **Root Node:** The top-level node must be named \"\
      Methodology Taxonomy\" and contain all papers in its index.\n    * **Layer 1
      (Fundamental Paradigm):** First, partition all papers into **mutually exclusive**
      high-level categories based on their fundamental research goal or paradigm.
      Examples of such high-level categories include **\"Predictive Modeling\", \"\
      Causal Inference\", \"Generative Methods\", \"System Architecture & Design\"\
      , or \"Theoretical Analysis\"**.\n    * **Layer 2 (Core Technique Family):**
      Subdivide each high-level group based on the core family of techniques employed.
      Examples: **\"Neural Network Architectures\", \"Statistical & Probabilistic
      Methods\", \"Optimization Algorithms\", \"Novel System Components\"**.\n   \
      \ * **Layer 3+ (Distinctive Features):** If necessary, create deeper layers
      to refine the classification based on unique characteristics. Examples: **\"\
      Supervised vs. Unsupervised Learning\", \"Adaptive Components\", \"Scalability
      Enhancements\"**. The `{pros_cons_summary}` is very useful here.\n    * **Leaf
      Nodes:** The final nodes of the tree must be the individual papers.\n\n3.  **Node
      Schema Requirements:**\n    * **Intermediate Nodes (Categories):**\n       \
      \ ```json\n        {{\n          \"name\": \"<Classification Label>\",\n   \
      \       \"content\": \"1) Grouping feature: [Explicit shared characteristic]<br>2)
      Child differences: [Distinguishing aspects between subgroups]\",\n         \
      \ \"index\": [\"P1\", \"P4\"],\n          \"children\": [ ...subnodes... ]\n\
      \        }}\n        ```\n    * **Leaf Nodes (Papers):**\n        ```json\n\
      \        {{\n          \"name\": \"<Exact Paper Filename>\",\n          \"content\"\
      : \"<Provide a 2-3 sentence, concise summary of the paper's specific methodology,
      derived from the input summaries.>\",\n          \"index\": \"<PaperID>\",\n          \"children\": []\n        }}\n        ```\n\n[VALIDATION
      RULES & CONSTRAINTS]\n* **Completeness:** Every paper from the input must appear
      as a leaf node exactly once.\n* **Strict Partitioning:** A paper cannot appear
      in more than one branch of the tree. The classification must be **mutually exclusive
      at all levels.**\n* **Hierarchy:** Children must be strict semantic subsets
      of their parents.\n* **Specificity:** Each new layer must introduce a more specific,
      distinguishing dimension.\n* **Fidelity:** Preserve original technical terms.
      Do not generalize or rename concepts.\n* **No Generic Bins:** Do not create
      vague categories like \"Other\", \"Miscellaneous\", or \"General Approaches\"\
      . Every group must have a clear, defining characteristic.\n* **Node Collapsing:**
      If an intermediate node has only one child, it must be merged to prevent redundant
      layers. **Create a new, semantically meaningful name for the merged node that
      accurately represents the more specific category.** For example, if a **'Learning-based
      Approaches'** node has only one child **'Supervised Methods'**, the new merged
      node could be named **'Supervised Learning Approaches'**. The new node adopts
      the children of the original child.\n\n[OUTPUT REQUIREMENTS]\n- Provide ONLY
      the final JSON object enclosed in a ```json code fence.\n- Do not include any
      additional text, notes, or explanations before or after the JSON block.\n- Ensure
      the output is a single, syntactically valid JSON object."

experiment_setting_extractor:
  extract_experiment_summary: |
    You are a highly precise information extractor for scientific papers.

    ## üéØ Task
    Given the **experiment-related sections** of a research paper, extract a structured JSON summary containing:

    1. **Paper Metadata**  
    2. **Used Datasets**  
    3. **Used Evaluation Metrics**  
    4. **Baselines Compared Against** (including citation numbers)  
    5. **Performance of the Proposed Method** on each dataset using each metric

    The goal is to document what **datasets** the paper uses, what **metrics** it evaluates with, what **baselines** it compares against (and how they are cited), and how well its **proposed method** performs ‚Äî including numeric values and supporting context.

    ---

    ## üßæ Input Format

    You will receive:
    - The **text from the experiment section(s)** of a single paper
    - The **paper title** and **paper_id** (or filename)

    The text may include:
    - Performance tables
    - Descriptions of datasets and baselines
    - Metric definitions
    - Numeric results (in sentences or tables)

    ---

    ## üì¶ Output Format (strict JSON only)

    ```json
    {
      "paper_id": "unique_id",
      "paper_title": "Full Paper Title",
      "proposed_method_name": "MethodXYZ",

      "datasets": [
        {
          "name": "DatasetName",
          "description": "Short description if available",
          "task_type": ["classification", "recommendation"],
        }
      ],

      "metrics": [
        {
          "name": "Accuracy",
          "description": "What the metric measures (if stated)",
          "formulation": "Optional LaTeX if present",
        }
      ],

      "baselines": [
        {
          "name": "BaselineModel",
          "description": "Short description if available",
          "category": "deep learning",   // optional (e.g., heuristic, classical ML, deep, graph, LLM)
          "citation_number": "[5]",      // e.g., citation reference [5] or [Smith et al., 2021]
        }
      ],

      "results": [
        {
          "dataset_name": "DatasetName",
          "metric_name": "Accuracy",
          "value": 0.942,
          "processing": "Details of how the dataset was processed",  // optional
        }
      ]
    }
    ```

    ---

    ## ‚úÖ Extraction Instructions

    - **Only include information that is explicitly stated** in the text provided.
    - You must fill in `"name"` for datasets, metrics, and baselines. Other fields are optional but encouraged.
    - The `proposed_method_name` is often referred to as "our method", "we", "proposed model", or bolded in tables. Use your best judgment from the context.
    - For each dataset and metric, list only once ‚Äî avoid duplicates.

    ### Datasets
    - Extract datasets that are used in main experiments.
    - Include descriptions or task types if stated.
    - If the dataset was preprocessed (e.g., resized, filtered, sampled), include the information in the `processing` field.

    ### Metrics
    - Extract the names of evaluation metrics used.
    - If the paper provides a definition or mathematical formulation, include it.

    ### Baselines
    - Extract all models or methods that the proposed method is compared against in experiments.
    - Use the full name of the baseline method if available.
    - Add a short description or category (e.g., graph-based, deep learning, LLM) if stated.
    - **Include the in-text citation number or reference tag** (e.g., ‚Äú[5]‚Äù or ‚Äú[Smith et al., 2021]‚Äù) if it appears near the baseline mention.

    ### Results
    - Include **only the main experiment results**.  
      ‚ùó **Do not include results from ablation studies, hyperparameter tuning, robustness tests, or other analysis sections.**
    - Include one entry per (dataset, metric) pair for the proposed method.
    - Normalize all metric values to the range **[0, 1]**.  
      ‚ùó If the paper reports values as percentages (e.g., 94.2%), convert to decimal (e.g., 0.942).
    - Remove any field referring to units, split types, or model variants.
    - If values appear as ‚Äúwe achieve 94.2% accuracy...‚Äù, extract the value and quote the phrase as provenance.
    - Use numeric types when possible.

    ---

    ## üîí Output Rules

    - Return strictly valid UTF-8 JSON.
    - Do not include explanations or reasoning.
    - Do not add content that is not grounded in the input text.
    - Omit fields if no reliable information is available.
    - Include a `"confidence"` field (between 0 and 1) for each dataset, metric, baseline, and result item.
    - Do not hallucinate URLs, references, or method names.

    ---

    ## üß† Controlled Vocabulary (optional)

    - `task_type` options (for datasets):  
      `"classification"`, `"regression"`, `"ranking"`, `"recommendation"`, `"entity matching"`, `"clustering"`, `"generation"`, `"qa"`, `"segmentation"`, `"summarization"`

    - Use metric names as-is if known (e.g., `"Accuracy"`, `"RMSE"`, `"NDCG@10"`)

    ---

    ## üß™ Example (abbreviated)

    **Input Text (snippet):**
    > We evaluate our model on CIFAR-10 and TinyImageNet. Our model achieves 94.2% accuracy on CIFAR-10 and 64.1% on TinyImageNet. We compare against ResNet-50 [5] and DeiT-Small [7].

    **Expected JSON Output:**
    ```json
    {
      "paper_id": "paper_001",
      "paper_title": "Image Classification via Self-Distillation",
      "proposed_method_name": "OurModel",

      "datasets": [
        {"name": "CIFAR-10", "processing": "images resized to 32x32", "confidence": 0.95},
        {"name": "TinyImageNet", "confidence": 0.92}
      ],

      "metrics": [
        {"name": "Accuracy", "description": "percentage of correct predictions", "confidence": 0.9}
      ],

      "baselines": [
        {"name": "ResNet-50", "category": "deep learning", "citation_number": "[5]", "confidence": 0.9},
        {"name": "DeiT-Small", "category": "transformer", "citation_number": "[7]", "confidence": 0.85}
      ],

      "results": [
        {
          "dataset_name": "CIFAR-10",
          "metric_name": "Accuracy",
          "value": 0.942,
          "provenance": {"quote": "achieves 94.2% accuracy on CIFAR-10", "source": "text"},
          "confidence": 0.95
        },
        {
          "dataset_name": "TinyImageNet",
          "metric_name": "Accuracy",
          "value": 0.641,
          "provenance": {"quote": "64.1% on TinyImageNet", "source": "text"},
          "confidence": 0.9
        }
      ]
    }
    ```

    ---

    ## üì• Now process the following:

    **paper_id**: `{paper_id}`  
    **paper_title**: `{paper_title}`

    **Experiment Sections**:
    ```
    {experiment_text}
    ```
  merge_baselines: |
    You are given a list of baseline method entries extracted from multiple research papers. Your task is to merge these entries into a **single unified list** of canonical baseline methods.

    ---

    ## üéØ Goal

    For each baseline method, create a unified entry with:

    * `canonical_name`
    * Collected `aliases`
    * Merged metadata (`category`, `description`)
    * `supporting_papers` for provenance
    * `usage_frequency` = number of distinct papers referencing it

    ---

    ## üßæ Input Format

    ```json
    [
      {
        "paper_id": "abc123",
        "baselines": [
          {
            "name": "XGBoost",
            "description": "Gradient boosting decision tree method.",
            "category": "classical ML"
          },
          {
            "name": "BERT",
            "description": "Transformer-based pretrained language model.",
            "category": "deep learning"
          }
        ]
      },
      {
        "paper_id": "xyz456",
        "baselines": [
          {
            "name": "xgb",
            "description": "Boosted tree method widely used for tabular tasks.",
            "category": "classical ML"
          }
        ]
      }
    ]
    ```

    Each block contains:

    * `paper_id` ‚Äì identifier of the paper
    * `baselines` ‚Äì list of baseline method entries

    Each baseline entry has at least a `name` and may include:

    * `description`
    * `category`

    ---

    ## üì¶ Output Format

    ```json
    {
      "baselines": [
        {
          "canonical_name": "xgboost",
          "aliases": ["XGBoost", "xgb"],
          "description": "Gradient boosting decision tree method.",
          "category": "classical ML",
          "supporting_papers": ["abc123", "xyz456"],
          "usage_frequency": 2
        },
        {
          "canonical_name": "bert",
          "aliases": ["BERT"],
          "description": "Transformer-based pretrained language model.",
          "category": "deep learning",
          "supporting_papers": ["abc123"],
          "usage_frequency": 1
        }
      ]
    }
    ```

    ---

    ## ‚úÖ Instructions

    * Normalize names to create `canonical_name`: lowercase, remove punctuation, unify hyphen/underscore.
    * Merge entries with the same `canonical_name`.
    * For conflicting descriptions or categories, keep the one supported by more papers.
    * Merge `supporting_papers` and count unique `paper_id`s for `usage_frequency`.
    * Preserve all observed surface forms in `aliases`.

    Return only valid JSON in the output:

    ```json
    {"baselines": [ ... ]}
    ```

    Below is the input:
    [Baseline_input]

  merge_datasets: |
    You are given a collection of dataset entries extracted from multiple sources. Each source provides a **`paper_id`** and its corresponding list of **`datasets`**. Your task is to merge all dataset entries across papers into a **single unified list**.

    ---

    ## üéØ Goal

    For each dataset, generate a single canonical entry by:

    * Unifying different surface names into one `canonical_name`
    * Collecting all observed `aliases`
    * Merging non-conflicting fields
    * Preserving provenance via `supporting_papers`
    * Computing `usage_frequency` as the number of distinct papers referencing it

    ---

    ## üßæ Input Format

    ```json
    [
      {
        "paper_id": "abc123",
        "datasets": [
          {
            "name": "Example-Dataset",
            "description": "A sample dataset used for testing.",
            "task_type": ["classification"],
            "processing": "Preprocessed with tokenization and normalization.",
            "confidence": 0.95
          },
          {
            "name": "SampleData",
            "description": "Another dataset for benchmarking purposes.",
            "task_type": ["regression"],
            "confidence": 0.9
          }
        ]
      },
      {
        "paper_id": "xyz456",
        "datasets": [
          {
            "name": "Ex_DS",
            "description": "Structured dataset commonly used for evaluation.",
            "task_type": ["classification"],
            "confidence": 0.92
          }
        ]
      }
    ]
    ```

    Each paper block contains:

    * `paper_id` ‚Äì unique identifier of the paper
    * `datasets` ‚Äì a list of dataset entries mentioned in that paper

    Each dataset entry has at least a `name` and may include:

    * `description`
    * `profile` (e.g., `size`, `attributes`, `categories`, etc.)
    * `task_type`
    * `url`
    * `processing`
    * `confidence`

    ---

    ## üì¶ Output Format

    ```json
    {
      "datasets": [
        {
          "canonical_name": "example_dataset",
          "aliases": ["Example-Dataset", "Ex_DS"],
          "description": "A sample dataset used for testing.",
          "profile": { ... },
          "task_type": ["classification"],
          "url": "...",
          "processing": "Preprocessed with tokenization and normalization.",
          "supporting_papers": ["abc123", "xyz456"],
          "usage_frequency": 2
        },
        {
          "canonical_name": "sampledata",
          "aliases": ["SampleData"],
          "description": "Another dataset for benchmarking purposes.",
          "task_type": ["regression"],
          "supporting_papers": ["abc123"],
          "usage_frequency": 1
        }
      ]
    }
    ```

    ---

    ## ‚úÖ Instructions

    * Normalize names to create `canonical_name`: lowercase, remove punctuation, collapse whitespace, unify hyphen/underscore.
    * Merge entries with the same `canonical_name`.
    * For conflicting descriptions, keep the one supported by more distinct papers.
    * Merge `supporting_papers` and count unique `paper_id`s for `usage_frequency`.
    * Preserve all observed surface forms in `aliases`.

    Return only valid JSON in the output.

    Below is the input:
    [Dataset_input]

  merge_metrics: |
    You are given a list of metric definitions extracted from multiple research papers. Your task is to merge them into a **single unified list** of metrics.

    ---

    ## üéØ Goal

    For each metric:

    * Normalize to a `canonical_name`
    * Collect all observed surface names as `aliases`
    * Merge non-conflicting fields (`description`, `formulation`, `task_scope`)
    * Preserve provenance in `supporting_papers`
    * Compute `usage_frequency` as the number of distinct papers mentioning it

    ---

    ## üßæ Input Format

    ```json
    [
      {
        "paper_id": "abc123",
        "metrics": [
          {
            "name": "RMSE",
            "description": "Root Mean Square Error, measures average magnitude of error.",
            "formulation": "sqrt(mean((y - y_hat)^2))",
            "task_scope": ["regression"]
          },
          {
            "name": "Accuracy",
            "description": "Proportion of correct predictions out of total predictions.",
            "formulation": "(TP + TN) / (TP + TN + FP + FN)",
            "task_scope": ["classification"]
          }
        ]
      },
      {
        "paper_id": "xyz456",
        "metrics": [
          {
            "name": "Root Mean Square Error",
            "description": "Measures deviation between predicted and observed values.",
            "formulation": "sqrt(Œ£(y - y_hat)^2 / n)",
            "task_scope": ["recommendation"]
          }
        ]
      }
    ]
    ```

    Each block contains:

    * `paper_id` ‚Äì unique identifier of the paper
    * `metrics` ‚Äì list of metric entries extracted from that paper

    Each metric entry has at least a `name` and may include:

    * `description`
    * `formulation`
    * `task_scope`

    ---

    ## üì¶ Output Format

    ```json
    {
      "metrics": [
        {
          "canonical_name": "rmse",
          "aliases": ["RMSE", "Root Mean Square Error"],
          "description": "Root Mean Square Error, measures average magnitude of error.",
          "formulation": "sqrt(mean((y - y_hat)^2))",
          "task_scope": ["regression", "recommendation"],
          "supporting_papers": ["abc123", "xyz456"],
          "usage_frequency": 2
        },
        {
          "canonical_name": "accuracy",
          "aliases": ["Accuracy"],
          "description": "Proportion of correct predictions out of total predictions.",
          "formulation": "(TP + TN) / (TP + TN + FP + FN)",
          "task_scope": ["classification"],
          "supporting_papers": ["abc123"],
          "usage_frequency": 1
        }
      ]
    }
    ```

    ---

    ## ‚úÖ Instructions

    * Normalize names to create `canonical_name`: lowercase, trim, remove punctuation, unify hyphen/underscore.
    * Merge entries with the same `canonical_name`.
    * For conflicting descriptions/formulations, keep the one supported by more distinct papers.
    * Merge `task_scope` lists and deduplicate.
    * Merge `supporting_papers` and count unique `paper_id`s for `usage_frequency`.
    * Preserve all observed surface names in `aliases`.

    Return only valid JSON in the output:

    ```json
    {"metrics": [ ... ]}
    ```

    Below is the input:
    [Metric_input]


              
summary_generator: |
  You are given the full content of a single section from a research paper. Your task is to **summarize the content of this section only**.

  **Instructions:**

  * Write a concise summary (2‚Äì4 sentences) that captures the main purpose, arguments, methods, or findings in this section.
  * If the section has subsections, integrate their content into one coherent summary.
  * Do not copy sentences verbatim; rephrase in clear academic language.
  * **Only output the summary text. Do not include labels, JSON, formatting, or any additional commentary.**

  **Input:**
  [Section Content]

  **Output:**
  [Summary of that section only]

KB_functions:
  Experiment_settings_recommendation: |
    \[ROLE DESCRIPTION]
    You are an expert research experiment designer. Your task is to analyze canonical experiment metadata (datasets, baselines, and metrics) collected from multiple papers in a given research topic and recommend the **most suitable experiment settings** for a new research paper in that topic.

    \[INPUTS]
    You will be given:

    1. **Canonical Datasets (`{canonical_datasets}`):** A structured list of datasets, their aliases, descriptions, processing details, usage frequency, and supporting papers.
    2. **Canonical Baselines (`{canonical_baselines}`):** A structured list of baselines, their aliases, descriptions, categories, usage frequency, and supporting papers.
    3. **Canonical Metrics (`{canonical_metrics}`):** A structured list of evaluation metrics, their aliases, descriptions, formulations, usage frequency, and supporting papers.

    \[PROCESSING INSTRUCTIONS]

    1. **Dataset Selection**

      * Recommend datasets that are **widely adopted** (higher usage frequency) and **representative** of the topic.
      * Ensure coverage across different dataset characteristics (e.g., scale, modality, or diversity).
      * Mark the **top three most recommended datasets** as `recommended = True`, others as `recommended = False`.

    2. **Baseline Selection**

      * Recommend a **balanced set** of baselines, including:

        * **Classical approaches** for traditional comparison.
        * **Recent state-of-the-art methods** for benchmarking.
        * **Alternative paradigms** (unsupervised, graph-based, LLM-based, etc.) if frequently used or highlighted.
      * Mark the **top three most recommended baselines** as `recommended = True`, others as `recommended = False`.

    3. **Metric Selection**

      * Always include **core evaluation metrics** commonly used in the topic.
      * Add additional metrics if they are **task- or setting-specific**.
      * Mark the **top three most recommended metrics** as `recommended = True`, others as `recommended = False`.

    4. **Output Formatting**

      * For each dataset, baseline, and metric, output objects in the following format:

        ```json
        {
          "name": "<canonical_name>",
          "description": "<description>",
          "usage_count": <integer>,
          "recommended": true/false
        }
        ```
      * Wrap all results in a single JSON object with the following structure:

        ```json
        {
          "recommended_datasets": [ ... ],
          "recommended_baselines": [ ... ],
          "recommended_metrics": [ ... ],
          "rationale": "Explanation of why the top 3 in each category were marked as recommended."
        }
        ```

    \[OUTPUT]
    Only output the JSON object, no extra explanation. **The size of the recommended datasets, baselines, and metrics should be less than 5.**

    Below is the input:
    [Experiment_settings_recommendation_input]








