chunker:
  section_labeling: |
    You are given a list of section titles from a single research paper. Categorize each title into one of the following:

    ["Abstract", "Introduction", "Problem Definition", "Methodology", "Related Work", "Experiment"]

    If a title does not clearly fit any category, assign it to the most likely one based on typical academic paper structure. Multiple titles can belong to the same category. But one title should not belong to multiple categories.

    Please output only the raw JSON object without any explanation or formatting ‚Äî do not wrap the output in triple backticks or add a language label. Format: 
    {
      "Abstract": [titles assigned to Abstract],
      "Introduction": [titles assigned to Introduction],
      "Problem Definition": [titles assigned to Problem Definition],
      "Methodology": [titles assigned to Methodology],
      "Related Work": [titles assigned to Related Work],
      "Experiment": [titles assigned to Experiment],
      "Conclusion": [titles assigned to Conclusion],
      "References": [titles assigned to References],
      "Acknowledgement": [titles assigned to Acknowledgement],
      "Appendix": [titles assigned to Appendix]
    }

    Input:
    [Title List]
    Output:


taxonomy_generator:
  task_taxonomy:
    extract_problem_definition: |
      You are acting as a **research assistant** tasked with analyzing a research paper to extract its **core research problem** in a **structured**, **methodology-agnostic** format.

      Your objective is to **identify or infer the problem definition** and decompose it into precise components that describe **what the problem is**, **without referencing how it is solved**. Do not include any mention of algorithms, models, frameworks, training strategies, or implementation details.

      If the paper uses shorthand (e.g., acronyms, variables, or mathematical notation), refer to the preliminaries or notation sections to rewrite the problem in a **clear and unambiguous** manner.

      ---

      ### ‚úÖ Output Format

      ```yaml
      paper_id: "<Title of the paper or name of the proposed method>"

      problem_formulation:
        simple_description: >
          "<One-sentence summary of the problem, or `None` if not found.>"

        formal_definition:
          input: >
            "<Describe the input, including its type, structure, and properties. Mention hardware constraints only if they are explicitly part of the problem setting‚Äîdo NOT infer hardware requirements from experimental setups.>"
          output: >
            "<Describe the desired output, including type, structure, and expected characteristics.>"
      ```

      ---

      ### üß≠ Step-by-Step Instructions

      #### üîπ Step 1: Generate `simple_description`

      Create a concise, self-contained summary of the problem using this template:

      > **"Given <input>, the goal is to obtain <output> by achieving <objective>."**

      Follow this order when searching for the problem statement:

      1. Dedicated **Problem Definition** section
      2. **Introduction**
      3. **Abstract**
      4. Relevant parts of the **Methodology** *(only for high-level task description‚Äînot implementation details)*
      5. Any other section as necessary

      > If no coherent one-sentence summary can be extracted, return:

      ```yaml
      simple_description: None
      ```

      > ‚úÖ When a paper describes multiple related problems, prioritize the **primary** or **most general** problem formulation unless others are clearly independent.

      ---

      #### üîπ Step 2: Generate `formal_definition`

      Decompose the problem into the following structured components:

      | Field      | What to Describe                                                                                                                                                                                                                                                                         |
      | ---------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | **input**  | The data or contextual information provided to the system. Include type, format, and any explicitly stated constraints (e.g., memory, disk). **Do not treat the hardware used in experimental settings as input constraints unless they are explicitly part of the problem definition.** |
      | **output** | The expected result, product, or prediction. Include type and structure. Do not include how it is computed.                                                                                                                                                                              |

      > Leave any field blank (`""`) if it cannot be inferred from the paper.

      ---

      ### üö´ Do Not Include:

      * Specific methods, algorithms, architectures, or procedural details
      * Training objectives, loss functions, or optimization strategies
      * System or model names
      * Hardware specifications from experiments (e.g., GPUs, clusters), **unless explicitly part of the problem setting**

      ---

      ### ‚ö†Ô∏è Common Pitfalls to Avoid

      | Case                                               | Guideline                                                                                                                                                  |
      | -------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | **Hardware in experiments**                        | Never assume experimental hardware is part of the problem input unless **explicitly described** (e.g., ‚Äúthe task must run on edge devices with ‚â§2GB RAM‚Äù). |
      | **Vague or entangled problem-method descriptions** | Isolate the **task definition** only. If it's mixed with solution details, extract only the input‚Äìoutput‚Äìgoal relationship.                                |
      | **Multiple sub-problems**                          | Focus on the **primary problem** or provide a clearly separated list if multiple are independently defined.                                                |
      | **No clear problem statement**                     | Use context from preliminaries, task descriptions, or dataset definitions to infer a possible formulation. If still ambiguous, return `"None"` as needed.  |

      Below is the paper content:
      [Paper Content]

    aspect_classification: |
      You are given a list of problem definitions extracted from a collection of computer science research papers. Each definition includes structured aspects such as:

      * **Input**
      * **Output**

      ---

      ## ‚úÖ Task: Aspect-Wise Classification Process

      Focus on the following two aspects:

      * **Input**
      * **Output**

      For **each aspect**, process its corresponding content independently and repeat the following steps:

      ---

      ### üî• Step 1: Extract Key Entities

      Iterate over each paper. From the content of the current aspect:

      * Identify the main research-related entities (key concepts) that **describe the authors‚Äô formulation of this aspect** and are **explicitly emphasized in `problem_formulation` as central to the problem definition**.
      * If `problem_formulation` lacks sufficient information, refer to the corresponding structured field (**input**, **output**) for clarification.

      Focus on extracting noun phrases that match the aspect‚Äôs definition:

      * **Input**: Describe the input, including its type, structure, and properties. Mention hardware constraints only if they are explicitly part of the problem setting‚Äîdo NOT infer hardware requirements from experimental setups.
      * **Output**: Describe the desired output, including type, structure, and expected characteristics.

      ---

      ### üî• Step 2: Group Entities Within Sublists

      Group entities within the same paper and aspect into **local entity groups**. These groups reflect strong correlations between entities (they appear together in the same problem definition). Build connections among entities in each group to capture their co-occurrence relationships.

      ---

      ### üî• Step 3: Align and Merge Entities Across Sublists

      * **Identify Related Entities:** Merge entities across papers if they are exact matches, semantically equivalent, or containment relationships.
      * **Merge into Unified Nodes:** Example: {"user-item matrix", "rating matrix"} ‚Üí `Interaction Matrix`.
      * **Build Global Entity Groups:** Connected components of unified nodes form candidate classes.

      ---

      ### üî• Step 4: Refine Classes and Classify Papers

      1. **Provide Class Explanations:** For each resulting class, provide a short description of its scope and relevance.
      2. **Drop or Merge Small Classes:** Merge or drop classes with <2 papers.
      3. **Align Class Names:** Use formal research terminology.
      4. **Assign Papers:** Assign each paper to one or more classes per aspect.

      ---

      ### üî• Step 5: Identify Unassigned or Misaligned Papers

      * Detect **unassigned papers** (entities extracted but no class assignment).
      * Detect **misalignments** (entities poorly matched to class descriptions).
      * Suggest corrections (reassignment or new class).

      ---

      ### üî• Step 6: Implement Corrections

      * For each **unassigned or misaligned paper** identified in Step 5, update the classification by:

        * Assigning it to the most appropriate existing class if suggested.
        * Or creating a new class if no suitable class exists.
      * Ensure that the **final classification output incorporates these corrections directly**, so that all papers have appropriate assignments.

      ---

      ## üìã Output Format

      Return the results **strictly in valid JSON format** with the following structure:

      ```json
      {
        "input_classification": {
          "classes": [
            {
              "class_name": "User-Item Matrix",
              "class_description": "Matrix representation of user-item interactions, commonly used in recommendation tasks.",
              "papers": ["Paper A", "Paper C"]
            },
            {
              "class_name": "Knowledge Graph",
              "class_description": "Graph-based structured input capturing relational facts.",
              "papers": ["Paper B", "Paper F"]
            },
            {
              "class_name": "Temporal Sequence Data",
              "class_description": "Sequential data used for modeling time-dependent behaviors.",
              "papers": ["Paper G"]   // Correction applied here
            }
          ]
        },
        "output_classification": {
          "classes": [
            {
              "class_name": "Hash Codes",
              "class_description": "Compact binary representations of input data used for efficient retrieval.",
              "papers": ["Paper A", "Paper D"]
            },
            {
              "class_name": "Ranked List",
              "class_description": "Ordered set of relevant results returned for a given query.",
              "papers": ["Paper B", "Paper E"]
            }
          ]
        }
      }
      ```
      Below is the problem definition list:
      [Problem Definitions]

    taxonomy_generation: |
      You are an expert research taxonomist. You are given one **JSON object**, containing the classification result of the problem aspects:

      * **Input**: The nature or structure of data the research problem takes as input
      * **Output**: The expected result, format, or structure produced by the method.

      ---

      ### üì• Input Format

      Each JSON object contains an array of class entries. Each entry has the following fields:

      ```json
      {
        "class_name": "string",
        "class_description": "string",
        "supporting_papers": ["PaperA", "PaperB", ...]
      }
      ```

      Below is the classification result:
      [Classification Result]

      ---

      ## ‚úÖ Goal

      Construct a **single hierarchical taxonomy of meaningful research tasks**, where each task is defined by a combination of **Input** and **Output** classes.

      Each node in the taxonomy must represent a **specific, expressive, and well-scoped research task**, not just a data structure or format.

      ---

      ### üî• Step 1: Normalize and Refine Class Labels

      1. **Normalize Class Names:** Use standard academic terminology where possible. Resolve informal, ambiguous, or overly generic names using their class descriptions and example papers.
      2. **Merge Equivalent or Contained Classes:** If two classes are synonyms or exhibit strict containment (e.g., ‚Äúuser-item matrix‚Äù ‚äÜ ‚Äúinteraction matrix‚Äù), merge them.
      3. **Drop or Collapse Weak Classes:** Drop classes with too few or overly vague supporting papers unless they can be merged into broader categories.

      **Output:** Refined JSON list of input and output classes.

      ---

      ### üî• Step 2: Form Meaningful Task Nodes by Input‚ÄìOutput Pairing

      1. **Pair Input and Output Classes:** For every (Input, Output) pair, check if it defines a **coherent, meaningful research task**.
      2. **Keep only valid task nodes** that meet all three criteria:

        * The pair appears in at least **2 different papers**
        * The pairing reflects a **common and recognizable research objective**
        * The task name can be made **expressive** (not just ‚ÄúMatrix ‚Üí Vector‚Äù).
      3. **Name Each Task:**
        Use the format:

        ```
        TASK:L{level}:{CANONICAL_NAME}
        ```

      ---

      ### üî• Step 3: Build a Hierarchy over the Tasks

      1. **Group Similar Tasks:**

        * Parent-child relationships based on task granularity and generalization.
        * Example:

          * Parent: `KNOWLEDGE_GRAPH_TASKS`

            * Child: `KNOWLEDGE_GRAPH_EMBEDDING`
            * Child: `KNOWLEDGE_GRAPH_COMPLETION`

      2. **Hierarchy Rules:**

        * Use only **IS-A** relationships.
        * Construct a **tree** (no DAGs or cycles).
        * Root node: `ROOT`.

      ---

      ## üìã Output Format (JSON Only)

      Return the taxonomy strictly as **valid JSON**. Each node must have:

      * `task_id`: Unique ID in the format `TASK:L{level}:{CANONICAL_NAME}`
      * `task_name`: Expressive descriptive name
      * `input_class`: Input class name
      * `output_class`: Output class name
      * `explanation`: 1‚Äì2 sentence explanation of the task
      * `papers`: List of supporting papers
      * `children`: Array of child task nodes

      Example:

      ```json
      {
        "taxonomy": {
          "task_id": "ROOT",
          "task_name": "ROOT",
          "children": [
            {
              "task_id": "TASK:L1:KNOWLEDGE_GRAPH_COMPLETION",
              "task_name": "Knowledge Graph Completion",
              "input_class": "Knowledge Graph",
              "output_class": "Completed Triples",
              "explanation": "Predict missing edges in a relational graph based on observed triples.",
              "papers": ["PaperA", "PaperD"],
              "children": []
            },
            {
              "task_id": "TASK:L1:HASH_CODE_GENERATION_FOR_RETRIEVAL",
              "task_name": "Hash Code Generation for Retrieval",
              "input_class": "User-Item Interaction Matrix",
              "output_class": "Hash Codes",
              "explanation": "Generate compact binary representations for fast retrieval in recommendation settings.",
              "papers": ["PaperB", "PaperF"],
              "children": []
            }
          ]
        }
      }
      ```

      * Ensure the JSON is **syntactically valid**.
      * Do not output Markdown, tables, or text outside the JSON block.

      ---
  method_taxonomy:
    extract_method_summary: |
      [ROLE]
      You are an AI research methodology analyst specializing in decomposing scientific methods. You interpret textual descriptions and HTML table data associated with figures.

      [TASK]
      Analyze `{section_text}` containing methodology descriptions and potential HTML tables. Generate an integrated Markdown summary using this workflow:

      1. **Method Decomposition**  
      - Extract core components, workflow sequence, and objectives
      - Represent components in a Markdown table with columns:  
          `Component | Function | Inputs | Outputs`
      - Map workflow steps in a numbered list showing transitions

      2. **Figure Analysis**  
      - Identify referenced figures (e.g., "Figure 1 shows...")
      - For each figure:  
          ‚Ä¢ Locate descriptive text  
          ‚Ä¢ Parse associated HTML tables if present  
          ‚Ä¢ Create analysis table with columns:  
          `Figure Ref | Element Type | Key Relationships | Data Insights`

      3. **Integration**  
      - Cross-reference figures with method components using bold tags (e.g., **ComponentX**)
      - Embed figure insights directly in workflow steps
      - Fuse objectives with supporting evidence from tables

      [OUTPUT RULES]
      ```markdown
      ### Core Components
      | Component | Function | Inputs | Outputs |
      |-----------|----------|--------|---------|
      | ...       | ...      | ...    | ...     |

      ### Workflow Sequence
      1. [Step 1] ‚Üí **(Figure X)**  
      - [Action] using [Component]  
      - *Table insight: [Value] from Table Y*

      ### Objectives
      - [Goal 1] achieved through [Component/Step]  
      *(Supported by Figure Z: [Observation])*

      ### Figure Analysis
      | Fig Ref | Element Type | Key Relationships | Data Insights |
      |---------|--------------|-------------------|---------------|
      | Fig 1   | [Type]       | [Connection]      | [Table Data]  |
      ```
      **Critical Constraints**  
      - Use ONLY standard Markdown (no LaTeX, HTML, or images)  
      - Represent mathematical concepts verbally (e.g., "loss minimization")  
      - Maximum 2 sentences per table cell  
      - Omit redundant explanations already covered in tables  
      - Bold method elements when linked to figures (**ComponentA**)  
      - Prioritize table representations over paragraphs  
    extract_pros_and_cons: |
      [ROLE DESCRIPTION]
      You are an expert research analyst specializing in cross-study methodology comparison. You excel at synthesizing structured data to identify shared patterns and distinctive features across research papers.

      [TASK DESCRIPTION]
      Analyze methodology analyses from multiple research papers. Identify common/unique advantages and limitations, then synthesize findings into a structured Markdown comparison table. Prioritize visual clarity and information density.

      [INPUT]
      - **Methodology Data (MD Format):** Contains one or more methodology analyses in flexible markdown format.  
      **Each analysis corresponds to a single paper's methodology**.
      input: `{method_summaries}`

      [PROCESSING INSTRUCTIONS]
      1. **Data Extraction:**
        - Extract JSON from Markdown code blocks (prioritize ```json tagged blocks)
        - Parse into array of paper objects: `papers = [{{paper_id, methodology: {{name, description, pros, cons}}}}, ...]`

      2. **Cross-Analysis:**
        ```python
        # Aggregate metrics
        all_pros = flatten(paper.methodology.pros for paper in papers)
        all_cons = flatten(paper.methodology.cons for paper in papers)
        
        # Identify common elements (appearing in ‚â•2 papers)
        common_pros = [p for p in set(all_pros) if count_occurrences(p) >= 2]
        common_cons = [c for c in set(all_cons) if count_occurrences(c) >= 2]
        
        # Map unique elements per paper
        for paper in papers:
            paper["unique_pros"] = [p for p in paper.methodology.pros if p not in common_pros]
            paper["unique_cons"] = [c for c in paper.methodology.cons if c not in common_cons]
        ```

      3. **Output Structure:**
        ```markdown
        # Methodology Comparison Report

        ## Key Commonalities
        | Aspect       | Shared Findings                          |
        |--------------|------------------------------------------|
        | **Pros** | {{bullet_list(common_pros)}}               |
        | **Cons** | {{bullet_list(common_cons)}}               |
        *Note: Appear in ‚â•2 papers*

        ## Paper-Level Analysis
        | Paper ID | Method Name     | Description | Common Pros | Unique Pros | Common Cons | Unique Cons |
        |----------|-----------------|-------------|-------------|-------------|-------------|-------------|
        {{generate_table_rows}}
        ```

      4. **Table Generation Rules:**
        - **Header:** Use exact column names above
        - **Cell Formatting:**
          - `Description`: First 50 words + "..." if truncated
          - Pros/Cons columns: Bullet lists with line breaks (`<br>‚Ä¢ `)
          - Empty cells: "‚Äî" when no items
        - **Sorting:** Order papers by method name alphabetically
        - **Special Cases:**
          - Single paper input: Omit "Key Commonalities" section
          - Empty lists: Display "‚Äî" in table cells

      [OUTPUT REQUIREMENTS]
      1. Strict Markdown format within ```markdown code fence
      2. Table must contain all papers from input
      3. Preserve original text annotations in all pros/cons
      4. Escape Markdown special characters in source text
      5. Include footer note: "*Unique aspects highlight methodological distinctions*"
    taxonomy_generation: "[ROLE DESCRIPTION]\nYou are an expert research methodology
      taxonomist. Your task is to synthesize information from multiple sources about
      a set of research papers and generate a single, hierarchical classification
      tree in a strict JSON format.\n\n[TASK DESCRIPTION]\nAnalyze the provided data
      blocks to build a multi-layered taxonomy of research methodologies. Use the
      detailed methodology summaries and the pros/cons report as the primary basis
      for fine-grained technical classification. Use the introductions and related
      work sections (if provided) for broader, high-level contextual grouping. The
      final output must be a valid JSON object representing the classification tree.\n
      \n[INPUTS]\nYou will be provided with the following data blocks. Some blocks
      (like introductions and related works) are optional and may be empty.\n\n1.\
      \  **Methodology Summaries (`{method_summaries}`):** A JSON object mapping paper
      filenames to their detailed, structured methodology summaries. This is your
      primary source for technical details.\n2.  **Pros & Cons Report (`{pros_cons_summary}`):**
      A Markdown report containing a comparative analysis of the advantages and limitations
      found across the methods. Use this to identify key trade-offs and distinguishing
      features.\n3.  **Introductions (`{introductions}`):** (Optional) A JSON object
      mapping paper filenames to their introduction sections. Use this to understand
      the core problem, motivation, and high-level claims of each paper, which can
      help in forming top-level categories.\n4.  **Related Works (`{related_works}`):**
      (Optional) A JSON object mapping paper filenames to their related work sections.
      Use this to understand how each paper positions itself against others, which
      aids in determining its methodological paradigm.\n\n[CORE INSTRUCTIONS FOR TAXONOMY
      GENERATION]\n\n1.  **Identify Papers:** The paper filenames (e.g., \"paper1.md\"\
      ) are the keys in the `{method_summaries}` and other JSON objects. Use these
      filenames as the unique identifiers for each paper. For the `index` field in
      the output, convert these filenames to a \"P\" format (e.g., extract the number
      from \"paperX.md\" to create \"PX\").\n\n2.  **Build Taxonomy Tree (Hierarchical
      Classification):**\n    * **Root Node:** The top-level node must be named \"\
      Methodology Taxonomy\" and contain all papers in its index.\n    * **Layer 1
      (Fundamental Paradigm):** First, partition all papers into **mutually exclusive**
      high-level categories based on their fundamental research goal or paradigm.
      Examples of such high-level categories include **\"Predictive Modeling\", \"\
      Causal Inference\", \"Generative Methods\", \"System Architecture & Design\"\
      , or \"Theoretical Analysis\"**.\n    * **Layer 2 (Core Technique Family):**
      Subdivide each high-level group based on the core family of techniques employed.
      Examples: **\"Neural Network Architectures\", \"Statistical & Probabilistic
      Methods\", \"Optimization Algorithms\", \"Novel System Components\"**.\n   \
      \ * **Layer 3+ (Distinctive Features):** If necessary, create deeper layers
      to refine the classification based on unique characteristics. Examples: **\"\
      Supervised vs. Unsupervised Learning\", \"Adaptive Components\", \"Scalability
      Enhancements\"**. The `{pros_cons_summary}` is very useful here.\n    * **Leaf
      Nodes:** The final nodes of the tree must be the individual papers.\n\n3.  **Node
      Schema Requirements:**\n    * **Intermediate Nodes (Categories):**\n       \
      \ ```json\n        {{\n          \"name\": \"<Classification Label>\",\n   \
      \       \"content\": \"1) Grouping feature: [Explicit shared characteristic]<br>2)
      Child differences: [Distinguishing aspects between subgroups]\",\n         \
      \ \"index\": [\"P1\", \"P4\"],\n          \"children\": [ ...subnodes... ]\n\
      \        }}\n        ```\n    * **Leaf Nodes (Papers):**\n        ```json\n\
      \        {{\n          \"name\": \"<Exact Paper Filename>\",\n          \"content\"\
      : \"<Provide a 2-3 sentence, concise summary of the paper's specific methodology,
      derived from the input summaries.>\",\n          \"index\": \"<Single PaperID,
      e.g., P3>\",\n          \"children\": []\n        }}\n        ```\n\n[VALIDATION
      RULES & CONSTRAINTS]\n* **Completeness:** Every paper from the input must appear
      as a leaf node exactly once.\n* **Strict Partitioning:** A paper cannot appear
      in more than one branch of the tree. The classification must be **mutually exclusive
      at all levels.**\n* **Hierarchy:** Children must be strict semantic subsets
      of their parents.\n* **Specificity:** Each new layer must introduce a more specific,
      distinguishing dimension.\n* **Fidelity:** Preserve original technical terms.
      Do not generalize or rename concepts.\n* **No Generic Bins:** Do not create
      vague categories like \"Other\", \"Miscellaneous\", or \"General Approaches\"\
      . Every group must have a clear, defining characteristic.\n* **Node Collapsing:**
      If an intermediate node has only one child, it must be merged to prevent redundant
      layers. **Create a new, semantically meaningful name for the merged node that
      accurately represents the more specific category.** For example, if a **'Learning-based
      Approaches'** node has only one child **'Supervised Methods'**, the new merged
      node could be named **'Supervised Learning Approaches'**. The new node adopts
      the children of the original child.\n\n[OUTPUT REQUIREMENTS]\n- Provide ONLY
      the final JSON object enclosed in a ```json code fence.\n- Do not include any
      additional text, notes, or explanations before or after the JSON block.\n- Ensure
      the output is a single, syntactically valid JSON object."






