# LLM Configuration File
# This file manages configurations for different LLM providers

llm_providers:
  openai:
    enabled: true
    api_key: ${OPENAI_API_KEY}  # Use environment variable
    base_url: "https://api.openai.com/v1"
    default_model: "gpt-4o-2024-08-06"
    models:
      - name: "gpt-4-turbo-preview"
        max_tokens: 4096
        temperature: 0.3
        top_p: 1.0
        frequency_penalty: 0
        presence_penalty: 0
      - name: "gpt-4.1"
        max_tokens: 4096
        temperature: 0.3
        top_p: 1.0
        frequency_penalty: 0
        presence_penalty: 0
      - name: "gpt-4"
        max_tokens: 8192
        temperature: 0.3
      - name: "gpt-3.5-turbo"
        max_tokens: 4096
        temperature: 0.3
      - name: "gpt-4o-2024-08-06"
        max_tokens: 10000
        temperature: 0.3
      - name: "gpt-4o-mini"
        max_tokens: 10000
        temperature: 0.3
    timeout: 60
    max_retries: 3
    retry_delay: 1

  google:
    enabled: true
    api_key: ${GOOGLE_API_KEY}
    base_url: "https://generativelanguage.googleapis.com/v1beta"
    default_model: "gemini-pro"
    models:
      - name: "gemini-pro"
        max_output_tokens: 2048
        temperature: 0.7
        top_p: 0.8
        top_k: 40
      - name: "gemini-pro-vision"
        max_output_tokens: 2048
        temperature: 0.4
    timeout: 60
    max_retries: 3
    retry_delay: 1

  anthropic:
    enabled: true
    api_key: ${ANTHROPIC_API_KEY}
    base_url: "https://api.anthropic.com/v1"
    default_model: "claude-3-opus-20240229"
    models:
      - name: "claude-3-opus-20240229"
        max_tokens: 4096
        temperature: 0.7
        top_p: 1.0
        top_k: 40
      - name: "claude-3-sonnet-20240229"
        max_tokens: 4096
        temperature: 0.7
      - name: "claude-3-haiku-20240307"
        max_tokens: 4096
        temperature: 0.7
    timeout: 60
    max_retries: 3
    retry_delay: 1

  grok:
    enabled: true
    api_key: ${GROK_API_KEY}
    base_url: "https://api.x.ai/v1"
    default_model: "grok-beta"
    models:
      - name: "grok-beta"
        max_tokens: 4096
        temperature: 0.7
        top_p: 1.0
        frequency_penalty: 0
        presence_penalty: 0
    timeout: 60
    max_retries: 3
    retry_delay: 1

# Default settings applied to all providers unless overridden
defaults:
  temperature: 0.7
  max_tokens: 2048
  timeout: 30
  max_retries: 3
  retry_delay: 1
  stream: false

# Rate limiting configuration
rate_limiting:
  enabled: true
  requests_per_minute: 60
  tokens_per_minute: 90000
  concurrent_requests: 10

# Logging configuration
logging:
  level: INFO  # DEBUG, INFO, WARNING, ERROR, CRITICAL
  log_file: "llm_factory.log"
  log_to_console: true
  log_api_calls: true
  log_responses: false  # Set to true to log full responses (careful with sensitive data)