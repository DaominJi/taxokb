{
    "Introduction": {
        "content": "Entity matching (EM) finds data instances that refer to the same real-world entity. Most EM solutions proceed in two steps: blocking and matching. Given two tables $A$ and $B$ to match, the blocking step uses heuristics to quickly remove tuple pairs $(a \\in A, b \\in B)$ judged unlikely to match. The matching step then applies a matcher to the remaining tuple pairs to predict match/no-match.\n\nBoth the blocking and matching steps have received significant attention (e.g., $[1,5-7,12,14,26,27,30,32]$ ). In this paper we focus on the blocking step. Over the past 30 years, numerous blocking solutions have been developed. The goal is to maximize recall (the\n\n[^0]fraction of true matches that survive blocking) while minimizing the output size and the runtime (see Section 2). In the past few years, as a part of the Magellan project at UW-Madison, which develops a comprehensive open-source EM platform [19], we have implemented many of the proposed blocker types, develop new blocker types [38], and applied them to many real-world EM tasks in domain sciences and industry [16]. While doing this, we found that a relatively simple blocking solution that uses the $\\mathrm{tf} / \\mathrm{idf}$ similarity measure, as implemented in the open-source Apache Lucene library, seems to work quite well.\n\nThis is rather surprising because as far as we can tell, $\\mathrm{tf} / \\mathrm{idf}$ based blocking has received virtually no attention. For example, the book \"Data Matching\" [6] and several recent EM surveys [14, 32] do not discuss any $\\mathrm{tf} / \\mathrm{idf}$ solutions for blocking, and we are not aware of any recent work proposing $\\mathrm{tf} / \\mathrm{idf}$ solutions. Yet we found $\\mathrm{tf} / \\mathrm{idf}$ blocking highly promising in many informal experiments.\n\nAs a result, in this paper we perform an in-depth examination of $\\mathrm{tf} / \\mathrm{idf}$ blocking. We begin by developing a solution called Sparkly Manual, which takes as input two tables $A$ and $B$ with the same schema, and outputs tuple pairs $(a \\in A, b \\in B)$ judged likely to match. There are two key ideas underlying Sparkly Manual. First, it performs top-k blocking. For each tuple $t$ of the larger table, say $B$, it finds the top $k$ tuples in $A$ with the highest $\\mathrm{tf} / \\mathrm{idf}$ scores ( $k$ is pre-specified), then pairs these tuples with $t$ and outputs the pairs.\n\nSecond, Sparkly Manual performs the above top-k computations in a distributed shared-nothing fashion, using Lucene on a Spark cluster (hence the name Sparkly, which stands for Spark + Lucene + Python). Specifically, it uses Lucene to build an inverted index $I$ for table $A$ on the driver node of the Spark cluster, ships the index $I$ to all worker nodes, distributes the tuples of table $B$ to the worker nodes, then uses Lucene to perform top-k computations for the tuples at the worker nodes. Thus, the worker nodes operate in parallel and share no dependencies. Each node processes a subset of tuples in $B$.\n\nWe compare Sparkly Manual with 8 state-of-the-art (SOTA) blockers on 15 datasets that have been extensively used in recent EM work [25, 29, 38]. Surprisingly, Sparkly Manual outperforms all of the above blockers. It achieves higher or comparable recall at a much smaller output size, and the performance gap is quite significant in several cases (see the experiment section).\n\nWhile appealing, Sparkly Manual has a limitation. It requires the user to manually identify the attributes to be blocked on, e.g., product title, or name and phone. Then it computes the $\\mathrm{tf} / \\mathrm{idf}$ score between any two tuples $a \\in A, b \\in B$ using only these attributes, after 3-gram tokenization.\n\nIt can be difficult for users to identify good blocking attributes. So we develop Sparkly Auto, which automatically identifies a set of\n\n\n[^0]:    This work is licensed under the Creative Commons BY-NC-ND 4.0 International License. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of this license. For any use beyond those covered by this license, obtain permission by emailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights licensed to the VLDB Endowment.\n    Proceedings of the VLDB Endowment, Vol. 16, No. 6 ISSN 2150-8097. doi:10.14778/3583140.3583163\n\ngood blocking attributes, together with an appropriate tokenizer for each attribute. The key observation underlying Sparkly Auto is that a good blocking attribute helps to discriminate between matches and non-matches. We propose techniques to quantify discriminativeness, then to effectively search a large space for the optimal combination of attributes and tokenizers that maximizes this quantity.\n\nWe show that Sparkly Auto achieves comparable or higher recall than Sparkly Manual, yet runs much faster. In particular, Sparkly Auto can block large datasets at reasonable time and cost, e.g., blocking tables of 10 M tuples under 100 minutes on an AWS cluster of 10 commodity nodes, costing only $\\$ 12.5$, and blocking tables of 26 M tuples under 130 minutes on an AWS cluster of 30 nodes, costing $\\$ 67.5$. This suggests that Sparkly Auto can already be practical for many real-world EM problems.\n\nWe conclude by discussing questions that arise in light of Sparkly's strong performance. In summary, the contributions and takeaways of this paper are as follows:\n\n- We develop Sparkly, a tf/idf blocker that uses Lucene to perform top-k blocking on a Spark cluster. We develop techniques to automatically identify good attributes and tokenizers to block on.\n- Extensive experiments show that Sparkly outperforms 8 state-of-the-art blockers. This is rather surprising because tf/idf blocking has received virtually no attention in the past 30 years. The takeaway here is that tf/idf blocking needs more attention, and that Sparkly forms a strong baseline that future blocking work should compare against.\n- We provide an in-depth analysis of Sparkly's performance. The takeaway here is that future blocking work should seriously consider top-k blocking, which helps improve recall, and a distributed share-nothing architecture, which helps improve scalability, predictability, and extensibility.\n- Based on the above analysis, we identify a number of promising research directions for blocking.\n\nFor more information on Sparkly, see [34], which provides the code, all experiment datasets (except Hospital, which is private), and a longer technical report.\n",
        "title": [
            "## 1 INTRODUCTION"
        ],
        "summary": "This section introduces and motivates the use of tf/idf-based blocking for entity matching, a technique that has received little prior attention despite showing strong empirical results. The authors present Sparkly Manual, a distributed top-k blocking system leveraging Lucene and Spark, and demonstrate its superior performance over eight state-of-the-art blockers on multiple datasets. To address the challenge of selecting blocking attributes, they develop Sparkly Auto, which automatically identifies effective attributes and tokenizers, delivering high recall and efficiency even on large datasets. The section concludes by highlighting Sparkly's surprising effectiveness, calling for greater focus on tf/idf and top-k blocking, and outlining future research directions."
    },
    "Problem Definition": {
        "content": "EM, Blocking, Matching: Many EM variations exist [6, 12]. A common EM variation [38], which we consider in this paper, is as follows: given two tables $A$ and $B$ with the same schema, find all tuple pairs $(a \\in A, b \\in B)$ that refer to the same real-world entity. We call these pairs matches.\n\nConsidering all pairs in $A \\times B$ takes too long for large tables. So EM is typically performed in two steps: blocking and matching [6, 12]. The blocking step uses heuristics to quickly remove a large number of pairs judged unlikely to match. The matching step applies a ruleor ML-based matcher to each remaining pair, to predict match or non-match. Figure 1 illustrates these steps. Here blocking keeps only those pairs that share the same state. In this paper we focus on the blocking step.\n\nExisting Blocker Types, Threshold vs. Top-k Blocking: Numerous blocking solutions have been developed (see [5, 27, 32] for\n![img-0.jpeg](img-0.jpeg)\n\nFigure 1: The blocking and matching steps of EM.\nsurveys). They fall roughly into five types: sort, hash, similaritybased, rule-based, and composite. Sorted neighborhood computes for each tuple a key, sorts tuples based on keys, then outputs a pair of tuples if their keys are within a pre-defined distance. Hash-based methods compute for each tuple a hash value (a.k.a. key), groups all tuples sharing the same hash value into a block, then outputs a pair of tuples if they belong to the same block. Examples of such methods include attribute equivalence, phonetic blocking, suffix array, etc. [5, 27, 32]. Most existing blocking methods are hash-based.\n\nSimilarity-based methods output only those tuple pairs where the similarity score between the tuples exceeds a pre-specified threshold, or one tuple is within the kNN (k-nearest) neighborhood of the other tuple [29].\n\nWe refer to the above two options as threshold blocking and top$k$ blocking, respectively. As we will see later, Sparkly uses top-k blocking, which we show to be critical to achieve high recall.\n\nSimilarity scores that have been considered for blocking include syntactic scores such as Jaccard, cosine, edit distance [12], and semantic scores such as those computed using word embedding/deep learning (DL) techniques [38].\n\nRule-based methods employ multiple blocking rules, where each rule can employ multiple predicates (e.g., if the Jaccard score of the titles is below 0.6 and the years are not equivalent, then the two papers do not match) [15]. Given a set of rules, the blocker figures out the best way to create a workflow and execute it using indexes [15]. Finally, composite methods generalizes rule-based blocking and can combine multiple blocking methods in a complex pre-specified workflow. Examples include canopy blocking [12] and the union of a DL method with a rule-based method in [38].\n\nRecent Research Directions: In recent years researchers have pursued several directions regarding the above five blocker types [27, 32]. They have examined how to scale blocking methods (e.g., using Hadoop/Spark) [9] and how to apply DL (e.g., to develop novel hash-based [13] and similarity-based blockers [38]).\n\nAs discussed earlier, hash-based methods generate blocks of tuples, then output pairs whose tuples belong to the same block. A novel recent direction, called meta-blocking, examines how to manage these blocks (e.g., remove/prune blocks) [32]. A related direction is token blocking, in which each block contains all tuples that share a particular token. These blocks can be managed using meta blocking. Another interesting direction, called schema-agnostic, drops the assumption that Tables $A$ and $B$ share the same schema. The well-known JedAI EM platform implements many meta-blocking,\n\ntoken-blocking, and schema-agnostic techniques [31, 33]. Other important directions include learning blockers [15], using the feedback from the matcher to improve the blocker, and explaining blockers $[5,27,32]$.\n\nEvaluating Blockers: Most existing works evaluate blockers in three aspects: recall, output size, and runtime. Let $G \\subseteq A \\times B$ be the set of (unknown) gold matches, and $C \\subseteq A \\times B$ be the set of tuple pairs output by a blocker $Q$. Then the recall of $Q$ is $|C \\cap G| /|G|$, the fraction of gold matches in the output of $Q$. The output size is $|C|$, and the runtime is measured from when the blocker receives the two tables $A$ and $B$ until when it outputs $C$.\n\nOther aspects considered important, especially in industry, include the ease of tuning, the ability to block on arbitrarily large tables (e.g., those with billions of tuples) without crashing, extensibility (e.g., with more blocking methods/rules), the ability to estimate the total blocking time, explainability, and the ability to run the blocker easily in a variety of environments (e.g., a single laptop, a Spark cluster, a Kubernetes cluster), among others.\n\nIn this paper, we will evaluate blockers using the above three popular aspects: recall, output size, and runtime. We will briefly discuss Sparkly regarding some additional aspects, but deferring a thorough evaluation of these aspects to future work.\n",
        "title": [
            "## 2 BLOCKING FOR ENTITY MATCHING"
        ],
        "summary": "This section provides an overview of entity matching (EM) and emphasizes the importance of blocking as a preprocessing step to efficiently reduce candidate pairs before matching. It categorizes existing blocking methods into five types\u2014sort, hash, similarity-based (including threshold and top-k blocking), rule-based, and composite\u2014and discusses recent research directions such as scaling with distributed systems, meta-blocking, token blocking, schema-agnostic approaches, and learning-based blockers. The section also outlines standard criteria for evaluating blockers, namely recall, output size, and runtime, while noting additional practical considerations like scalability, extensibility, and explainability. The paper focuses on blocking, particularly top-k similarity-based blocking, and evaluates methods primarily on recall, output size, and runtime."
    },
    "Methodology": {
        "content": "We now describe the tf/idf measure used in keyword search (KWS), the open-source KWS library Lucene, then Sparkly, which uses Lucene to perform blocking for EM.\nTF/IDF is a well-known family of scoring functions for ranking documents in KWS [24]. To explain, consider a set of documents $\\mathcal{D}=\\left\\{D_{1}, \\ldots, D_{N}\\right\\}$, where each document $D_{i}$ is a string (e.g., article, email). Given a user query $Q$, which is also a string, we want to find documents in $\\mathcal{D}$ that are most relevant to $Q$. To do so, we compute a score $s(D, Q)$ for each document $D$, then return the documents ranked in decreasing score.\n\nA well-known scoring function [12], TFIDF-cosine, is as follows. First we tokenize each document $D$ into a bag of tokens, also called terms. Next, we convert document $D$ into a vector $V_{D}$ of weights, one weight per term, where the weight for term $t$ is $V_{D}(t)=t f(t, D)$ idf $(t)$. Here $t f(t, D)$ is the frequency of term $t$ in document $D$, i.e., the number of times it occurs in $D$. The quantity idf $(t)$ is the inverse document frequency of term $t$, defined as $\\log (N / d f(t))$, where $N$ is the number of documents in $\\mathcal{D}$, and $d f(t)$ is the number of documents that contain term $t$.\n\nWe tokenize and convert query $Q$ into a vector of weights $V_{Q}$ in a similar fashion. Finally, we compute score $s(D, Q)$ to be the cosine of the angle between the two vectors $V_{D}$ and $V_{Q}$ :\n\n$$\ns(D, Q)=\\left[\\sum_{t} V_{D}(t) \\cdot V_{Q}(t)\\right] /\\left[\\sqrt{\\sum_{t} V_{D}(t)^{2}} \\cdot \\sqrt{\\sum_{t} V_{Q}(t)^{2}}\\right]\n$$\n\nwhere $t$ ranges over all terms in $D$ and $Q$. This definition captures the intuition that if a term $t$ of query $Q$ occurs often in a document $D$, then $D$ is likely to be relevant to $Q$ and score $s(D, Q)$ should be high. This is reflected in the use of the term frequency $t f(t, D)$. A higher\n$t f(t, D)$ leads to a higher weight for $t$ in $V_{D}$, and consequently a higher $s(D, Q)$. But this should not be true if term $t$ also occurs in many other documents. In such cases term $t$ should be discounted, i.e., its weight in $V_{D}$ should be low, and this is accomplished by multiplying the term frequency $t f(t, D)$ with the inverse document frequency idf $(t)$.\n\nOver the years, many tf/idf scoring functions have been proposed. Among them, the following function, called Okapi BM25, has become most popular, and is the default scoring function used by Lucene [36]:\n\n$$\ns(D, Q)=\\sum_{t \\in Q} \\frac{t f(t, D) \\cdot\\left(k_{1}+1\\right)}{t f(t, D)+k_{1} \\cdot\\left(1-b+b \\cdot \\frac{|D|}{a v g d l}\\right)} \\cdot \\operatorname{idf}(t)\n$$\n\nwhere $\\operatorname{idf}(t)=\\log \\left(\\frac{N-d f(t)+0.5}{d f(t)+0.5}+1\\right)$, and $k_{1}$ and $b$ are free parameters, often set as $k_{1} \\in[1.2,2.0]$ and $b=0.75$. The tech report explains the intuition behind BM25 (see also [36]), which has been shown to work quite well for KWS [18, 24].\nMany open-source software for KWS have been developed. Among them Apache Lucene has become most popular [18]. The latest releases of Lucene, since 2015, have used state-of-the-art techniques in KWS to be both accurate and fast [18].\n\nSpecifically, Lucene uses BM25 as the default scoring function, ensuring highly accurate KWS results. It has also been extensively optimized, to be very fast for top-k querying, i.e., given a query $Q$ and a set of documents $\\mathcal{D}$, find the top $k$ documents in $\\mathcal{D}$ that have the highest BM25 score with $Q$, for a pre-specified $k$ (typically up to a few hundreds). To do this, naively we can use an inverted index to find all documents in $\\mathcal{D}$ that share at least one term with $Q$, compute BM25 scores for all of them, then sort and return the top $k$ documents. This however would be very slow, because the set of documents sharing at least one term with $Q$ is often very large.\n\nTo solve this problem, Lucene uses a recently developed KWS technique called block-max WAND [3, 10, 11]. This technique allows Lucene at query time to perform a branch-and-bound search to find the top k. This way, Lucene can avoid examining a huge number of documents, and can generally find the top-k documents very fast, as we will see in the experiment section.\n\nLucene has become the library of choice for a wide variety of KWS applications. Two other popular open-source KWS systems, Solr and ElasticSearch, build on Lucene. As a library, Lucene provides two key API functions: indexing and querying. Solr (started in 2004) and ElasticSearch (started in 2010) use these API functions, but provide extensive support for indexing and querying a large number of documents on a cluster of machines.\nWe now describe Sparkly, which takes as input two tables $A$ and $B$ with the same schema, and outputs a table $C$ consisting of tuple pairs $(a \\in A, b \\in B)$ judged likely to match.\n\nTo do so, Sparkly uses two key ideas. First, it performs top-k blocking. Specifically, it builds an inverted index $I$ for the smaller table, say table $A$. Then for each tuple $b$ in table $B$, it probes $I$ to find the top $k$ tuples in $A$ with the highest tf/idf scores (where $k$ is pre-specified), then pairs these tuples with $b$ and outputs the pairs.\n\n![img-1.jpeg](img-1.jpeg)\n\nFigure 2: Sparkly's execution on a 3-node cluster.\nSecond, Sparkly executes the above steps in a distributed sharenothing fashion, using Lucene on a Spark cluster. We now describe the execution in detail, using the 3-node Spark cluster in Figure 2.\nBuild the inverted index I of table A: Suppose that tables $A$ and $B$ reside on the primary node $N_{1}$ (see Figure 2), and that $A$ is the smaller table, i.e., having fewer tuples than $B$. Sparkly chops table $A$ horizontally into multiple chunks, each containing multiple tuples, starts multiple threads on the entire Spark cluster, sends each chunk to a thread, which calls Lucene's indexing procedure to create an inverted index for that chunk. Sparkly then combines these inverted indexes into a single inverted index $I$ for table $A$, and writes $I$ to the local disk of node $N_{1}$.\nShip index I and tuples of table B to the secondary nodes: Sparkly then ships index $I$ to the local disks of the secondary nodes $N_{2}$ and $N_{3}$ (see Figure 2). Next, it chops table $B$ (on primary node $N_{1}$ ) into chunks, each containing multiple tuples (currently set to 500), sends each chunk to a secondary node and assigns to a thread on that node. Figure 2 shows that a chunk $B_{1}$ of table $B$ consisting of tuples $u_{1}, u_{2}, u_{3}$ is sent to a thread on node $N_{2}$, and that another chunk $B_{2}$ consisting of tuples $u_{4}, u_{5}, u_{6}$ is sent to a thread on node $N_{3}$.\nFind top-k tuples in table A for each tuple of table B: Each thread now goes through the tuples in the assigned chunk. For each tuple, it probes index $I$ to find the top $k$ tuples in table $A$ with the highest $\\mathrm{tf} / \\mathrm{idf}$ scores, pair these tuples with the probing tuple, then sends the pairs back to the primary node $N_{1}$. (The thread only sends back the IDs, not the full tuples.)\n\nConsider again the thread for chunk $B_{1}$ with tuples $u_{1}, u_{2}, u_{3}$ (under \"Node $N_{2}$ \" in Figure 2). Suppose $k=2$. This thread first processes tuple $u_{1}$ : it probes index $I$ to find the top 2 tuples in table $A$ with the highest $\\mathrm{tf} / \\mathrm{idf}$ scores with $u_{1}$. Suppose these are tuples $v_{1}, v_{2}$. Then the thread creates the pairs $\\left(u_{1}, v_{1}\\right),\\left(u_{1}, v_{2}\\right)$ and send them back to node $N_{1}$ (see the figure). Next, the thread processes tuple $u_{2}$, then tuple $u_{3}$. Similarly, Figure 2 shows how a thread on node $N_{3}$ processes chunk $B_{2}=\\left\\{u_{4}, u_{5}, u_{6}\\right\\}$.\n\nWhen Sparkly has processed all chunks of table $B$, and all pairs sent back from the secondary nodes have been collected into a table $C$ on primary node $N_{1}$, Sparkly terminates, returning $C$ as the blocking output.\nThe tf/idf scoring function: All that is left is to describe the scoring function used by Sparkly. First, we ask that the user manually identify a set of attributes to block on. Typically these are \"identity\"\nattributes, such as name, phone, address, product title, brand, etc. Next, for each tuple (in table $A$ or $B$ ), we concatenate the values of these attributes into a single string $s$, lowercase all characters in $s$, tokenize $s$ into a bag of 3-gram tokens, and remove all nonalphanumeric tokens.\n\nLet $B_{t}$ be the bag of 3 -grams for a tuple $t$. When indexing table $A$, for each tuple $t \\in A$, we index only $B_{t}$, not the entire tuple $t$. Finally, when querying, we compute the BM25 score between two tuples $u, v$ to be the BM25 score between $B_{u}$ and $B_{v}$.\n\nDiscussion: We now discuss the rationales behind the main design decisions of Sparkly.\nUse top-k instead of thresholding: This is the most important decision that we made. We use top-k instead of thresholding for the following reasons. First, it is often easier to select a value for $k$ than a threshold $\\alpha$. Given a value for $k$, we know precisely how big the blocking output will be, and the rule of thumb is to select $k$ that produces the largest blocking output that the matching step can handle, because the larger the blocking output, the higher the recall. On the other hand, we often do not have any guidances on how to select a good threshold $\\alpha$.\n\nSecond, we observe that real-world data is often so noisy that the similarity scores of many matching tuple pairs can be quite low (see Section 5). This makes it very difficult to set threshold $\\alpha$. A high threshold kills off many matches, producing low recall. A low threshold often blows up the blocking output size in unpredictable ways. In contrast, in such cases we observe that the matching tuples are often still within the top-k \"distance\" of each other, making top-k retrieval still effective, as we show in Section 4.\n\nFinally, Lucene and many other KWS systems are highly optimized runtime-wise for top-k search, but not for threshold search.\nDo top-k on just one side instead of both sides: Currently we do topk only from table $B$ into table $A$. Another option is to do top-k on both sides: from $B$ into $A$ and from $A$ into $B$, then return the union of the two outputs. We experimented with this option but found that it can significantly increase runtime yet improve recall only minimally. It also complicates coding (e.g., we have to write code to remove duplicate pairs from the outputs of both sides).\nDo top-k from the larger table: We index the smaller table, say table $A$, then do top-k probing from the larger table $B$ because indexing the smaller table takes less time and produces a smaller index $I$. Shipping this smaller index $I$ to the secondary Spark nodes takes less time. Finally, probing from the larger table rather than the smaller one tends to produce higher recall, given the same $k$ value.\nShip the index and tuples of table B to the secondary nodes: This is the second most important decision that we made. The challenge here is to find an efficient way to do distributed top-k probing on a Spark cluster. Toward this goal, recall that we create the inverted index $I$ for table $A$ on the primary node $N_{1}$. Table $B$ also resides on $N_{1}$. So the simplest solution is to do all top-k probings there, using only the cores of $N_{1}$. However, $N_{1}$ has a limited number of cores (e.g., 16,32 ), so it can run only a limited number of threads, severely limiting how much top-k probing we can do in parallel.\n\nThe next solution is to send the tuples of $B$ to the secondary Spark nodes, then do top-k probing from the secondary nodes into the index $I$ on primary node $N_{1}$. This way, the secondary nodes can\n\nrun a much larger number of threads. Unfortunately, when these threads contact primary node $N_{1}$ to do top-k probing, they would need to rely on the threads running on the cores of $N_{1}$ to do the actual probing into index $I$. So once again, the limited number of threads on $N_{1}$ becomes the bottleneck for scaling.\n\nAs a result, we decided to ship the index $I$ and the tuples of $B$ to the secondary nodes. Each secondary node then runs multiple threads, each doing top-k probing using the copy of $I$ on that node. So we can do as many top-k probings in parallel as the number of threads on the secondary nodes. This produces a share-nothing parallel solution that is highly modular and can scale horizontally as we add more secondary Spark nodes.\nPartitioning very large tables $A$ and $B$ : A major concern is whether shipping index $I$ would take too long, because it can be very large. This turned out not to be the case. For example, in our experiments, indexing a table of 10 M tuples produces indexes of size $1.3-2 \\mathrm{~GB}$, and shipping these takes 21-32 seconds (see Section 4).\n\nStill, one may ask what if the tables have 500 M or 5 B tuples? Would the indexes become too big to fit on the disks of Spark nodes? Our solution is to break table A (the smaller table, to be indexed) into partitions of say 50M tuples, then process the partitions sequentially. For example, if table $A$ has 100 M tuples, then we break $A$ into partitions $A_{1}$ and $A_{2}$ each having 50 M tuples, then run two blocking tasks: $A_{1}$ vs. $B$ and $A_{2}$ vs. $B$. Finally, we combine the top-k results produced by these tasks. This guarantees that Sparkly never has to build and ship indexes for more than 50M tuples.\nUse Lucene instead of ElasticSearch or Solr: We use Lucene because it provides highly effective procedures to index a table and do top-k probing, which are exactly what we need. ElasticSearch (ES) and Solr build on top of Lucene and provide a lot more capabilities that we do not need (e.g., sharding) yet can cause complications. For example, when we first built Sparkly, we used ES and observed two problems. First, it took much longer (and more pain) to install Sparkly, because we had to install ES as a part of the process. Second, Sparkly has less applicability, because we could not run it in certain environments, e.g., on a Kubernetes cluster, because we cannot ensure data locality (i.e., when Spark performs a top-k query, the query will go to an ES instance installed on the same node). So we switched to Lucene, which addresses the above problems.\nSo far we ask an expert user to manually select a set of attributes. Then we concatenate the values of these attributes into a string, tokenize it using a default (3-gram) tokenizer, then index and search on the tokenized string. We call this solution Sparkly Manual.\n\nSparkly Manual works well, but can suffer from three problems. First, it can be difficult even for expert users to select good blocking attributes. Second, concatenating the attributes is problematic because the importance of a token depends on which attribute it appears in. Finally, using a single tokenizer is also problematic because different attributes may best benefit from different tokenizers. To address these problems, we will automatically select blocking attributes and associated tokenizers, as elaborated below.\nProblem Definition: First we formally define this selection problem. Let the attributes of tables $A$ and $B$ be $F=\\left\\{f_{1}, \\ldots, f_{n}\\right\\}$.\n![img-2.jpeg](img-2.jpeg)\n\nFigure 3: Illustrating the discriminativeness of configs.\n\nLet $T=\\left\\{t_{1}, \\ldots, t_{m}\\right\\}$ be a set of tokenizers (e.g., 3-gram, wordlevel). We define a configuration $L$ (or config $L$ for short) as a set of (attribute, tokenizer) pairs $L=\\left\\{\\left(f_{i 1}, t_{i 1}\\right), \\ldots,\\left(f_{i p}, t_{i p}\\right)\\right\\}$, where $f_{i j} \\in F, t_{i j} \\in T, j=1 \\ldots p$. Thus, in a config $L$ different attributes can use different tokenizers.\n\nLet $\\mathcal{L}$ be the set of all configs. Our goal is to find the config $L \\in \\mathcal{L}$ that maximizes the recall. To define recall, we begin by defining the similarity score. Given two tuples $b \\in B, a \\in A$, we define their similarity score with respect to config $L$ as the sum of the BM25 scores of the individual attributes in the config (tokenized using the assigned tokenizers). Formally, we have $s(b, a, L)=\\sum_{j=1}^{p} s_{j}\\left[t_{i j}\\left(b, f_{i j}\\right), t_{i j}\\left(a, f_{i j}\\right)\\right]$. Here $t_{i j}\\left(b, f_{i j}\\right)$ applies the tokenizer $t_{i j}$ to the value of attribute $f_{i j}$ of tuple $b$, producing a bag of tokens, and $t_{i j}\\left(a, f_{i j}\\right)$ produces another bag of tokens. Then $s_{j}\\left[t_{i j}\\left(b, f_{i j}\\right), t_{i j}\\left(a, f_{i j}\\right)\\right]$ computes the BM25 score between these two bags of tokens.\n\nNext, we define the blocking output for when the above similarity score $s(b, a, L)$ is used. For any tuple $b \\in B$, let $Q(b, A, k, L)$ be the list of top-k tuples from $A$ that has the highest $\\mathrm{tf} /$ idf scores, as defined by $s(b, a, L)$, with $b$. Let $C(b, A, k, L)$ be the set of all pairs $(b, v)$ where $v \\in Q(b, A, k, L)$. Then the blocking output can be defined as $C(B, A, k, L)=\\cup_{b \\in B} C(b, A, k, L)$.\n\nFinally, let recall $(C(B, A, k, L))$ be the fraction of true matches in $C(B, A, k, L)$. Then our problem is to find a config $L \\in \\mathcal{L}$ that maximizes recall $(C(B, A, k, L))$ for a given $k$.\n\nThe above problem raises two challenges: how to estimate the recall of a config and how to find the config with the highest recall in a large space of configs. We now address these challenges.\nEstimating the Recall of a Config: Given a config $L$, it is not possible to estimate its recall recall $(C(B, A, k, L))$ because we do not know the true matches. To address this problem, we make the key observation that it is possible to estimate the discriminative power of a config $L$, which captures its ability to tell apart the matches from the non-matches. We can then search for the config with the maximal discriminativeness, on the heuristic assumption that this config is likely to achieve high recall.\n\nExample 3.1. To motivate, consider a tuple $b \\in B$ and three singleton configs $L_{1}, L_{2}, L_{3}$ involving attributes $f_{1}, f_{2}, f_{3}$, respectively. Let $r_{1}, r_{2}, r_{3}$ be the top-k lists for $b$, produced by querying the inverted index $I$ of table $A$ using the above 3 configs, respectively. Figure 3.a shows the top-k lists $r_{1}, r_{2}, r_{3}$. Note that each top-k list contains tuple IDs in $A$, already sorted in decreasing BM25 scores. For each list, the figure shows the scores, plotted against the ranks of where they appear in the list.\n\nFigure 3.a suggests that for the above tuple $b \\in B, r_{3}$ is quite \"discriminative\", because it \"slopes down\" steeply (i.e., the top few tuples of $r_{1}$ have very high scores while the rest of the tuples have much lower scores). In fact, the curve $r_{3}$ appears more discriminative than $r_{1}$ and $r_{2}$, which do not \"slope down\" as much.\n\nIt may appear that we can measure this discriminativeness as the area under the curve (AUC): smaller AUC means higher discriminativeness. In Figure 3.a, this is indeed true for $r_{3}$ and $r_{1}: A U C\\left(r_{3}\\right)<$ $A U C\\left(r_{1}\\right)$ and $r_{3}$ is more discriminative than $r_{1}$. But it is not true for $r_{3}$ and $r_{2}$, because $A U C\\left(r_{2}\\right)<A U C\\left(r_{3}\\right)$, yet $r_{2}$ is not more discriminative than $r_{3}$. The problem is that the BM25 scores of the curves (generated by using different configs) are not comparable, and hence the AUCs are also not comparable. To address this, we normalize the BM25 scores of each curve to be between [0,1] (by dividing the original scores in each curve by the maximum score). Figure 3.b shows the normalized curves. Now it is indeed the case that smaller AUC means higher discriminativeness.\n\nThus, we can define the discriminativeness of a config $L$ for a table $B$ (given a table $A$ and an inverted index $I$ ) as the average discriminativeness of config $L$ for each tuple in $B$ : mean $A U C(B, L, k)=$ $\\frac{1}{|B|} \\sum_{b \\in B} A U C(b, L, k)$.\n\nIn turn, we can define the discriminativeness of config $L$ for a tuple $b$ in $B$ as the normalized AUC. Let $r(b, L, k)=\\left(\\left(v_{1}, s_{1}\\right), \\ldots\\left(v_{k^{\\prime}}, s_{k^{\\prime}}\\right)\\right)$ be the top- $k$ tuple list retrieved from index $I$, for record $b \\in B$, scored according to config $L$, sorted in decreasing order of score $s_{1}, \\ldots, s_{k^{\\prime}}\\left(k^{\\prime} \\leq k\\right.$ because only tuples with positive score can be in the list). Then we can compute the area under the curve as $A U C(b, L, k)=\\frac{1}{k^{\\prime} \\cdot s_{i}} \\sum_{i=1}^{k^{\\prime}-1} s_{i+1}+\\frac{s_{i}-s_{i+1}}{2}$. The tech report [34] explains how we arrive at this formula.\n\nIn practice, computing mean $A U C(B, L, k)$ for a config $L$ is too expensive, as we have to query index $I$ with all tuples in $B$. So we approximate it using mean $A U C\\left(B^{\\prime}, L, k\\right)$, where $B^{\\prime}$ is a random sample of 10 K tuples of $B$ (and we set $k$ to 250 ).\n\nSearching for a Good Config: Our goal now is to find the config $L$ that maximizes mean $A U C\\left(B^{\\prime}, L, k\\right)$. The number of configs can be huge (e.g., in the millions). So we adopt a greedy search approach. First, we score all singleton configs (each using a single attribute/tokenizer pair) and find the top 10 configs with the lowest mean $A U C$ scores. Next, we combine these configs to create \"composite\" configs, where each config has up to 3 attributes. We do not consider configs of more than 3 attributes because in our experience these configs take much longer to run yet only minimally improve recall, if at all. Finally, we score all configs and return the one with the lowest mean $A U C$ score.\n\nSince we use at most 10 singleton configs to create more configs of size up to 3 attributes, the total number of configs to score is at most 175, making exhaustive scoring of all configs possible.\n\nWe further speed up the above search using a technique called early pruning. To illustrate, consider again the problem of scoring all singleton configs to find the top 10 configs. Scoring a config means querying the inverted index $I$ with all tuples $b \\in B^{\\prime}$. Even though $B^{\\prime}$ is small (currently set to 10 K ), this still takes time. So we score the configs using a sample $B^{\\prime \\prime}$ which is a small subset of $B^{\\prime}$, use a statistical test to remove all configs for which we can say with high confidence that they will not make it into the top 10, then\n\nTable 1: Datasets for our experiments.\n\n| Type | Dataset | Table A | Table B | \\#Matches | \\#Attr |\n| :--: | :--: | :--: | :--: | :--: | :--: |\n| Structured | Amazon-Google $_{1}$ | 1,363 | 3,226 | 1,300 | 4 |\n|  | Walmart-Amazon $_{1}$ | 2,554 | 22,074 | 1,154 | 6 |\n|  | DBLP-Google $_{2}$ | 2,616 | 64,263 | 5,347 | 4 |\n|  | DBLP-ACM $_{1}$ | 2,616 | 2,294 | 2,224 | 4 |\n|  | Hospital $_{1}$ | 1,786 | 1,786 | 3,949 | 7 |\n|  | Songs-Songs $_{1}$ | 1,000,000 | 1,000,000 | 1,292,023 | 5 |\n| Textual | Amazon-Google $_{2}$ | 1363 | 3,226 | 1,300 | 2 |\n|  | Walmart-Amazon $_{2}$ | 2,554 | 22,074 | 1,154 | 2 |\n|  | Abt-Buy | 1,081 | 1,092 | 1,097 | 3 |\n| Dirty | Amazon-Google $_{3}$ | 1,363 | 3,226 | 1,300 | 4 |\n|  | Walmart-Amazon $_{2}$ | 2,554 | 22,074 | 1,154 | 6 |\n|  | DBLP-Google $_{2}$ | 2,616 | 64,263 | 5,347 | 4 |\n|  | DBLP-ACM $_{2}$ | 2,616 | 2,294 | 2,224 | 4 |\n|  | Hospital $_{2}$ | 1,786 | 1,786 | 3,949 | 7 |\n|  | Songs-Songs $_{2}$ | 1,000,000 | 1,000,000 | 1,292,023 | 5 |\n\nexpand $B^{\\prime \\prime}$ with more tuples, re-score the remaining configs, and so on. Specifically: (1) Initialize the subsample $B^{\\prime \\prime}=\\emptyset$ and $S$ to be the set of all configs from which we have to compute the top 10 configs. (2) Expand the subsample $B^{\\prime \\prime}$ by adding to it a small random sample of $h$ tuples from $B^{\\prime} \\backslash B^{\\prime \\prime}$. (3) Compute the mean $A U C$ for all configs in $S$ using $B^{\\prime \\prime}$ and finding the set $\\hat{R}$ of the top-10 configs. (4) For each config $L \\in S \\backslash \\hat{R}$, use the Wilcoxon signed-rank test [39] to determine (with high confidence) if its mean $A U C$ score is greater than those of the configs in $\\hat{R}$. If yes, then $L$ is unlikely to ever be in the top 10. Remove $L$ from $S$. (5) If $S=\\hat{R}$ or $B^{\\prime \\prime}=B^{\\prime}$, return $\\hat{R}$ as the top-10 configs, otherwise go back to Step 2.\n\nWe also use the above early pruning procedure to search the space of the larger configs. We defer further details to the tech report [34].\nDerek Paulsen<br>University of Wisconsin-Madison<br>Informatica Inc.<br>dpaulsen2@wisc.edu\n\nYash Govind<br>Apple Inc.<br>yash_govind@apple.com\n\nAnHai Doan<br>University of Wisconsin-Madison<br>Informatica Inc.<br>anhai@cs.wisc.edu\n",
        "title": [
            "## 3 THE SPARKLY SOLUTION",
            "### 3.1 The TF_IDF Family of Scoring Functions",
            "### 3.2 The Lucene KWS Library",
            "### 3.3 The Sparkly Solution",
            "### 3.4 Selecting Attributes and Tokenizers",
            "# Sparkly_ A Simple yet Surprisingly Strong TF_IDF Blocker for Entity Matching"
        ],
        "summary": "This section details the use of tf/idf-based scoring, particularly the BM25 function implemented in Lucene, for efficient keyword search (KWS) and its integration into Sparkly\u2014a system for scalable blocking in entity matching (EM). It describes how Sparkly leverages Lucene\u2019s top-k retrieval and distributed execution on Spark clusters to pair likely matching tuples between two tables by indexing the smaller table and probing from the larger. The section also introduces an automated approach for selecting attribute-tokenizer configurations to maximize recall, using a discriminativeness measure based on normalized area under the curve (AUC) of BM25 scores, and applies a greedy, pruned search to efficiently identify strong configurations. Experimental considerations, such as partitioning and distribution strategies, are discussed to ensure scalability and performance for large datasets."
    },
    "Related Work": {
        "content": "EM has been a long-standing challenge in data management [1, $6,7,12,14,26,30]$. There has been multiple academic efforts on building scalable EM systems such as JedAI [31, 33], Magellan [19], and CloudMatcher [17].\n\nOver the past decades, numerous blocking solutions have been developed. See [5, 27, 32] for surveys, and see Section 2 for a discussion of the main blocker categories. However, tf/idf blocking has received virtually no attention, as far as we can tell. The closest work that we have found is the recent work [28], which performs token blocking, i.e., hashing each tuple to multiple blocks, each corresponding to a token in the tuple. This work removes blocks\nthat correspond to tokens of low tf/idf values. The work [4] develops a scoring function that can be viewed as the Jaccard similarity function using IDF. We evaluated this function in Section 5.\n\nTF/IDF has long been used in IR and Web search [24]. Lucene was released in 1999. For a long time it was somewhat slow and inaccurate, and was largely ignored by researchers [18]. In 2015, however, Lucene adopted cutting-edge techniques such as BM25 and block-max WAND. It is now viewed as quite accurate and fast, and has attracted attention from IR researchers [18]. TF/IDF has long been used in the matching step of EM [8].\n\nThe work [40] has studied top-k search, but only for similarity measures such as Jaccard, cosine, dice, and overlap, for string matching. As far as we can tell, top-k tf/idf search has been studied intensively by IR researchers (resulting in the block-max WAND technique), but not by database researchers. The work [41] develops AutoBlock, which was shown by [38] to underperform the DL methods Autoencoder and Hybrid, which underperform Sparkly. The work [22] also addresses blocking. But it maximizes recall while keeping precision (i.e., the fraction of pairs in the blocking output that are correct matches) above a threshold. We consider a fundamentally different problem of maximizing recall for any given $k$ (i.e., any given blocking output size).\n\nThe share-nothing architecture of Sparkly is reminiscent of sharenothing architectures for parallel processing of relational data [37], and our Spark-based probing method for blocking is reminiscent of distributed/parallel joins for relational data [20, 21]. But here we consider the novel context of blocking for EM. Finally, the work [2] describes an industrial blocking solution at Amazon, which uses meta blocking to manage token-centric blocks and uses sophisticated techniques to scale.\n",
        "title": [
            "## 6 ADDITIONAL RELATED WORK"
        ],
        "summary": "This section reviews prior work on entity matching (EM) and blocking techniques, highlighting that while many scalable systems and blocking strategies have been developed, tf/idf-based blocking has been largely overlooked in the database community. It contrasts recent advances and applications of tf/idf and related search techniques in information retrieval with their limited use in EM, while also discussing related work on top-k search, parallel architectures, and industrial approaches. The section emphasizes the novelty of focusing on maximizing recall for a given output size in blocking, and distinguishes the presented approach from existing methods that prioritize precision or use different architectures."
    },
    "Experiment": {
        "content": "Datasets: We use 15 datasets described in Table 1, which come from diverse domains and sizes, and have been extensively used in recent EM work [23, 25, 29, 38] (except Hospital, which is private). Structured datasets have short atomic attributes such as name, age, city. Textual datasets have only 2-3 attributes that are textual blobs (e.g., title, description). For dirty EM, we focus on one type of dirtiness, which is widespread in practice [25] mainly due to information extraction glitches, where attribute values are \"moved\" into other attributes. Textual and dirty datasets are derived from the corresponding structured datasets (e.g., the textual dataset AmazonGoogle $_{2}$ is derived from the structured dataset Amazon-Google $_{1}$ ).\n\nLater we use 6 additional datasets for certain experiments, as discussed in Section 4.5 and Section 5.\n\nMethods: We compare Sparkly to 8 state-of-the-art (SOTA) EM blockers.\nAutoencoder, Hybrid, Union(DL,RBB): A recent work [38] shows that deep learning (DL) based blockers significantly outperform many other blockers. So we compare Sparkly to the two best DL blockers: Autoencoder and Hybrid [38]. The work [38] also shows that combining the best DL blocker and RBB, a SOTA industrial blocker, produces even better recall at a minimal increase of blocking output size. As a result, we also compare Sparkly with that blocker, henceforth called Union(DL,RBB).\n\n![img-3.jpeg](img-3.jpeg)\n\nFigure 4: SM vs. the two best DL methods in terms of recall and blocking output size.\n\nPBW, DBW, JD: Hash blockers have been very popular, and recent EM work has developed highly effective hash blockers, as captured in the pioneering JedAI open-source EM platform [31, 33]. These blockers hash each tuple to be matched into multiple blocks, one per each unique token in the tuple, then employ sophisticated methods to remove/clean blocks, among others. Based on personal communications with the JedAI authors, we compare Sparkly to 3 SOTA blockers in JedAI: PBW, DBW, and JD (we describe these methods in the technical report [34]).\nkNN-cosine, kNN-jaccard: Finally, a recent work [29] shows that a kNN blocker outperforms many other blockers. This blocker finds all tuple pairs where a tuple is among the $k$ nearest, i.e., most similar, neighbors of the other tuple, where the similarity measure is cosine over 5-gram tokenization. So we compare Sparkly with this blocker, denoted kNN-cosine. We also compare Sparkly to kNN-blockers where the similarity measure is cosine over 3-gram tokenization and Jaccard over 3-gram and 5-gram tokenization.\nWe begin by comparing Sparkly to existing methods in terms of recall and output size. To keep the comparison manage-able, we first compare SM, the Sparkly version where the user manually selects the attributes to be blocked on (i.e., Sparkly Manual), with all SOTA blockers. Then we compare SM with SA, the Sparkly version that automatically selects blocking attributes (i.e., Sparkly Auto).\n\nComparing SM to DL Methods: Figure 4 compares SM with the two best DL blockers, Autoencoder and Hybrid [38]. The figure shows 15 plots, one per dataset. Consider the first plot, which is for the structured Amazon-Google dataset. Here the x-axis shows the recall $R=|C \\cap G| /|G|$, where $C$ is the blocking output and $G$ is the set of all gold matches. The $y$-axis shows the candidate set size ratio $\\operatorname{CSSR}=|C| /|A \\times B|$. Both axes show values in percentage. So a value of 85 on the x -axis means recall of $85 \\%$, and a value of 5 on the $y$-axis means CSSR of $5 \\%$. Like SM, the two DL blockers Autoencoder and Hybrid are also top-k. So we vary the value of $k$ to generate the above plot. We generate the remaining 14 plots in a similar way. Note that the $y$-axes of the 15 plots vary significantly\nin scale. This is necessary so that we can show the difference among the curves.\n\nAll 15 plots show that SM significantly outperforms the two DL blockers: for each recall value, SM achieves a much lower CSSR, and this gap widens dramatically as recall approaches $100 \\%$. For example, on the first plot, at recall of $98 \\%$, Sparkly-Man achieves CSSR of $2.5 \\%$, whereas the two DL blockers achieve CSSR of $10 \\%$. These gaps are bigger for textual datasets, suggesting that SM can better handle textual data than the DL blockers. The gaps are smaller but still quite significant on all dirty datasets.\n\nThe above two DL methods concatenate all attributes and then block on the concatenation. In the next experiment, we modified them to block on the concatenation of only those attributes that SM blocks on. Even in this case, SM still outperforms both DL methods on 14 datasets (sometimes by very large margins) and is comparable on 1 dataset (see the tech report for details).\n\nComparing SM to Other Methods: Next we compare SM with Union(DL,RBB), which combines the best DL blocker and RBB (a SOTA industrial blocker), and the three JedAI methods: PBW, DBW, and JD. It is very difficult to vary the parameters of these methods in such a way that generates meaningful recall-CSSR curves, because they do not have a top-k parameter that we can adjust. So we compare them with SM at $k=10,20,50$, as shown in Table 2.\n\nThis table shows that SM is very predictable: it achieves high recall for all datasets ( $92.5-100 \\%$ for $k=10,96.4-100 \\%$ for $k=20$, $98.7-100 \\%$ for $k=50$ ), and its output size is capped as $k *|B|$. In contrast, the remaining four methods are unpredictable. For example, PBW's recall can be perfect ( $100 \\%$ ) but also can be as low as $74.5 \\%$, and its output size can be small but can also be as high as 4.2 billions for the structured dataset Songs. (We report no results for \"S - D\" because PBW was out of memory on this dataset, on a machine with more than 100G of RAM). Similarly, DBW's recall can be as low as $84.7 \\%$ and output size as high as 454.5 M .\n\nJD produces much more reasonable output size across all datasets, but at the cost of lower recall 35.4-96.4\\%. Similar to JD, Union(DL,RBB) also produces reasonable output sizes (larger than those of JD), but varying recalls $83-99.9 \\%$.\n\nTable 2: SM vs. the three JedAI methods and Union(DL,RBB) in terms of recall and blocking output size.\n\n| Dataset | PBW |  | DBW |  | JD |  | Union (DL,RBB) |  | Sparkly K=10 |  | Sparkly K=20 |  | Sparkly K=50 |  |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n|  | $\\|C\\|$ | Recall | $\\|C\\|$ | Recall | $\\|C\\|$ | Recall | $\\|C\\|$ | Recall | $\\|C\\|$ | Recall | $\\|C\\|$ | Recall | |C| | Recall | |C| | Recall | |C| | Recall | |C| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\n\n![img-4.jpeg](img-4.jpeg)\n\nFigure 6: Runtime for (a) varying dataset sizes, and (b) varying cluster sizes using datasets of 10M tuples each.\nuse word tokenizers on some attributes, whereas SM only uses 3gram tokenizers. This produces fewer tokens, which often leads to faster top-k search.\n\nIt is noteworthy that SA can block datasets of size 10M under 100 minutes, incurring an AWS cost of only $\\$ 12.5$.\n\nFigure 6.b shows the runtime of Sparkly on the 10M WDC and 10M Songs datasets, as we vary the size of the AWS cluster. As the number of nodes goes from 3 to 15 , runtime decreases significantly, as expected. As we increase the cluster size, eventually the overhead time (indexing, shipping, attribute/tokenizer selection, etc.) will dominate (compared to the top-k probing time).\nWe find the indexing time to be minimal. For example, on the same AWS cluster described above, indexing Songs at size 5M and 10M takes 76 and 115 seconds, respectively. The resulting index sizes are also reasonable. For Songs and WDC at size 1M, 5M, and 10M, the sizes are $137,664,1318 \\mathrm{MB}$, and 214,1034 , and 2042 MB , respectively. Shipping these indexes to the Spark nodes takes minimal time. For Songs and WDC at 1M, 5M, and 10M, shipping the indexes takes $2.2,7.2,21$ seconds, and $2.5,11,32$ seconds, respectively.\n\nFinally, recall that SA performs a search for a good set of attributes/tokenizers (to block on). On Songs and WDC 1M, 5M, and 10M, without early pruning, this search takes $4,9.2,15.6$ mins and $4.6,10.1,17.2$ mins, respectively. Early pruning cuts these times by up to $70 \\%$, to $1.2,3.2,6$ mins, and $2,5.3,14$ mins, respectively. The greedy method used by the searcher was quite effective. We performed exhaustive search on 11 datasets to find the optimal configs, and found that the greedy method found a config with a score within $0-0.8 \\%$ of the optimal score on 10 datasets and within $10 \\%$ on 1 dataset.\nWe now vary the parameters of the major components to examine the sensitivity of Sparkly. We only summarize the findings here, deferring a detailed discussion to the TR.\n\nBlocking Attributes: Recall that in SM, the user manually selects a set of attributes $S$ to block on. We find that varying this set of attributes does impact the performance of SM, minimally by $0-1.5 \\%$ CSSR in 11 datasets, and moderately by $2-5 \\%$ CSSR in 4 datasets. This suggests that while manually selecting blocking attributes is a reasonable strategy, there is still room to improve, e.g., by automatically finding such attributes, as done in SA.\n\nTokenizers: Recall that SM uses a 3gram tokenizer. Next we examine replacing this tokenizer with a 2gram, 4gram, and word-based tokenizer, respectively. We find that changing the tokenizer can\nsignificantly impact the performance (e.g., by up to $11.5 \\%$ CSSR). Overall, the 3gram tokenizer (used by SM) is a good choice as it has reasonable performance on most datasets. The 2gram and 4gram tokenizers perform worst, with the 2gram tokenizer also incurring the longest runtime.\n\nBM25's Parameters: BM25 has two parameters: $k_{1}$ (default value 1.2) and $b$ (default value 0.75 ), to handle term saturation and document length. Varying $k_{1}$ from 1 to 2 does not significantly change SM's performance. This may be because in our setting blocking attributes do not have many terms, and they do not have high term frequency, so term saturation is not a major issue. Varying $b$ from 0.5 to 1 changes SM's performance more, by up to $2 \\%$ CSSR. But we also find that $b=0.75$ provides a good default value for SM, as its curve is either the best curve or very close to the best curve on most datasets.\n\nConfig Searcher's Parameters: SA uses a searcher to find a good blocking config. This searcher has four major parameters: (1) the size of $B^{\\prime}$, a sample of table $B$ on which to score the configs (set to 10K), (2) the number of tuples returned in each querying $k=250$, (3) the number of initial configs selected (set to 10), and (4) the max number of attributes considered in a config (set to 3 ).\n\nVarying (1) from 5 K to 15 K changes SA minimally. Varying (2) from 200 to 300 again changes SA minimally (only up to $0.2 \\%$ CSSR on 1 dataset). Similarly, varying (3) from 8 to 12 and varying (4) from 2 to 4 show minimal changes. In all cases, the default values for (1)-(4) provide a good curve, which is either the best or very near the best.\nWe now examine how Sparkly performs on very large datasets, how it compares runtime-wise to DL methods, and whether DL methods can achieve higher accuracy, given larger datasets (to train on).\n\nIt is very difficult to find very large public datasets with complete gold, i.e., all true matches (without which we cannot compute the blocking recall). After an extensive search, we settle on three datasets: BC, MB, and WDC. BC (Big Citations) blocks two tables of 2.5 M and 1.8 M paper citations. MB (Music Brainz) blocks a table of 20M songs (against itself), and WDC blocks a table of 26M product descriptions [35]. BC and MB have complete gold, but WDC does not (see the tech report).\n\nTable 3 shows the results. First, we deployed an AWS cluster of 30 m 5 .4 xlarge nodes ( 16 cores, 64G RAM, $\\$ 0.75 /$ hour, per node), then ran Sparkly on all three datasets (see the first three rows of the table). Each row lists the results of SM and SA separated by \" \". Column \"Time\" shows the total time in minutes, while the next three columns show the recall at $k=10,25,50$. We cannot compute recall for WDC as it does not have the complete gold.\n\nThe first three rows show that Sparkly scales to very large datasets, and that SA is much faster than SM, taking only 130 and 168 mins to block WDC 26M and MB 20M, respectively, at a reasonable cost of less than $\\$ 67.5$ on AWS. Sparkly achieves high recall on MB and BC at $k=50$.\n\nSkipping the 4th row of Table 3 (which we discuss later), we now consider the DL method Autoencoder. Unfortunately we had\n\nTable 3: Sparkly and DL methods on large datasets.\n\n| Method | Dataset | Time | Recall @ 10 | Recall @ 25 | Recall @ 50 |\n| :--: | :--: | :--: | :--: | :--: | :--: |\n| Sparkly | WDC 26M | 603/130 | - | - | - |\n|  | MB 20M | 449/168 | 79/95 | 87/97 | 91/98 |\n|  | BC 2.5M | 44/11 | 99/79 | 100/89 | 100/94 |\n|  | MB 10M | 132/61 | 85/96 | 91/98 | 94/98 |\n| Autoencoder | WDC 10M | 625 | - | - | - |\n|  | MB 10M | 691 | 30 | 35 | 40 |\n|  | BC 2.5M | 146 | 81 | 84 | 85 |\n| Hybrid | BC 2.5M | 2719 | 73 | 76 | 78 |\n\ntremendous difficulties scaling Autoencoder to large datasets. Autoencoder is a prototype code used in the paper [38], for datasets of up to 1 M tuples. It runs on a single GPU and uses many Python libraries that are not well suited to large datasets (e.g., the SVD implementation of Sklearn). So when applied to large datasets, Autoencoder quickly exhausts memory and crashes, and there is no easy way to modify it to run in a distributed setting (where it can use a lot more GPU memory).\n\nAfter extensive optimization efforts, we managed to apply Autoencoder to BC 2.5M, WDC 10M, and MB 10M, on a SOTA hardware available to us ( $32 \\mathrm{t} / 16 \\mathrm{c}$ CPU with 64 G RAM coupled with RTX 2080ti GPU with 11G RAM). Table 3 shows the results. While it is not entirely fair to compare the runtimes of Autoencoder and Sparkly, because they run on different hardware, it is still interesting to note that Autoencoder takes much more time than Sparkly, e.g., 691 vs 132/61 mins (for SM/SA) on MB 10M, and 146 vs 44/11 mins on BC 2.5M. Autoencoder spent most time in preprocessing and self-supervised training.\n\nHybrid is far more complex than Autoencoder, and we only managed to run it on BC 2.5M (it ran out of memory on WDC 5M and MB 5M). Even on BC, its runtime is already very high (2719 mins). This suggests that existing prototype DL blockers do not scale to large datasets, requiring a lot more future work on this topic.\n\nBoth Autoencoder and Hybrid achieve far lower recall at $k=50$ than Sparkly (see the rows for BC 2.5M and MB 10M), suggesting that these methods still cannot exploit larger datasets to achieve higher accuracy than Sparkly. In the tech report we discuss how the remaining SOTA methods also do not scale to these large datasets.\n",
        "title": [
            "## 4 EMPIRICAL EVALUATION",
            "### 4.1 Recall and Output Size",
            "### 4.3 Performance of Sparkly's Components",
            "### 4.4 Sensitivity Analysis",
            "### 4.5 Additional Experiments"
        ],
        "summary": "This section details the experimental setup and evaluation comparing Sparkly, a new entity matching (EM) blocker, against eight state-of-the-art (SOTA) blocking methods across 15 diverse datasets, plus additional large-scale datasets. Sparkly, in both its manual (SM) and automatic (SA) configurations, consistently outperforms deep learning and hash-based blockers in recall and blocking output size, demonstrating greater predictability and efficiency, especially on textual and dirty data. The section further explores Sparkly's scalability on datasets with up to 26 million records, showing that it maintains high recall and efficient runtimes, while prototype deep learning blockers struggle to scale and deliver lower accuracy. Sensitivity analyses confirm Sparkly's robustness to parameter changes, with its default settings yielding near-optimal results in most cases."
    }
}