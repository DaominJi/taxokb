{
    "Introduction": {
        "content": "Entity Matching (EM) aims to identify whether two entity records from two relational tables refer to the same real-world entity, which is one of the fundamental and significant tasks in data management. Most existing solutions [23, 30, 31] assume that two tables are homogeneous with the aligned schema. However, there is an urgent need to generalize the entity matching problem for more practical scenarios. Taking paper matching as an example, as shown in\n\n[^0]![img-0.jpeg](img-0.jpeg)\n\nFigure 1: An example of generalized entity matching.\nFigure 1, paper metadata is usually stored in relational tables or semi-structured JSON objects, while paper description (e.g., abstract) is textual data. It is not practical to unify their schemas since we need a potentially expensive schema matching in the preprocessing step [28], which is even not applicable when matching data of different formats (e.g., matching paper metadata stored in tabular format with their textual abstracts). Therefore, traditional EM is unable to support those practical scenarios like paper matching [50]. Recently, TDmatch [1] first attempts to match structured data and textual data while having extremely poor scalability (to be analyzed via experiments in Section 5.4). To support more practical application scenarios, following the previous study [50], we study Generalized Entity Matching (GEM), which allows matching entity records of relational, semi-structured, or textual types.\n\nExisting methods designed for GEM typically perform in a supervised learning way, which relies on a large amount of labeled training data, and thus, is extremely labor-intensive. Recent studies [23, 30] have achieved considerable performance by leveraging the power of pre-trained language models (LMs) and the fine-tuning paradigm. Nonetheless, the fine-tuning paradigm still requires a non-trivial amount of high-quality labeled examples (e.g., thousands of labels for a typical GEM application [50]). TDmatch [1] performs in an unsupervised learning way via graph creation and random walk. However, two drawbacks restrict its real-world applications as confirmed via experiments in Section 5: (i) it has unstable performance due to the absence of labeled samples; and (ii) random walk is not scalable on large datasets [44], incurring enormous execution time and memory usage (e.g., more than 120 hours and 130 gigabytes for once training of the SEMI-REL). To satisfy real-life applications, a low-resource (i.e., using only a small number of labeled examples) but an effective and efficient solution is required. To the best of our knowledge, low-resource GEM remains unexplored.\n\nTo overcome the low-resource dilemma, semi-supervised learning techniques (e.g., self-training [46]) are good choice. Self-training has recently been shown to obtain state-of-the-art performance\n\n\n[^0]:    This work is licensed under the Creative Commons BY-NC-ND 4.0 International License. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of this license. For any use beyond those covered by this license, obtain permission by emailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights licensed to the VLDB Endowment.\n    Proceedings of the VLDB Endowment, Vol. 16, No. 1 ISSN 2150-8097. doi:XX.XX/XXX.XX\n\nfor low-resource tasks like sequence generation [18] and speech recognition [52]. In self-training, a teacher model is trained on some labeled data, and is used to produce pseudo-labels on unlabeled data. Furthermore, the original labeled data is augmented with the pseudo-label data, and is employed to train a student model. Thus, a large amount of unlabeled data can be utilized effectively and the requirement of labeled data is correspondingly reduced. Although self-training has achieved promising performance in a wide range of applications, it has not been explored in GEM.\n\nMotivated by the above considerations, we study the problem of learning high-quality models for low-resource GEM by means of self-training. Our goal is to develop an effective and efficient low-resource GEM solution based on pre-trained LMs along with leveraging self-training to boost performance, which is a challenging endeavor. The challenges are mainly three-folds:\nChallenge I: How to tune pre-trained LMs for GEM better? Despite the success of fine-tuning LMs for the matching problem, some recent studies [16, 24, 25] find that there is a significant gap between objective forms in pre-training and fine-tuning, which restricts taking full advantage of knowledge in LMs. Pre-training is usually formalized as a cloze-style task to predict target words (e.g., masked language models). However, the approaches based on fine-tuning add additional layers to do different objective forms (e.g., classification and generation) as illustrated in Figure 2. To do GEM, existing approaches treat it as a classification problem. This gap hinders the transfer and adaptation of knowledge in LMs for GEM tasks.\nChallenge II: How to select high-quality pseudo-labels? The quality of pseudo-labels determines whether self-training can improve performance. Thus, the pseudo-label selection strategy is extremely important. A common strategy is using confidence to select pseudolabels. However, this strategy has some serious drawbacks [32, 37]. On the one hand, incorrect predictions can have high confidence scores in poorly calibrated networks. On the other hand, if we only aim at the pseudo-labels with high confidence produced by the teacher, there is little to gain for the student model.\nChallenge III: How to avoid expensive self-training? Traditional self-training can be costly. To be more specific, the labeled data is augmented by the pseudo-labels produced by the teacher model, which may be beneficial to performance but result in a longer training time. Intuitively, maybe not all training data contribute to boosting the performance of the student model. Nevertheless, how to quantify the importance of training data to avoid expensive self-training is still challenging.\n\nTo tackle the above three challenges, we propose a low-resource GEM solution PromptEM. Prompt-tuning is a new promising paradigm in natural language processing, and is able to bridge the gap of objective forms between pre-training and fine-tuning [24, 39]. To address the gap between pre-training and fine-tuning (CI), we cast GEM as a cloze-style task via designing the GEM-specific prompt-tuning, which has the same objective form as pre-training. Thus, we can stimulate the rich knowledge distributed in LMs through prompt-tuning. To select high-quality pseudo-labels (CII), we develop a lightweight uncertainty-aware self-training method to boost performance. High-quality pseudo-labels are a prerequisite for boosting performance of self-training. To this end, we employ recent advances in Bayesian deep learning [12] to obtain uncertainty estimates of the teacher model for pseudo-labeling and boosting the self-training process. To avoid expensive self-training (CIII), we prune useless training data dynamically using our proposed MC-EL2N, making the self-training process more lightweight and efficient. Our contributions are summarized as follows:\n\n- Low-resource GEM. This is the first work formally studying the problem of low-resource generalized entity matching. We articulate the importance of this problem in more practical scenarios.\n- Prompt-tuning for GEM. We present PromptEM, a new GEM solution based on prompt-tuning, which casts GEM as a clozestyle task. To the best of our knowledge, PromptEM is the first GEM (EM) solution that stimulates the rich knowledge distributed in LMs via designing GEM-specific prompt-tuning.\n- Generic Lightweight Self-training. To further improve the performance in low-resource settings, we develop a generic lightweight self-training method, which selects pseudo-labels using uncertainty and makes self-training more lightweight and efficient by dynamic data pruning.\n- Extensive Experiments. We conduct comprehensive experimental evaluation on GEM tasks compared against state-of-the-art approaches, using eight real-world datasets from various domains. Extensive experimental results demonstrate the superiority of our proposed PromptEM in terms of effectiveness and efficiency. Outline. Section 2 presents the problem definition and overviews preliminaries. We introduce prompt-tuning for GEM in Section 3. We further improve the performance by lightweight self-training in Section 4. Section 5 presents the experimental results. Finally, we discuss related work in Section 6 and conclude in Section 7.\nIn this section, we first present the problem definition of generalized entity matching (GEM). Next, we introduce the serializing method for GEM, followed by an introduction of conventional vanilla finetuning and prompt-based tuning with LMs.\nGiven two collections of data entries, entity matching (EM) is to identify pairs of data entries that refer to the same real-world entity. A classic EM workflow [20] has two main steps: blocking and matching. The blocking [42] typically uses simple heuristics or deep learning techniques to reduce the quadratic number of candidates. The matching identifies whether each candidate pair is a real match or not. In this paper, we focus on the matching. Formally, given two tables $E_{A}$ and $E_{B}$, we assign a binary label $y \\in\\{0,1\\}$ for each candidate pair $\\left(e_{a}, e_{b}\\right) \\in E_{A} \\times E_{B}$. Here, $y=1$ denotes a truly matched pair, while $y=0$ represents a mismatched pair.\n\nTo generalize the classic setting to more practical application scenarios, Machamp [50] comes up with the new research problem, generalized entity matching (GEM). GEM can support a variety of matching tasks with practical applications.\n\nProblem 1. Generalized Entity Matching (GEM). Given two structured, semi-structured, or unstructured entity tables $E_{A}$ and $E_{B}$ with homogeneous or heterogeneous schema, GEM is to assign a binary label $y \\in\\{0,1\\}$ for each candidate $\\left(e_{a}, e_{b}\\right) \\in E_{A} \\times E_{B}$.\nThe matching problem can be effectively solved by formulating it as a sequence classification task [13, 23, 50]. First, entity pairs\n\n![img-1.jpeg](img-1.jpeg)\n\nFigure 2: The illustration of fine-tuning and prompt-tuning. The blue rectangles in the figure are special prompt tokens, whose parameters are initialized and learnable during prompt-tuning.\nare serialized to sequences, and then, a pre-trained LM is finetuned to solve the task. Existing methods are designed for EM over structured data with homogeneous data, which are not suitable for GEM. Following [50], we extend the serialization method presented in Ditto [23] and introduce a reasonable way to fulfill this task.\nStructured tables. For structured tables, an entity with $n$ attributes can be denoted as $e=\\left\\{\\operatorname{attr}_{i}, \\operatorname{val}_{i}\\right\\}_{i \\in[1, n]}$, where $\\operatorname{attr}_{i}$ is the attribute name and $\\operatorname{val}_{i}$ is the corresponding attribute value. Then the serialization is denoted as:\nserialize $(e)::=[\\mathrm{COL}] \\operatorname{attr}_{1}[\\mathrm{VAL}] \\mathrm{val}_{1} \\ldots[\\mathrm{COL}] \\operatorname{attr}_{\\mathrm{n}}[\\mathrm{VAL}] \\mathrm{val}_{\\mathrm{n}}$ where [COL] and [VAL] are two special tags indicating the start of attribute names and values respectively. Taking the relational entity in Figure 1 as an example, we serialize it as:\n[COL] title [VAL] efficient similarity ... [COL] authors [VAL] renald... [COL] venue [VAL] SIGMOD [COL] year [VAL] 2003\nSemi-structured tables. The semi-structured tables can be serialized in a similar way. Specially, two differences exist: (i) For nested attributes, we recursively add the [COL] and [VAL] tags along with attribute names and values in each level of nests. (ii) To reduce the length of the sequence while ensuring the amount of information, we concatenate the elements in the list into one string for attributes whose content is a list. As an example, given the semi-structured entity in Figure 1, we serialize it as:\n[COL] title [VAL] efficient similarity ... [COL] year [VAL] 2003\n[COL] authors [VAL] ronald fagin ravi kumar d. sivakumar\nUnstructured tables. Unstructured textual entities are sequences originally, and hence, there does not need to serialize them.\nPre-trained language models (LMs) (e.g., BERT [6] and RoBERTa [26]) have demonstrated powerful semantic expression abilities, which can support lots of downstream tasks (e.g., classification and question answering). Formally, GEM can be treated as a sequence pair classification task denoted as $\\mathcal{T}=\\{\\mathcal{X}, \\mathcal{Y}\\}$, where $\\mathcal{X}$ is a candidate pair set and $\\mathcal{Y}$ is a class set. For each instance $x \\in \\mathcal{X}$, it is serialized by $x::=[\\mathrm{CLS}]$ serialize $(e)[\\mathrm{SEP}]$ serialize $\\left(e^{\\prime}\\right)[\\mathrm{SEP}]$, and is annotated with a label $y \\in \\mathcal{Y}$. Here, [CLS] and [SEP] are special tokens used to mark the beginning and end of a sequence.\n\nGiven a $\\mathrm{LM} \\mathcal{M}$, vanilla fine-tuning first coverts $x$ to the input sequence $\\{[C L S], w_{1}, \\ldots, w_{n},[S E P], w^{\\prime} 1, \\ldots, w^{\\prime}{ }_{m},[S E P]\\}$, and then, it uses $\\mathcal{M}$ to encode all tokens of the input sequence into corresponding vectors $\\left\\{\\mathbf{h}_{[C L S]}, \\mathbf{h}_{w_{1}}, \\ldots, \\mathbf{h}_{w_{n}}, \\mathbf{h}_{[S E P]}, \\mathbf{h}_{w_{1}^{\\prime}}, \\ldots, \\mathbf{h}_{w_{m}^{\\prime}}, \\mathbf{h}_{[S E P]}\\right\\}$, where $w_{i}$ is the token and $\\mathbf{h}_{w_{i}}$ is the corresponding embedding. For a downstream classification task (e.g., GEM), a task-specific head is trained to predict the probability distribution over the label set\n$y$ with the softmax function $p(\\cdot \\mid x)=\\operatorname{Softmax}\\left(\\mathbf{W} \\times \\mathbf{h}_{[C L S]}+\\mathbf{b}\\right)$. Here, $\\mathbf{h}_{[C L S]}$ is the embedding of special classification token [CLS], $b$ is the bias for the layer, and $\\mathbf{W}$ is a learnable matrix randomly initialized before fine-tuning. The parameters of $\\mathcal{M}, b$, and $\\mathbf{W}$ are tuned to maximize $\\frac{1}{|X|} \\sum_{x \\in X} \\log p(y \\mid x)$.\nPrompt-based tuning has been proposed to apply cloze-style tasks to tune LMs. Formally, we define a label word set $\\mathcal{V}_{y}=\\left\\{w_{1}, \\ldots, w_{m}\\right\\}$. $\\mathcal{V}_{y}$ is a subset of the vocabulary $\\mathcal{V}$ of the $\\mathrm{LM} \\mathcal{M}$, i.e., $\\mathcal{V}_{y} \\subseteq \\mathcal{V}$. We get an overall dictionary $\\mathcal{V}^{*}$ by taking the union of the dictionary corresponding to each label. Another primary component of prompt-tuning is a prompt template $T(\\cdot)$, which modifies the original input $x$ into a prompt input $T(x)$ by adding a set of additional tokens in $x$. Generally, a token [MASK] is added for LMs to predict the missing label word $w \\in \\mathcal{V}^{*}$. Thus, in prompt-tuning, a classification problem is transferred into a masked language modeling problem $p(y \\in \\mathcal{Y} \\mid x)=p\\{[\\operatorname{MASK}]=w \\in \\mathcal{V}_{y} \\mid T(x)\\}$.\n",
        "title": [
            "## 1 INTRODUCTION",
            "## 2 PRELIMINARIES",
            "### 2.1 Problem Formulation",
            "### 2.2 Serializing",
            "### 2.3 Vanilla Fine-tuning",
            "### 2.4 Prompt-based Tuning"
        ],
        "summary": "This section introduces the problem of Generalized Entity Matching (GEM), which extends conventional entity matching to heterogeneous and unstructured data formats commonly encountered in real-world scenarios. The authors highlight the limitations of existing approaches, especially their reliance on large labeled datasets and poor scalability, and argue for the necessity of low-resource solutions. They propose PromptEM, a novel approach leveraging prompt-tuning of pre-trained language models and lightweight, uncertainty-aware self-training to address key challenges in GEM, including objective form misalignment, pseudo-label selection, and efficiency. The section also formalizes the GEM task, details methods for serializing various data types, and contrasts traditional fine-tuning with prompt-based tuning strategies for language models."
    },
    "Problem Definition": {
        "content": "",
        "title": [],
        "summary": "Certainly! Please provide the full content of the section you would like summarized."
    },
    "Methodology": {
        "content": "In this section, we detail how to utilize prompt-tuning to deal with GEM. We first design GEM-specific prompt templates and label words, and then, we describe the training and inference process.\nTo cast the GEM problem as a prompt-tuning one, we first design suitable prompt templates (i.e., hard-encoding templates and continuous templates) and label words set (to consider general binary relationship).\nHard-encoding templates. For the choice of hard-encoding templates, we do not use automatic searching methods for discrete prompts since the GEM task is clearly defined and the prompts are easily purposeful. Given each candidate pair $x=\\left(e, e^{*}\\right)$, we construct the following templates:\n$\\mathrm{T}_{1}(x)=$ serialize $(e)$ serialize $\\left(e^{\\prime}\\right)$ They are [MASK]\n$\\mathrm{T}_{2}(x)=$ serialize $(e)$ is [MASK] to serialize $\\left(e^{\\prime}\\right)$\nContinuous templates. As prompt construction is to find a method that allows a LM to effectively perform a task, rather than being for human consumption, it is not necessary to limit the prompt to human-interpretable natural language [24]. Thus, continuous prompts are proposed to perform prompting directly in the embedding space of the model. Here, we employ P-tuning [25], where continuous prompt tokens are learned by inserting trainable variables into the embedding input. Specifically, trainable prompt tokens are initialized, and then, BiLSTM [15] is utilized to account for interaction between prompt tokens. This enables the model to\n\nfind better continuous prompts beyond the original vocabulary $\\mathcal{V}$ of $\\mathcal{M}$ could express. We give an illustrative example in Figure 2.\nLabel words set. In addition to designing templates, another primary component is to design the label words set. Note that, traditional EM tasks find pairs of entities that are identical [50]. However, GEM might require finding out entity pairs satisfying a general binary relationship. Taking paper matching as an example, our goal is to find pairs between paper metadata and abstracts. Indeed, the relationship between them is whether they are relevant, which is more general beyond matching. Considering general binary relationship, we map the label $y=$ yes into a set $\\mathcal{V}_{y}=\\{$ matched, similar, relevant $\\}$. Similarly, the label set for label $y=\\mathrm{no}$ is $\\mathcal{V}_{y}=\\{$ mismatched, different, irrelevant $\\}$.\nA classification problem is transferred into a masked language modeling problem via prompt-tuning. In masked language modeling, we use confidence scores of all the words in $\\mathcal{V}_{y}$ to construct the final score of the particular class $y$. Given a candidate pair $x$ (which is mapped to $T(x)$ ) and its class $y$ (which is mapped to $\\mathcal{V}_{y}=\\left\\{w_{1}, \\ldots, w_{m}\\right\\}$ ), the conditional probability is computed as:\n\n$$\nP(y \\mid x)=\\frac{1}{m} \\sum_{j}^{m} P\\left(\\{\\text { MASK }]=w_{j} \\mid T(x)\\right)\n$$\n\nTraining. The continuous prompt tokens can be parameterized by $\\phi$ and optimized along with $\\mathcal{M}$ during training. We tune the pretrained model $\\mathcal{M}$ (parameterized by $\\theta$ ) along with the additional prompt embeddings by using the cross-entropy loss function $\\mathcal{L}=$ $-\\sum \\log P(y \\mid x ; \\theta, \\phi)$. Here, we prompt-tune a pre-trained LM for the GEM task as follows:\n(1) Design task-specific prompt templates and label words set.\n(2) Initialize the network with parameters from the pre-trained LM and continuous prompt tokens.\n(3) Train the network on the training set until convergence.\n\nDifferent tasks require different prompt templates and label words. Step (1) is specifically designed for GEM tasks. Continuous prompt tokens in Step (2) are specifically designed to enable the model to find better prompts beyond $\\mathcal{V}$ of $\\mathcal{M}$ could express.\nInference. For inference, we aim to assign a label for the input, which can directly use Eq. 1 to predict the class of the current input instance based on predicted words of the [MASK] position.\nWith prompt-tuning, we can stimulate the rich knowledge distributed in LMs, which achieves considerable performance under low-resource settings. To further improve the performance and avoid expensive self-training, we develop a generic lightweight self-training method.\nLet $D_{L}=\\left\\{\\left\\{x^{(i)}, y^{(i)}\\right\\}\\right\\}_{i=1}^{N_{L}}$ and $D_{U}=\\left\\{x^{(i)}\\right\\}_{i=1}^{N_{U}}$ be a labeled dataset with $N_{L}$ samples and an unlabeled dataset with $N_{U}$ samples, respectively. Our lightweight self-training aims to boost the performance (i.e., effectiveness) using uncertainty meanwhile being more efficient and lightweight than traditional self-training via dynamic data pruning. We describe the lightweight self-training procedure, with its pseudo code presented in Algorithm 1. Given a labeled\n\n```\nAlgorithm 1: Lightweight Self-training\n    Input: the number Iter of iterations, a labeled train set \\(D_{L}\\),\n        an unlabeled train set \\(D_{U}\\)\n    Output: a student model \\(\\mathcal{M}_{\\theta, \\phi}\\)\n    for \\(i \\leftarrow 1\\) to Iter do\n        Initialize a new teacher model \\(\\mathcal{M}_{t, \\theta, \\phi, i}\\)\n        for epoch \\(\\leftarrow 1\\) to Epochs of teacher do\n            Train \\(\\mathcal{M}_{t, \\theta, \\phi, i}\\) using the train set \\(D_{L}\\)\n            Uncertainty-aware Pseudo-label Selection\n            \\(D_{P} \\leftarrow\\) Select pseudo-labels from \\(D_{U}\\)\n            \\(D_{U} \\leftarrow D_{U}-D_{P}\\)\n            \\(D_{L} \\leftarrow D_{L} \\cup D_{P}\\)\n            Initialize a new student model \\(\\mathcal{M}_{s, \\theta, \\phi, i}\\)\n            for epoch \\(\\leftarrow 1\\) to Epochs of student do\n                Train \\(\\mathcal{M}_{s, \\theta, \\phi, i}\\) using the train set \\(D_{L}\\)\n            \\(\\triangleright\\) Dynamic Data Pruning\n            if (epoch mod frequency of pruning ) \\(=0\\) then\n                \\(D_{D} \\leftarrow\\) Select useless samples from \\(D_{L}\\)\n                \\(D_{L} \\leftarrow D_{L}-D_{D}\\)\n    return the best student model \\(\\mathcal{M}_{\\theta, \\phi}\n```\n\ndataset $D_{L}$, a teacher model $\\mathcal{M}_{t}$ is initialized and trained on $D_{L}$ until convergence (Lines 2-4). Then the teacher model $\\mathcal{M}_{t}$ produces pseudo-labels on $D_{U}$. After that, we introduce an uncertainty-aware pseudo-label selection strategy to select high-quality pseudo-labels $D_{P}$ (Lines 5-6). Meanwhile, $D_{U}$ and $D_{L}$ are updated (Lines 7-8). Next, a student model $\\mathcal{M}_{s}$ is initialized and trained on the updated $D_{L}$ (Lines 9-11). To make self-training more lightweight and efficient, we present a dynamic data pruning strategy, which can prune useless samples and their labels in $D_{L}$ every fixed number of epochs (Lines 12-15). Finally, we choose the best student model with the best performance on the validation set (Line 16). Since LST is general enough to incorporate with other approaches, it is possible to be widely used in practical low-resource applications.\nSelecting high-quality pseudo-labels is a prerequisite for boosting self-training performance. Therefore, we aim at reducing the noise present in the selected samples to improve the overall performance. A straightforward way to select pseudo-labels is by selecting samples with high-confidence predictions. However, incorrect predictions can have high confidence scores in poorly calibrated networks [37]. Besides, if the teacher model already predicts some samples with high confidence, then these is little to gain for student model with these samples [32]. Based on the observation that prediction uncertainties can be leveraged to negate the effect of poor calibration [37], we employ an uncertainty-aware pseudo-label selection strategy. Formally, uncertainty can be divided into epistemic uncertainty and aleatoric uncertainty [46]. The former comes from uncertainty in the parameters of the model, and the latter is uncertainty inherent in the data (e.g., two samples of different classes are similar). We focus on quantifying epistemic uncertainty. Inspired by [32, 37], we use MC-Dropout [12] to obtain an uncertainty measure by calculating the standard deviation of a fixed number (e.g., 10 in\n\nTable 1: Statistics of the datasets used in our experiments. \"All\" denotes the total number of labeled samples, and \"Train\" represents the number of training samples used in our default low-resource setting.\n\n| Datasets | Domain | Left Table |  | Right Table |  | Labeled Examples |  |  |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n|  |  | \\#row | \\#attr | \\#row | \\#attr | All | \\% rate | Train |\n| REL-HETER | restaurant | 534 | 6.00 | 332 | 7.00 | 567 | $10 \\%$ | 57 |\n| SEMI-HOMO | citation | 2,616 | 8.65 | 64,263 | 7.34 | 17,223 | $5 \\%$ | 861 |\n| SEMI-HETER | book | 22,133 | 12.28 | 23,264 | 12.03 | 1,240 | $10 \\%$ | 124 |\n| SEMI-REL | movie | 29,180 | 8.00 | 32,823 | 13.81 | 1,309 | $10 \\%$ | 131 |\n| SEMI-TEXT-w | product | 9,234 | 10.00 | 9,234 | 1.00 | 5,540 | $10 \\%$ | 554 |\n| SEMI-TEXT-c | product | 20,897 | 10.00 | 20,897 | 1.00 | 12,538 | $5 \\%$ | 627 |\n| REL-TEXT | citation | 2,616 | 1.00 | 2,295 | 6.00 | 7,417 | $10 \\%$ | 742 |\n| GEO-HETER | geo-spatial | 2,469 | 5.00 | 2,788 | 4.00 | 2,500 | $10 \\%$ | 250 |\n\nour experiments) of stochastic forward passes. To avoid carefully chosen thresholds, we choose $N_{P}$ samples with the least uncertainty after calculating the uncertainties of $D_{U}$ :\n\n$$\nD_{P}=\\left\\{\\left(x^{(i)}, \\hat{y}^{(i)}\\right)\\right\\}_{i=1}^{N_{P}}=\\text { Top } \\cdot N_{P}\\left(D_{U} \\mid-u^{(i)}\\right)\n$$\n\nHere, $N_{P}=N_{U} \\cdot u_{r}, u^{(i)}$ is the uncertainty of the sample, $\\hat{y}^{(i)}$ is the pseudo-label produced by the teacher model, and $u_{r}$ is the proportion of the unlabeled samples. The time complexity of the uncertainty estimation is $O\\left(\\left|N_{P}\\right| \\times \\log \\left(\\left|N_{U}\\right|\\right)\\right)$. Uncertainty-aware pseudo-label selection makes the self-training process more effective, which will be analyzed in Section 5.5.\nTraditional self-training can be expensive as the training set grows, resulting in more training time. Recently, Paul et al. [35] show that the Error L2-Norm (EL2N) score can identify important examples early in training. In other words, it can prune significant fractions of useless training data without sacrificing test accuracy, which can reduce the number of the training set and training time. Inspired by EL2N [35] and MC-Dropout [12], we combine these approaches and propose MC-EL2N, which is able to quantify importance scores more stably. Formally, the MC-EL2N score of a training sample $(x, y)$ is defined as $\\frac{\\sum_{i=1}^{n}| | M(x)-y | |_{2}}{n}$, where $n$ is the number of stochastic forward passes. Similarly, to avoid carefully chosen thresholds, we choose $N_{D}$ samples with the least MC-EL2N score after quantifying the importance of $D_{L}$ :\n\n$$\nD_{D}=\\left\\{\\left(x^{(i)}, y^{(i)}\\right)\\right\\}_{i=1}^{N_{D}}=\\text { Top } \\cdot N_{D}\\left(D_{L} \\mid-e^{(i)}\\right)\n$$\n\nHere, $N_{D}=N_{L} \\cdot e_{r}, e^{(i)}$ is the MC-EL2N score of the sample, and $e_{r}$ is the proportion of the labeled samples. Similar to the uncertainty estimation, this process can be computed efficiently in $O\\left(\\left|N_{D}\\right| \\times\\right.$ $\\log \\left(\\left|N_{L}\\right|\\right)$ ) time. We prune the useless samples every fixed epochs using dynamic data pruning, making the self-training process more lightweight and efficient. We will confirm the efficiency of dynamic data pruning, to be presented in Section 5.4.\nPengfei Wang, Xiaocan Zeng, Lu Chen, Fan Ye, Yuren Mao, Junhao Zhu, Yunjun Gao. PromptEM: Prompt-tuning for Low-resource Generalized Entity Matching. PVLDB, 16(1): XXX-XXX, 2022.\ndoi:XX.XX/XXX.XX\nPengfei Wang, Xiaocan Zeng, Lu Chen, Fan Ye, Yuren Mao, Junhao Zhu, Yunjun Gao<br>Zhejiang University, Hangzhou, China<br>\\{wangpf,zengxc,luchen,fan.ye,maoyuren,zhujunhao,gaoyj\\}@zju.edu.cn\nThe source code, data, and/or other artifacts have been made available at https://github.com/ZJU-DAILY/PromptEM.\n",
        "title": [
            "## 3 PROMPT TUNING FOR GEM",
            "### 3.1 Prompt Templates",
            "### 3.2 Training and Inference",
            "## 4 LIGHTWEIGHT SELF-TRAINING",
            "### 4.1 Overview",
            "### 4.2 Uncertainty-aware Pseudo-label Selection",
            "### 4.3 Dynamic Data Pruning",
            "## PVLDB Reference Format_",
            "# PromptEM_ Prompt-tuning for Low-resource Generalized Entity Matching",
            "## PVLDB Artifact Availability_"
        ],
        "summary": "This section introduces a prompt-tuning approach for generalized entity matching (GEM), outlining the design of both hard-encoded and continuous prompt templates, as well as task-specific label word sets to capture general binary relationships between entity pairs. The authors describe how the classification task is reframed as masked language modeling, leveraging pre-trained language models fine-tuned with cross-entropy loss. To enhance performance in low-resource settings, they propose a lightweight self-training method that incorporates uncertainty-aware pseudo-label selection using MC-Dropout and dynamic data pruning based on MC-EL2N scores, thereby efficiently refining the training data and improving model effectiveness. The section also provides dataset statistics used in experiments and details the implementation of these strategies."
    },
    "Related Work": {
        "content": "\nEntity Matching (EM) is one of the fundamental and significant tasks in data management. Many efforts have devoted to develop effective approaches for EM, including rule-based methods [11, 41, 49], crowdsourcing-based methods [14, 29, 48], and traditional ML-based methods [3, 5, 20, 38]. Recently, deep learning has been\nused widely in EM, and achieved promising results. DeepER [10] uses deep neural networks to extract features of entity pairs, and then models EM as a binary classification task. DeepMatcher [31] systematically describes a DL architecture, and designs a space of DL solutions for EM. However, a lot of labeled training data are still needed for those DL-based approaches, which is extremely expensive in practice. To decrease the demand for high-quality training data, Ditto [23] applies pre-trained language models to EM, performing well with the help of some data augmentation (DA) techniques. Rotom [30] effectively improves the performance of EM tasks via combining multiple DA operators. DADER [45] develops a framework that significantly advances EM by applying domain adaptation. Some other attempts have also been made to enhance the performance via information fusion [53], active learning [19, 33], and transfer learning [27, 43, 54]. Nonetheless, these methods only focus on EM tasks in low-resource scenarios but perform poorly on GEM tasks. TDmatch [1] first attempts to match textual content and structured data under an unsupervised setting. However, it has one serious shortcoming: it is not scalable on large-scale datasets, which makes it hard to be used in practical scenarios.\nDespite the success of fine-tuning pre-trained LMs [6, 26], the huge objective form gap between pre-training and fine-tuning still hinders the full use of pre-trained knowledge for downstream tasks [24, 25]. The birth of GPT-3 [4] is the seminal work that stimulates the development of prompt-tuning, which applies hand-encoding prompts for tuning and achieves impressive performance on various tasks, especially under the low-resource settings. Following GPT-3, many hand-encoding prompts [7, 25] are widely explored. Recently, automatic prompt search [40] and continuous prompts [17, 25] are proposed to avoid labor-intensive prompt design and enhance the expressiveness of the prompt. The burst of prompt-tuning has led to significant advancement in many areas such as natural language inference [25] and entity typing [7]. However, for the first time, we introduce prompt-tuning in EM for the better usage of pretrained LMs. PromptEM provides a good connection to recent NLP advancements with applications to the data management task.\n",
        "title": [
            "## 6 RELATED WORK",
            "### 6.1 Entity Matching",
            "### 6.2 Prompt-tuning"
        ],
        "summary": "This section reviews the evolution of entity matching (EM) methods, including rule-based, crowdsourcing, traditional machine learning, and, more recently, deep learning approaches that often require extensive labeled data. It highlights advances such as data augmentation, domain adaptation, and transfer learning to address low-resource EM scenarios, but notes these methods struggle with generalized entity matching (GEM) and scalability. The section then discusses the impact of pre-trained language models and the emergence of prompt-tuning, which bridges the gap between pre-training and fine-tuning, leading to significant progress in various NLP tasks. Finally, it introduces the novel application of prompt-tuning in EM through PromptEM, leveraging recent NLP innovations to enhance EM performance."
    },
    "Experiment": {
        "content": "In this section, we experimentally evaluate the proposed PromptEM on eight real-world datasets. We aim at answering the following research questions:\n\n- RQ1: How does PromptEM perform compared with the state-of-the-art methods under low-resource settings?\n- RQ2: How does each module affect the overall performance of the model?\n- RQ3: How does PromptEM perform compared with state-of-theart approaches in terms of efficiency?\n- RQ4: Why do we choose these key modules (i.e., continuous templates and uncertainty-aware pseudo-label selection)?\nDataset. We use all the seven real-world benchmark datasets with different structures from Machamp [50] and one geospatial dataset (GEO-HETER) [2]. The detailed GEO-HETER construction can be found in our online version ${ }^{1}$. The statistics of datasets are summarized in Table 1. Each dataset consists of the left and right tables of entities with possibly different formats (i.e., relational (REL) format, semi-structured (SEMI) format, or textual (TEXT) format). When they are of the same format, they can have a homogeneous (HOMO) or heterogeneous (HETER) schema. We use rate\\% of labeled data as training set (e.g., 57 labeled data for REL-HETER), and use the same train/valid/test splits as Machamp.\nBaselines. We compare PromptEM with eight SOTA EM methods, among which three (i.e., Ditto, DADER, and Rotom) have made efforts to low-resource EM, and TDmatch is an unsupervised matching method for structural and textual data.\n\n- DeepMatcher [31] is an entity matching framework that uses RNN architecture to aggregate the attribute values and then align the aggregated representations of the attributes.\n- BERT [6] is fine-tuned to treat GEM as a sequence pair classification task.\n- SentenceBERT [36] proposes a siamese architecture for pretrained LMs for sentence matching, and could also be applied to the task of GEM.\n- Ditto [23] is the SOTA EM approach that fine-tunes a pre-trained LM with three optimizations (i.e., domain knowledge, TF-IDF summarization, and data augmentation).\n- DADER [45] presents a transfer learning based EM framework via domain adaptation.\n- Rotom [30] proposes a meta-learning framework that selects and weights the augmented data to better fine-tune the LMs.\n- TDmatch [1] is an unsupervised approach to match textual and structured data using graph creation and random walk. Furthermore, we build an MLP classifier on top of its embeddings to perform in the supervised setting, called TDmatch*.\nImplementation details. We implement PromptEM in PyTorch [34], the Transformers library [51] and the OpenPrompt library [8]. We use RoBERTa-base [26] as the backbone structure of our model in all the experiments. Unless particularly specified, the experimental results are conducted under the low-resource setting shown in Table 1. We further apply the half-precision floating-point ( fp 16 ) optimization to save the GPU memory usage and running time. In all the experiments, the max sequence length is set to 512 ; the learning rate is set to $2 \\mathrm{e}-5$; the batch size is set to 32 ; the number of iterations for self-training is set to 1 ; and the number of passes for MC-Dropout is set to 10 . We use AdamW as the optimizer for training, fix the epochs of training the teacher model to 20 , and set the epochs of training the student model to 30 . We prune the train set for every 8 epochs. We tune the hyper-parameters by doing a grid search and selecting the one with the best performance. Specifically, the continuous template is selected from $\\left\\{\\mathrm{T}_{1}(\\cdot), \\mathrm{T}_{2}(\\cdot)\\right\\}, u_{r}$ is selected\n\n[^0]\n[^0]:    ${ }^{1}$ https://arxiv.org/pdf/2207.04802.pdf\n\nTable 2: Results of all the methods under the default low-resource setting.\n\n| Methods | REL-HETER |  |  | SEMI-HOMO |  |  | SEMI-HETER |  |  | SEMI-REL |  |  | SEMI-TEXT-c |  |  | SEMI-TEXT-w |  |  | REL-TEXT |  |  | GEO-HETER |  |  |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n|  | P | R | F | P | R | F | P | R | F | P | R | F | P | R | F | P | R | F | P | R | F | P | R | F |\n| DeepMatcher | 0.0 | 0.0 | 0.0 | 74.6 | 72.9 | 73.8 | 78.3 | 22.6 | 35.1 | 70.1 | 44.8 | 54.7 | 23.0 | 39.6 | 29.1 | 23.5 | 1.9 | 3.5 | 36.9 | 17.1 | 23.4 | 28.9 | 90.9 | 43.8 |\n| BERT | 100 | 90.9 | 95.2 | 90.1 | 93.2 | 91.6 | 63.6 | 17.6 | 27.6 | 92 | 94.5 | 93.3 | 56.9 | 47.2 | 51.6 | 19.6 | 20.9 | 20.2 | 27.1 | 26.6 | 26.9 | 70.8 | 90.4 | 79.4 |\n| SentenceBERT | 100 | 90.9 | 95.2 | 92.6 | 93.6 | 93.1 | 81.5 | 13.8 | 23.7 | 83.2 | 100 | 90.8 | 60.0 | 51.3 | 55.3 | 26.2 | 21.3 | 23.5 | 36.4 | 52.0 | 42.9 | 74.5 | 72.1 | 73.3 |\n| Ditto | 100 | 86.4 | 92.7 | 90.2 | 90.3 | 90.2 | 79.3 | 14.5 | 24.5 | 88.0 | 88.5 | 88.3 | 56.8 | 47.1 | 51.5 | 29.5 | 31.3 | 30.3 | 34.7 | 50.5 | 41.1 | 74.7 | 87.2 | 80.5 |\n| DADER | 81.8 | 81.8 | 81.8 | 81.5 | 91.4 | 86.2 | 98.4 | 37.7 | 54.6 | 87.6 | 96.2 | 91.7 | 15.0 | 87.4 | 25.6 | 11.4 | 100 | 20.5 | 26.1 | 64.6 | 37.2 | 54.2 | 92.7 | 68.4 |\n| Rotom | 100 | 77.3 | 87.2 | 89.2 | 94.3 | 91.7 | 83.3 | 15.7 | 26.5 | 95.8 | 88.0 | 91.7 | 68.0 | 54.6 | 60.5 | 43.6 | 34.1 | 38.3 | 51.9 | 45.5 | 48.5 | 76.7 | 78.5 | 77.6 |\n| TDmatch | 56.4 | 100 | 72.1 | 93.7 | 42.0 | 58.0 | 97.2 | 88.1 | 92.4 | 97.5 | 85.8 | 91.3 | 69.0 | 10.4 | 18.0 | 42.3 | 14.2 | 21.3 | 80.2 | 47.3 | 59.5 | 72.8 | 73.0 | 72.9 |\n| TDmatch* | 10.0 | 4.6 | 6.3 | 80.2 | 87.3 | 83.6 | 37.5 | 18.9 | 25.1 | 66.5 | 77.1 | 71.4 | 42.7 | 30.2 | 35.4 | 32.0 | 23.2 | 26.9 | 48.6 | 40.3 | 44.1 | 51.0 | 51.0 | 51.0 |\n| PromptEM | 100 | 100 | 100 | 94.2 | 94.1 | 94.2 | 93.9 | 57.9 | 71.6 | 91.4 | 98.9 | 95.0 | 80.6 | 65.5 | 72.3 | 44.9 | 37.9 | 41.1 | 61.2 | 61.5 | 61.4 | 78.8 | 89.9 | 84.0 |\n| PromptEM w/o FT | 100 | 95.5 | 97.7 | 90.5 | 94.8 | 93.3 | 39.5 | 52.2 | 45.0 | 83.8 | 53.6 | 65.3 | 56.3 | 55.3 | 55.8 | 31.5 | 19.0 | 23.7 | 22.1 | 55.9 | 31.7 | 79.3 | 85.1 | 82.1 |\n| PromptEM w/o LST | 100 | 100 | 100 | 94.2 | 94.1 | 94.2 | 91.7 | 27.7 | 42.5 | 91.4 | 98.9 | 95.0 | 73.4 | 59.1 | 65.5 | 35.8 | 31.8 | 33.7 | 58.0 | 60.4 | 59.2 | 76.4 | 90.4 | 82.8 |\n| PromptEM w/o DDP | 100 | 100 | 100 | 93.0 | 94.8 | 93.9 | 80.5 | 57.2 | 66.9 | 92.1 | 95.1 | 93.6 | 79.0 | 68.9 | 73.6 | 47.3 | 33.2 | 39.0 | 55.5 | 63.3 | 59.2 | 85.0 | 88.1 | 86.5 |\n\nTable 3: Results of all the methods under the extremely challenging low-resource setting.\n\n| Methods | REL-HETER |  |  | SEMI-HOMO |  |  | SEMI-HETER |  |  | SEMI-REL |  |  | SEMI-TEXT-c |  |  | SEMI-TEXT-w |  |  | REL-TEXT |  |  | GEO-HETER |  |  |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n|  | P | R | F | P | R | F | P | R | F | P | R | F | P | R | F | P | R | F | P | R | F | P | R | F |\n| DeepMatcher | 32.3 | 45.5 | 37.7 | 46.9 | 22.5 | 30.4 | 28.6 | 40.3 | 33.4 | 43.0 | 92.9 | 58.8 | 0.0 | 0.0 | 0.0 | 28.6 | 1.0 | 1.8 | 9.1 | 0.2 | 0.4 | 29.1 | 100 | 45.1 |\n| BERT | 100 | 95.5 | 97.7 | 85.9 | 82.4 | 85.8 | 78.8 | 16.4 | 27.1 | 97.0 | 71.6 | 82.4 | 21.2 | 31.4 | 25.3 | 18.4 | 8.5 | 11.7 | 18.6 | 9.9 | 12.9 | 45.8 | 78.5 | 57.8 |\n| SentenceBERT | 95.5 | 95.5 | 95.5 | 86.8 | 73.3 | 79.5 | 100 | 14.5 | 25.3 | 71.0 | 74.9 | 72.9 | 20.8 | 13.8 | 16.6 | 23.1 | 8.5 | 12.5 | 15.5 | 4.1 | 6.4 | 53.8 | 64.8 | 58.8 |\n| Ditto | 100 | 81.8 | 90.0 | 80.9 | 81.8 | 81.4 | 95.2 | 12.6 | 22.2 | 78.7 | 95.1 | 86.1 | 13.9 | 100 | 24.3 | 12.6 | 69.7 | 21.4 | 18.0 | 99.3 | 30.4 | 33.0 | 85.4 | 47.6 |\n| DADER | 88.9 | 72.7 | 80.0 | 75.9 | 86.6 | 80.9 | 47.1 | 65.0 | 54.6 | 94.0 | 86.3 | 90.0 | 41.7 | 0.9 | 1.7 | 12.4 | 7.1 | 9.0 | 60.2 | 11.9 | 19.9 | 63.0 | 87.9 | 73.4 |\n| Rotom | 100 | 95.5 | 97.7 | 80.1 | 93.5 | 86.2 | 77.8 | 17.6 | 28.7 | 96.3 | 85.8 | 90.8 | 23.5 | 31.4 | 26.9 | 22.0 | 5.2 | 8.4 | 19.8 | 23.9 | 21.6 | 72.0 | 78.3 | 75.0 |\n| TDmatch | 56.4 | 100 | 72.1 | 93.7 | 42.0 | 58.0 | 97.2 | 88.1 | 92.4 | 97.5 | 85.8 | 91.3 | 69.0 | 10.4 | 18.0 | 42.3 | 14.2 | 21.3 | 80.2 | 47.3 | 59.5 | 72.8 | 73.0 | 72.9 |\n| TDmatch* | 11.1 | 9.1 | 10.0 | 37.8 | 27.8 | 32.0 | 37.8 | 17.6 | 24.0 | 47.8 | 75.4 | 58.5 | 16.3 | 5.2 | 7.9 | 19.0 | 7.1 | 10.3 | 20.0 | 14.4 | 16.8 | 36.7 | 33.6 | 35.1 |\n| PromptEM | 100 | 100 | 100 | 86.1 | 92.2 | 89.0 | 93.9 | 28.9 | 44.2 | 94.0 | 94.5 | 94.3 | 40.8 | 29.0 | 33.9 | 15.7 | 46.9 | 23.6 | 26.5 | 50.2 | 34.7 | 78.0 | 81.9 | 79.9 |\n\nfrom $[0.05,0.10,0.15,0.20,0.25]$, and $e_{r}$ is selected from $[0.1,0.2,0.3$, $0.4,0.5]$. We select the epoch with the highest F1-score on the validation set, and report the values of precision, recall, and F1-score on the test set. All the experiments are conducted on a machine with an Intel Xeon Silver 4216 CPU, an NVIDIA A100 GPU and 512GB memory. We use the same serializing method as PromptEM to implement each baseline method and report the performance under their optimal settings. We present the implementation of baselines in our online version.\nEvaluation metrics. Following related studies [13, 23, 45], we employ three widely-used classification metrics, namely, precision (P), recall (R), F1-score (F).\nResults under the default low-resource setting. We first verify the performance under low-resource setting of PromptEM using the above eight baselines. The benchmark results of all methods across the datasets are reported in Table 2. DeepMatcher achieves the worst performance, since it does not leverage the recent advances in pre-trained LMs. Existing low-resource EM approaches (i.e., Ditto, DADER, and Rotom) achieve relatively poor performance, because the GEM problem is more intractable than EM (e.g., heterogeneous tables). In particular, TDmatch is not stable across different datasets due to the absence of label guidance, which can achieve the best F1-score on SEMI-HETER but only 18.0 F1-score on SEMI-TEXT-c. TDmatch outperform other LM-based approaches on SEMI-HETER. The reason is that SEMI-HETER has lots of numeric attributes, i.e., $53 \\%$ attribute values are digits. It is well known that LMs are not good at understanding digits [47]. Also, we find that the scalability of TDmatch is extremely poor, which will be confirmed in Section 5.4. We can observe that TDmatch performs better than TDmatch* in most cases. This is because TDmatch is\nspecifically designed for unsupervised learning, not necessarily suitable for supervised learning. Besides, we have a similar finding as those studies [21, 22]: BERT can be generalized to the specific domain, and hence, BERT based methods (including PromptEM) achieve better performance on GEO-HETER.\nEffectiveness to different low-resource settings. We reduce the training rate from $25 \\%$ to $5 \\%$ to see the performance under different low-resource settings. Experimental results are depicted in Figure 3. We observe that PromptEM achieves SOTA performance in most cases, while TDmatch and DADER achieve unstable results across different datasets due to lacking the guidance of labels and the heterogeneity of datasets. We also evaluate methods in a more challenging setting, i.e., the number of available data for training is only 80 for all the datasets. This setting is extremely challenging for supervised methods, e.g., only using $0.46 \\%$ labeled examples on SEMI-HOMO. As shown in Table 3, PromptEM achieves SOTA performance on most datasets, which demonstrates the great robustness of PromptEM compared to baselines. Moreover, it also shows the outstanding scalability of PromptEM, which achieves considerable performance only using a small number of labeled data examples.\n\nOverall, our PromptEM is superior to all the baselines in almost all the cases under various low-resource settings. As mentioned in Challenge I, there is a significant gap between objective forms in pre-training and fine-tuning. This gap hinders the transfer and adaptation of knowledge in LMs for GEM tasks, which restricts taking full advantage of knowledge in LMs. Prompt-tuning is a new promising paradigm in natural language processing, and is able to bridge the gap of objective forms between pre-training and fine-tuning [16, 24]. Thus, we can stimulate the rich knowledge distributed in LMs through designing GEM-specific prompt-tuning\n\n![img-2.jpeg](img-2.jpeg)\n\nFigure 3: Comparison results under different low-resource settings (\\%).\n[16, 24]. Recently, prompt-tuning has been applied to lots of tasks such as machine translation, information extraction, question answering, and so on [24]. In summary, prompt-tuning has the potential to outperform fine-tuning for those tasks based on LMs.\nNext, we study the effectiveness of each module in PromptEM (i.e., prompt-tuning (PT), lightweight self-training (LST), dynamic data pruning (DDP)) by comparing PromptEM with its variants without the key module. The results are listed in Tables 2.\nPromptEM vs. PromptEM w/o PT. PromptEM w/o PT denotes that we fine-tune the LM instead of prompt-tuning. It is observed that the use of prompt-tuning contributes to a large portion of the performance gain. The F1-score drops $15.7 \\%$ on average under the low-resource setting. This confirms that prompt-tuning greatly helps to stimulate the rich knowledge distributed in the LM.\nPromptEM vs. PromptEM w/o LST. We use LST to boost the performance under low-resource settings. We can observe that LST can bring performance improvement in most cases. For example, LST brings $6.8 \\%$ improvement on SEMI-TEXT-c. Also notice that LST brings relatively low improvement on some datasets. This is attributes to the nature of the datasets, as it is relatively much easier for PromptEM to achieve the extremely high performance, e.g., $100 \\%$ F1-score on REL-HETER.\n\nPromptEM vs. PromptEM w/o DDP. We can observe that DDP can prune useless training data without sacrificing test accuracy. It is worth noting that DDP can prune training data while slightly improving test accuracy in some datasets. This is because DDP makes the model focus on important and useful training data.\nWe further explore the efficiency of our proposed PromptEM in terms of training time and memory usage, and the results are presented in Table 4. Since it is common for methods to use a similar strategy for evaluating the GEM results in the test set, we do not report the test time of every evaluated approach. SBERT denotes SentenceBERT, and PromptEM- represents PromptEM without dynamic data pruning.\n\nPromptEM vs. best baselines. Due to the limitation of space, we report PromptEM with the other evaluated approaches that achieve the best quality of GEM results in the corresponding categories, i.e., the normal EM method SBERT, the low-resource EM approach Rotom, and the unsupervised matching method TDmatch. We report the GPU memory for the methods running on GPU and the CPU memory for the method (i.e., TDmatch) running on CPU, respectively. As observed, PromptEM needs more training time than SBERT to obtain the SOTA results. This demonstrates a trade-off between the effectiveness and efficiency of the GEM problem. To sum up, it is significant that spending a relatively longer time in achieving better matching results. Rotom requires two-stage training, incurring a long training process. SBERT, Rotom, and PromptEM need similar memory usage since they are all based on LMs. We would like to emphasize that TDmatch needs too much training time and memory usage, especially on relatively large datasets (e.g., 120.3 hours and 131.5 Gigabytes on SEMI-REL), which is very costly in real-world applications.\nPromptEM vs. PromptEM-. We also compare PromptEM with PromptEM- to evaluate the efficiency of dynamic data pruning. It is observed that DDP greatly helps to reduce the training time, i.e., reduce $26.1 \\%$ time on average. This is because the proposed MCEL2N is able to quantify useless training data effectively. Meanwhile, DDP does not bring extra memory usage as it does not require any new model parameters. As analyzed in Section 5.3, DDP does not hurt the performance. This further demonstrates that PromptEM is effective and efficient in solving the GEM problem.\nFinally, we investigate the performance of PromptEM using alternative modules by conducting the following experiments.\nEffect of template choices. Designing prompt templates is a primary component of prompt-tuning. We verify the effect of different templates, i.e., continuous $\\mathrm{T}_{1}(\\cdot)$, hard-encoding $\\mathrm{T}_{1}(\\cdot)$, continuous $\\mathrm{T}_{2}(\\cdot)$ and hard-encoding $\\mathrm{T}_{2}(\\cdot)$. Their average F1-scores on all datasets are $74.4,67.8,77.0$, and 74.5 , respectively. Continuous templates achieve better performance than hard-encoding templates. This further validates the effectiveness of the proposed continuous\n\nTable 4: Efficiency comparison between PromptEM and its competitors, including the running time and memory usage. \"s\" denotes seconds, \"m\" denotes minutes, \"h\" denotes hours, and \"G\" represents gigabytes. Due to the limitation of space, we use the abbreviations of datasets.\n\n| Datasets | SBERT |  | Rotom |  | TDmatch |  | PromptEM- |  | PromptEM |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n|  | T. | M. | T. | M. | T. | M. | T. | M. | T. M. |\n| R-H | 28.8s | 22.9 G | 35.4 s | 32.8 G | 14.0 m | 6.2 G | 38.3 s | 27.4 G | 26.6 s |\n| S-HO | 2.4 m | 29.5 G | 5.2 m | 32.8 G | 51.0 h | 41.4 G | 11.5 m | 29.2 G | 7.4 m 29.2 G |\n| S-HE | 43.4s | 24.9 G | 1.8 m | 35.8 G | 102.8 h | 105.3 G | 1.6 m | 29.0 G | 1.5 m 29.0 G |\n| S-R | 50.1s | 35.8 G | 2.6 m | 32.8 G | 120.3 h | 131.5 G | 1.4 m | 30.6 G | 1.1 m 30.6 G |\n| S-T-c | 2.0 m | 36.3 G | 20.2 m | 32.8 G | 10.7 h | 25.4 G | 20.8 m | 29.2 G | 11.5 m 29.2 G |\n| S-T-w | 1.8 m | 35.6 G | 11.2 m | 32.8 G | 2.2 h | 12.9 G | 6.1 m | 29.2 G | 5.3 m 29.2 G |\n| R-T | 2.3 m | 34.4 G | 11.1 m | 29.7 G | 5.6 h | 50.5 G | 8.1 m | 30.6 G | 5.6 m 30.6 G |\n| G-H | 49.2s | 27.9 G | 1.3 m | 32.8 G | 36.2 m | 16.7 G | 6.2 m | 30.6 G | 4.6 m 30.6 G |\n\nTable 5: Results of pseudo-label selection strategies.\n\n| Datasets | Uncertainty |  | Confidence |  | Clustering |  |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n|  | TPR | TNR | TPR | TNR | TPR | TNR |\n| REL-HETER | 1 | 1 | 0.250 | 0.864 | 0.250 | 0.881 |\n| SEMI-HOMO | 1 | 0.998 | 0.197 | 0.803 | 0.193 | 0.815 |\n| SEMI-HETER | 1 | 0.963 | 0.979 | 0.985 | 0.350 | 0 |\n| SEMI-REL | 1 | 1 | 0.426 | 0.583 | 0.432 | 0.602 |\n| SEMI-TEXT-c | 0.969 | 1 | 0.113 | 0.897 | 0.128 | 0.879 |\n| SEMI-TEXT-w | 0.333 | 0.967 | 0.056 | 0.928 | 0.114 | 0.912 |\n| REL-TEXT | 0.910 | 0.966 | 0.194 | 0.820 | 0.148 | 0.846 |\n| GEO-HETER | 0.867 | 1 | 0.644 | 0.758 | 0.236 | 0.738 |\n\ntemplates, which can find better continuous prompts beyond the original vocabulary $\\mathcal{V}$ of $\\mathcal{M}$ could express.\nEffect of label words choices. We compare our designed label words with a simple one (i.e., matched and mismatched). Using continuous $\\mathrm{T}_{1}$ and $\\mathrm{T}_{2}$, our designed label words achieve $+5.2 \\%$ and $+9.4 \\%$ average F1-score improvements over the simple one, respectively. This confirms the effectiveness of our designed label words. In other words, considering the more general relationship between entities is beneficial to the predictions.\nPseudo-label selection strategies. We consider several pseudolabel selection strategies, including uncertainty [37], confidence, and clustering [9]. We fix $u_{r}$ to 0.1 on all datasets. Similarly, confidence and clustering both select the samples whose scores are in the top $10 \\%$. Following [13], we use true-positive rate (TPR) and true-negative rate (TNR) to evaluate the quality of the pseudo-labels generated by different strategies. Formally, TPR represents the proportion of matched entity pairs that are correctly labeled, denoted as $\\frac{T P}{T P+F N}$; TNR represents the proportion of mismatched pairs that are correctly labeled, denoted as $\\frac{T N}{T N+F P}$. The results are reported in Table 5. As expected, uncertainty can achieve state-of-the-art performance when generating pseudo-labels, e.g., TPR and TNR are 0.88 and 0.99 on average, respectively. It confirms the effectiveness of the uncertainty-aware pseudo-label selection strategy.\n",
        "title": [
            "## 5 EXPERIMENTS",
            "### 5.1 Experimental Setup",
            "### 5.2 Main Results (RQ1)",
            "### 5.3 Ablation Study (RQ2)",
            "### 5.4 Efficiency Analysis (RQ3)",
            "### 5.5 Model Variants (RQ4)"
        ],
        "summary": "This section presents a comprehensive experimental evaluation of PromptEM on eight diverse real-world datasets, comparing its performance, efficiency, and module contributions against eight state-of-the-art entity matching (EM) methods in low-resource settings. The results demonstrate that PromptEM consistently outperforms all baselines, particularly under challenging low-resource scenarios, due to its use of prompt-tuning, lightweight self-training, and dynamic data pruning. Ablation studies confirm the effectiveness of each module, with prompt-tuning yielding the largest performance gains, while dynamic data pruning notably reduces training time without sacrificing accuracy. Further analysis shows that continuous prompt templates and uncertainty-aware pseudo-label selection strategies further enhance performance, underscoring the thoughtful design choices underlying PromptEM\u2019s superior results."
    }
}