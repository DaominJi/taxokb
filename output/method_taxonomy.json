{
  "metadata": {
    "num_papers": 16,
    "papers_processed": [
      "DTA.md",
      "BatchER.md",
      "BERTEM.md",
      "CollaborER.md",
      "CLER.md",
      "ADAMEL.md",
      "LLM-CER.md",
      "PromptEM.md",
      "Sparkly.md",
      "GSMB.md",
      "DIAL.md",
      "MinoanER.md",
      "DeepBlocker.md",
      "ZeroEA.md",
      "SMASH.md",
      "Battleship.md"
    ],
    "generator": "MethodTaxonomyGenerator"
  },
  "method_summaries": {
    "DTA.md": {
      "paper_id": "f32a10ff",
      "methodology_summary": "```markdown\n### Core Components\n| Component           | Function                                   | Inputs                | Outputs                   |\n|---------------------|--------------------------------------------|-----------------------|---------------------------|\n| Data Preprocessing  | Clean and format raw input data            | Raw datasets          | Structured input data     |\n| Feature Extraction  | Derive informative features for modeling   | Structured input data | Feature vectors           |\n| Model Training      | Build predictive model                     | Feature vectors       | Trained model             |\n| Model Evaluation    | Assess model performance                   | Trained model, test data | Performance metrics    |\n| Ablation Study      | Analyze impact of individual components    | Modified models       | Comparative performance   |\n\n### Workflow Sequence\n1. Data Preprocessing → **(Figure 1)**\n   - Clean and structure raw datasets using **Data Preprocessing**\n   - *Table insight: Table 1 in Figure 1 shows missing value statistics pre- and post-cleaning*\n2. Feature Extraction → **(Figure 2)**\n   - Generate feature vectors from structured data with **Feature Extraction**\n   - *Table insight: Table 2 in Figure 2 lists top 5 features ranked by importance*\n3. Model Training\n   - Train predictive model on feature vectors with **Model Training**\n4. Model Evaluation → **(Figure 3)**\n   - Evaluate the trained model using **Model Evaluation**\n   - *Table insight: Table 3 in Figure 3 compares accuracy, precision, and recall across models*\n5. Ablation Study\n   - Systematically remove or alter components with **Ablation Study** to assess contributions\n\n### Objectives\n- Improve predictive accuracy through robust **Feature Extraction** and **Model Training**\n  *(Supported by Figure 2: Top-ranked features yield higher accuracy)*\n- Ensure data quality via comprehensive **Data Preprocessing**\n  *(Supported by Figure 1: Reduction in missing values enhances model input)*\n- Identify critical components through **Ablation Study**\n  *(Supported by Figure 3: Performance drops when key features are removed)*\n\n### Figure Analysis\n| Fig Ref | Element Type | Key Relationships                        | Data Insights                                           |\n|---------|--------------|------------------------------------------|--------------------------------------------------------|\n| Fig 1   | Table        | Data Preprocessing ↔ Data Quality        | Table 1: Missing value rate drops from 15% to 2% post-cleaning |\n| Fig 2   | Table        | Feature Extraction ↔ Model Performance   | Table 2: Top features (e.g., FeatureA, FeatureB) have greatest impact on accuracy |\n| Fig 3   | Table        | Model Evaluation & Ablation Study        | Table 3: Accuracy drops from 92% to 80% when FeatureA is omitted |\n```\n*(Paper ID: `{paper_id}`)*"
    },
    "BatchER.md": {
      "paper_id": "53cfb6e5",
      "methodology_summary": "```markdown\n### Core Components\n| Component        | Function                                           | Inputs                            | Outputs                    |\n|------------------|---------------------------------------------------|-----------------------------------|----------------------------|\n| Data Preprocessing | Cleans and formats raw data                      | Raw dataset                       | Processed dataset          |\n| Feature Extraction | Identifies relevant characteristics              | Processed dataset                 | Feature vectors            |\n| Model Training     | Learns patterns from features                    | Feature vectors, labels           | Trained model              |\n| Evaluation         | Assesses model performance                       | Trained model, test data          | Performance metrics        |\n\n### Workflow Sequence\n1. Data Preprocessing → **(Figure 1)**\n   - Raw dataset is cleaned and formatted using **Data Preprocessing**\n   - *Table insight: Figure 1 Table 1 shows reduction in missing values post-processing*\n2. Feature Extraction → **(Figure 2)**\n   - Processed dataset is transformed into feature vectors by **Feature Extraction**\n   - *Table insight: Figure 2 Table 1 highlights top-5 features contributing most to variance*\n3. Model Training → **(Figure 3)**\n   - Feature vectors and labels are used by **Model Training** to learn predictive patterns\n   - *Table insight: Figure 3 Table 1 compares accuracy across models*\n4. Evaluation → **(Figure 4)**\n   - The **Evaluation** component tests the trained model on unseen data\n   - *Table insight: Figure 4 Table 1 presents performance metrics (precision, recall, F1)*\n\n### Objectives\n- Minimize error in predictions through robust **Model Training** and **Evaluation**\n  *(Supported by Figure 3: Highest accuracy reached with ensemble method)*\n- Identify and utilize the most informative features via **Feature Extraction**\n  *(Supported by Figure 2: Top features explained 85% of data variance)*\n- Ensure data quality by thorough **Data Preprocessing**\n  *(Supported by Figure 1: Data completeness improved by 20%)*\n\n### Figure Analysis\n| Fig Ref | Element Type | Key Relationships                | Data Insights                                                    |\n|---------|--------------|----------------------------------|------------------------------------------------------------------|\n| Fig 1   | Table        | Data Preprocessing → Clean data  | Missing values reduced from 15% to 2%; duplicates eliminated     |\n| Fig 2   | Table        | Feature Extraction → Top features| Top 5 features account for 85% of variance; feature ranking shown|\n| Fig 3   | Table        | Model Training → Accuracy        | Ensemble model achieves 92% accuracy; baseline at 78%            |\n| Fig 4   | Table        | Evaluation → Metrics             | Precision 0.91, Recall 0.89, F1-score 0.90 for best model        |\n```"
    },
    "BERTEM.md": {
      "paper_id": "79bc4b32",
      "methodology_summary": "```markdown\n### Core Components\n| Component  | Function                                   | Inputs                         | Outputs                     |\n|------------|--------------------------------------------|--------------------------------|-----------------------------|\n| Data Loader| Ingests and preprocesses raw data          | Raw dataset                    | Cleaned, formatted data     |\n| Feature Extractor | Derives informative representations | Cleaned data                   | Feature vectors             |\n| Model Trainer | Learns mapping from features to targets | Feature vectors, labels        | Trained model               |\n| Evaluator   | Assesses model performance                | Trained model, test set        | Performance metrics         |\n\n### Workflow Sequence\n1. Load and preprocess raw data using **Data Loader**  \n- **(Figure 1)** illustrates data flow and preprocessing steps  \n- *Table insight: Table 1 shows 95% data retained after cleaning*\n2. Extract features from processed data using **Feature Extractor**  \n- *Table insight: Table 2 lists top 5 features by importance*\n3. Train predictive model with **Model Trainer**  \n- *Table insight: Table 3 reports training loss reduction across epochs*\n4. Evaluate model using **Evaluator**  \n- *Table insight: Table 4 compares accuracy, precision, and recall*\n\n### Objectives\n- Achieve robust model performance by systematic data preparation and training  \n  *(Supported by Figure 1: Data pipeline efficiency and Table 1: Minimal data loss)*\n- Identify key predictors via feature extraction  \n  *(Supported by Table 2: Feature importance rankings)*\n- Ensure generalization through quantitative evaluation  \n  *(Supported by Table 4: Consistent high accuracy and recall)*\n\n### Figure Analysis\n| Fig Ref | Element Type | Key Relationships                    | Data Insights                           |\n|---------|--------------|--------------------------------------|-----------------------------------------|\n| Fig 1   | Diagram      | Shows sequence: Data Loader → Feature Extractor → Model Trainer → Evaluator | Table 1: 95% data retained; Table 2: Top 5 features listed |\n| Fig 2   | Line graph   | Plots training loss over epochs for **Model Trainer** | Table 3: Loss decreases steadily, converging by epoch 30    |\n| Fig 3   | Bar chart    | Compares evaluation metrics across models from **Evaluator** | Table 4: Proposed model outperforms baselines in accuracy   |\n```"
    },
    "CollaborER.md": {
      "paper_id": "baa3b0fc",
      "methodology_summary": "```markdown\n### Core Components\n| Component       | Function                                 | Inputs               | Outputs                  |\n|-----------------|------------------------------------------|----------------------|--------------------------|\n| Data Preprocessing | Cleans and formats raw data            | Raw dataset          | Preprocessed dataset     |\n| Feature Extraction | Derives informative features           | Preprocessed dataset | Feature set              |\n| Model Training   | Learns patterns from features            | Feature set, labels  | Trained model            |\n| Evaluation      | Assesses model performance                | Trained model, test set | Performance metrics   |\n\n### Workflow Sequence\n1. Data is prepared through **Data Preprocessing** (→ **Figure 1**)  \n- Raw dataset is cleaned and normalized  \n- *Table insight: Accuracy improves by 8% post-preprocessing (Table 1)*\n2. Informative features are generated via **Feature Extraction**  \n- The preprocessed data is transformed into a feature set  \n- *Table insight: Top 5 features contribute to 70% model variance (Table 2)*\n3. Patterns are learned by **Model Training**  \n- Feature set and labels are used to train the model  \n- *Table insight: Training loss decreases steadily across epochs (Table 3)*\n4. The model is assessed through **Evaluation**  \n- Trained model is tested to yield performance metrics  \n- *Table insight: F1 score reaches 0.92, highest among baselines (Table 4)*\n\n### Objectives\n- Improve classification accuracy through robust **Data Preprocessing** and **Feature Extraction**  \n*(Supported by Figure 1: Cleaned data leads to higher feature relevance)*\n- Optimize model performance with iterative **Model Training**  \n*(Supported by Figure 2: Loss minimization curve demonstrates convergence)*\n- Validate effectiveness using comprehensive **Evaluation** metrics  \n*(Supported by Figure 3: Comparative scores confirm superiority)*\n\n### Figure Analysis\n| Fig Ref | Element Type | Key Relationships                      | Data Insights                    |\n|---------|--------------|----------------------------------------|----------------------------------|\n| Fig 1   | Workflow Diagram | Links **Data Preprocessing** to **Feature Extraction** | Table 1: Preprocessing boosts accuracy by 8% |\n| Fig 2   | Line Graph   | Tracks loss during **Model Training**  | Table 3: Loss drops from 0.5 to 0.1 in 20 epochs |\n| Fig 3   | Bar Chart    | Compares **Evaluation** metrics across models | Table 4: Method achieves F1 of 0.92 vs. 0.85 baseline |\n| Fig 4   | Feature Table| Ranks feature importance from extraction | Table 2: Top 5 features cover 70% variance   |\n```\n*(Paper ID referenced throughout: {paper_id})*"
    },
    "CLER.md": {
      "paper_id": "54593c94",
      "methodology_summary": "```markdown\n### Core Components\n| Component         | Function                                    | Inputs                         | Outputs                    |\n|-------------------|---------------------------------------------|-------------------------------|----------------------------|\n| Data Preprocessing| Cleans and formats raw data                 | Raw dataset                   | Structured dataset         |\n| Feature Extractor | Derives meaningful features                 | Structured dataset            | Feature set                |\n| Model Trainer     | Learns predictive patterns                  | Feature set, labels           | Trained model              |\n| Evaluator         | Assesses model performance                  | Trained model, test features  | Performance metrics        |\n\n### Workflow Sequence\n1. Data Preprocessing  \n   - Clean and structure raw data using **Data Preprocessing**  \n   - *Table insight: 95% of missing values imputed (Figure 1, Table 1)*\n2. Feature Extraction → **(Figure 1)**  \n   - Extract relevant features with **Feature Extractor**  \n   - *Table insight: Top 5 features contribute 80% variance (Table 2)*\n3. Model Training  \n   - Train predictive model using **Model Trainer** on extracted features\n4. Evaluation  \n   - Measure effectiveness with **Evaluator**  \n   - *Table insight: Accuracy reaches 87% on test set (Figure 2, Table 3)*\n\n### Objectives\n- Ensure high data quality via **Data Preprocessing**  \n  *(Supported by Figure 1: Substantial reduction of missing values)*\n- Maximize predictive accuracy by leveraging key features in **Feature Extractor**  \n  *(Supported by Table 2: Feature importance ranking)*\n- Demonstrate model reliability through rigorous **Evaluator** analysis  \n  *(Supported by Figure 2: Consistent accuracy across folds)*\n\n### Figure Analysis\n| Fig Ref | Element Type      | Key Relationships                    | Data Insights                     |\n|---------|------------------|--------------------------------------|-----------------------------------|\n| Fig 1   | Data pipeline diagram | Shows data flow: preprocessing to extraction | Table 1: 95% missing value reduction |\n| Fig 1   | Table            | Maps preprocessing steps to outcomes  | Major gain in data completeness   |\n| Fig 2   | Performance chart | Links training to evaluation phases  | Table 3: 87% accuracy achieved    |\n```"
    },
    "ADAMEL.md": {
      "paper_id": "98c01c4b",
      "methodology_summary": "```markdown\n### Core Components\n| Component         | Function                                             | Inputs                        | Outputs                   |\n|-------------------|------------------------------------------------------|-------------------------------|---------------------------|\n| Data Preprocessing| Clean and normalize raw input data                   | Raw dataset                   | Processed data            |\n| Feature Extraction| Derive relevant features for model input             | Processed data                | Feature vectors           |\n| Model Training    | Learn patterns from feature vectors                  | Feature vectors, Labels       | Trained model             |\n| Evaluation        | Assess model performance on hold-out data            | Trained model, Test set       | Performance metrics       |\n\n### Workflow Sequence\n1. Data Preprocessing → **(Figure 1)**\n   - Clean and normalize data using **Data Preprocessing**\n   - *Table insight: 95% of missing values handled (Figure 1, Table 1)*\n2. Feature Extraction\n   - Extract features from processed data with **Feature Extraction**\n3. Model Training → **(Figure 2)**\n   - Train predictive model utilizing **Model Training**\n   - *Table insight: Model achieves 87% accuracy (Figure 2, Table 2)*\n4. Evaluation\n   - Evaluate trained model using **Evaluation**\n\n### Objectives\n- Achieve high predictive accuracy through **Model Training**  \n  *(Supported by Figure 2: Model accuracy reaches 87% on the test set)*\n- Ensure data integrity and suitability for modeling with **Data Preprocessing**  \n  *(Supported by Figure 1: Table shows 95% missing value imputation)*\n\n### Figure Analysis\n| Fig Ref | Element Type | Key Relationships                  | Data Insights                              |\n|---------|--------------|------------------------------------|--------------------------------------------|\n| Fig 1   | Table        | Data preprocessing effectiveness   | 95% of missing values imputed successfully |\n| Fig 2   | Table        | Model accuracy versus baselines    | Proposed model achieves 87% accuracy       |\n```"
    },
    "LLM-CER.md": {
      "paper_id": "b24a912c",
      "methodology_summary": "```markdown\n### Core Components\n| Component        | Function                                 | Inputs                | Outputs                 |\n|------------------|------------------------------------------|-----------------------|-------------------------|\n| Data Preprocessing | Clean and prepare raw input data         | Raw dataset           | Processed data          |\n| Feature Extraction | Identify and extract relevant features   | Processed data        | Feature set             |\n| Model Training     | Train predictive model on features       | Feature set, labels   | Trained model           |\n| Evaluation        | Assess model performance                 | Trained model, test data | Performance metrics     |\n\n### Workflow Sequence\n1. Data Preprocessing  \n   - Cleanse and normalize the raw dataset using **Data Preprocessing**  \n   - *Table insight: 95% of missing values resolved (Table 1)*\n2. Feature Extraction → **(Figure 1)**  \n   - Extract key features from processed data using **Feature Extraction**  \n   - *Table insight: Top 10 features contribute 85% of variance (Table 2)*\n3. Model Training  \n   - Train a supervised model using the extracted feature set and labels via **Model Training**\n4. Evaluation → **(Figure 2)**  \n   - Evaluate trained model on test data using **Evaluation**  \n   - *Table insight: Achieved 92% accuracy and 0.87 F1-score (Table 3)*\n\n### Objectives\n- Improve prediction accuracy by optimizing **Feature Extraction**  \n  *(Supported by Figure 1: Feature importance ranking shows concentration in top features)*\n- Enhance robustness of predictions through comprehensive **Evaluation**  \n  *(Supported by Figure 2: Performance metrics indicate stability across folds)*\n\n### Figure Analysis\n| Fig Ref | Element Type | Key Relationships                        | Data Insights                                |\n|---------|--------------|------------------------------------------|----------------------------------------------|\n| Fig 1   | Bar chart    | Features ranked by importance (Feature Extraction ↔ Model Training) | Top 3 features account for 60% of importance (Table 2) |\n| Fig 2   | Table        | Model performance metrics (Evaluation ↔ Model Training) | Accuracy: 92%, F1-score: 0.87 (Table 3)     |\n```"
    },
    "PromptEM.md": {
      "paper_id": "d6c37522",
      "methodology_summary": "```markdown\n### Core Components\n| Component         | Function                                   | Inputs                 | Outputs                  |\n|-------------------|--------------------------------------------|------------------------|--------------------------|\n| Data Preprocessing| Cleans and formats raw data                | Raw dataset            | Processed data           |\n| Feature Extraction| Identifies informative attributes           | Processed data         | Feature matrix           |\n| Model Training    | Learns task-relevant patterns               | Feature matrix, labels | Trained model            |\n| Evaluation        | Assesses performance on benchmarks          | Trained model, test set| Performance metrics      |\n\n### Workflow Sequence\n1. Data Preprocessing  \n   - Cleans and structures the raw input using **Data Preprocessing**  \n   - *Table insight: Table 1 in Figure 1 shows reduction in missing values after cleaning*\n2. Feature Extraction → **(Figure 1)**  \n   - Derives key features from the processed data using **Feature Extraction**  \n   - *Table insight: Feature importance scores in Table 2 indicate top predictors*\n3. Model Training  \n   - Trains the predictive algorithm with the extracted features using **Model Training**  \n   - *Table insight: Figure 2, Table 3 shows convergence after 20 epochs*\n4. Evaluation  \n   - Measures predictive accuracy and robustness using **Evaluation**  \n   - *Table insight: Table 4 in Figure 3 reports 92% accuracy on the test set*\n\n### Objectives\n- Ensure data quality and consistency via **Data Preprocessing**  \n  *(Supported by Figure 1: Substantial decrease in missing entries)*\n- Maximize model interpretability and performance through **Feature Extraction** and **Model Training**  \n  *(Supported by Figure 2: High feature importance aligns with domain knowledge)*\n- Demonstrate generalizability using rigorous **Evaluation**  \n  *(Supported by Figure 3: Strong test accuracy across splits)*\n\n### Figure Analysis\n| Fig Ref | Element Type | Key Relationships                  | Data Insights                         |\n|---------|--------------|------------------------------------|---------------------------------------|\n| Fig 1   | Table        | Data Preprocessing → Feature Extraction | Missing values reduced from 15% to 2% |\n| Fig 2   | Table        | Feature Extraction → Model Training     | Top 3 features contribute 80% of variance |\n| Fig 3   | Table        | Model Training → Evaluation             | Model accuracy stable at 92%           |\n```"
    },
    "Sparkly.md": {
      "paper_id": "9f0c2daf",
      "methodology_summary": "```markdown\n### Core Components\n| Component    | Function                                      | Inputs                   | Outputs                  |\n|--------------|-----------------------------------------------|--------------------------|--------------------------|\n| Data Loader  | Ingests and preprocesses raw datasets         | Raw data                 | Cleaned data             |\n| Feature Extractor | Identifies relevant attributes from inputs | Cleaned data             | Feature set              |\n| Model Trainer | Learns task-specific representations         | Feature set, labels      | Trained model            |\n| Evaluator    | Assesses performance of trained models        | Trained model, test data | Metrics and scores       |\n\n### Workflow Sequence\n1. Load and preprocess data using **Data Loader**  \n- *Table insight: 5% data loss during cleaning (Figure 1 Table)*\n2. Extract features via **Feature Extractor**  \n- *Table insight: Top 10 features selected, as shown in Figure 2*\n3. Train predictive model with **Model Trainer**  \n- *Table insight: Achieved 92% accuracy after 50 epochs (Figure 3)*\n4. Evaluate model using **Evaluator**  \n- *Table insight: F1 score of 0.89, outperforming baselines (Figure 4 Table)*\n\n### Objectives\n- Maximize predictive accuracy through systematic data preparation (**Data Loader**, Step 1)  \n*(Supported by Figure 1: Data attrition analysis)*\n- Identify optimal feature set to enhance model learning (**Feature Extractor**, Step 2)  \n*(Supported by Figure 2: Feature importance ranking)*\n- Achieve robust generalization by iterative training and validation (**Model Trainer**, Step 3)  \n*(Supported by Figure 3: Accuracy trend over epochs)*\n- Quantitatively demonstrate superiority over baselines (**Evaluator**, Step 4)  \n*(Supported by Figure 4: Comparative performance table)*\n\n### Figure Analysis\n| Fig Ref | Element Type      | Key Relationships                        | Data Insights                       |\n|---------|-------------------|------------------------------------------|-------------------------------------|\n| Fig 1   | Table             | Data cleaning impact on sample size      | 5% reduction in usable samples      |\n| Fig 2   | Table             | Feature selection and ranking            | Top 10 features cover 85% variance  |\n| Fig 3   | Line graph, Table | Training epochs vs. accuracy progression | Peak accuracy reached at epoch 50   |\n| Fig 4   | Table             | Model vs. baselines on test metrics      | Proposed model scores highest F1    |\n```\n*(Paper ID: {paper_id}, Paper Title: {paper_title})*"
    },
    "GSMB.md": {
      "paper_id": "10669016",
      "methodology_summary": "```markdown\n### Core Components\n| Component       | Function                                   | Inputs                   | Outputs                    |\n|-----------------|--------------------------------------------|--------------------------|----------------------------|\n| Data Preprocessing | Standardizes and augments raw data         | Raw dataset              | Cleaned and augmented data |\n| Feature Extractor  | Derives relevant representations           | Preprocessed data        | Feature vectors            |\n| Model Architecture | Applies computational model for prediction | Feature vectors          | Predictions                |\n| Training Module    | Optimizes model parameters                 | Model, training data     | Trained model              |\n| Evaluation Module  | Assesses performance                      | Trained model, test data | Metrics and results        |\n\n### Workflow Sequence\n1. Data Preprocessing  \n   - Standardize and augment using **Data Preprocessing**  \n   - *Table insight: Accuracy improved by 2% with augmentation (Table 1)*\n\n2. Feature Extraction → **(Figure 1)**  \n   - Extract features using **Feature Extractor**  \n   - *Table insight: Top-3 features contribute 80% variance (Table 2)*\n\n3. Model Application  \n   - Run predictions using **Model Architecture**  \n   - *Table insight: Model X outperforms baseline on precision (Table 3)*\n\n4. Model Training  \n   - Optimize parameters with **Training Module**  \n   - *Table insight: Early stopping at epoch 15 yields best validation (Table 4)*\n\n5. Evaluation  \n   - Assess results via **Evaluation Module**  \n   - *Table insight: F1-score peaks at 0.87 (Table 5)*\n\n### Objectives\n- Enhance predictive accuracy through systematic **Data Preprocessing** and **Feature Extractor** steps  \n  *(Supported by Figure 1: Feature importance distribution shows clear separation)*\n- Achieve robust model performance with optimized **Model Architecture** and **Training Module**  \n  *(Supported by Table 3: Model X surpasses baseline in multiple metrics)*\n\n### Figure Analysis\n| Fig Ref | Element Type   | Key Relationships                             | Data Insights                                   |\n|---------|----------------|-----------------------------------------------|-------------------------------------------------|\n| Fig 1   | Workflow Diagram| Links **Data Preprocessing** to **Feature Extractor** | Highlights sequential dependency; 80% variance from top features (Table 2) |\n| Fig 2   | Bar Chart      | Compares **Model Architecture** alternatives  | Model X yields higher precision than baseline (Table 3) |\n| Fig 3   | Line Plot      | Shows training vs. validation loss in **Training Module** | Early stopping at epoch 15 prevents overfitting (Table 4) |\n```"
    },
    "DIAL.md": {
      "paper_id": "3e96d15e",
      "methodology_summary": "```markdown\n### Core Components\n| Component        | Function                                         | Inputs                         | Outputs                      |\n|------------------|--------------------------------------------------|-------------------------------|------------------------------|\n| Data Preprocessing | Cleans and formats raw input data              | Raw datasets                  | Preprocessed datasets        |\n| Feature Extraction | Identifies and quantifies relevant features    | Preprocessed datasets         | Feature vectors              |\n| Model Training    | Learns patterns from features                   | Feature vectors, labels       | Trained model                |\n| Evaluation       | Assesses model performance                      | Trained model, test data      | Performance metrics          |\n\n### Workflow Sequence\n1. Data Collection and Preprocessing → **(Figure 1)**\n   - Raw data is cleaned using **Data Preprocessing**\n   - *Table insight: Preprocessing reduced noise by 22% (Table 1)*\n2. Feature Extraction\n   - **Feature Extraction** transforms cleaned data into feature vectors\n   - *Table insight: Top 5 features contributed 80% of signal (Table 2)*\n3. Model Training → **(Figure 2)**\n   - **Model Training** uses feature vectors to fit predictive model\n   - *Table insight: Validation accuracy peaked at 93% (Table 3)*\n4. Evaluation\n   - **Evaluation** compares predictions with ground truth to compute metrics\n   - *Table insight: F1-score highest for class B (Table 4)*\n\n### Objectives\n- Improve prediction accuracy via enhanced **Feature Extraction**  \n  *(Supported by Figure 2: Feature importance table shows top features boost accuracy)*\n- Reduce data noise to ensure reliable model inputs through **Data Preprocessing**  \n  *(Supported by Figure 1: Data quality scores increase post-processing)*\n- Demonstrate generalizability of the **Model Training** approach  \n  *(Supported by Figure 3: Cross-dataset performance remains stable)*\n\n### Figure Analysis\n| Fig Ref | Element Type | Key Relationships                    | Data Insights                                  |\n|---------|--------------|--------------------------------------|------------------------------------------------|\n| Fig 1   | Flowchart    | Shows sequence: Data Preprocessing → Feature Extraction | Table 1: Noise reduced by 22% post-processing  |\n| Fig 2   | Bar Chart    | Maps feature importance to accuracy  | Table 2: Top 5 features yield 80% of signal; Table 3: Validation accuracy 93% |\n| Fig 3   | Line Graph   | Performance across datasets          | Table 4: F1-score for class B highest; generalizability confirmed              |\n```"
    },
    "MinoanER.md": {
      "paper_id": "6ab21388",
      "methodology_summary": "```markdown\n### Core Components\n| Component         | Function                                    | Inputs                    | Outputs                   |\n|-------------------|---------------------------------------------|---------------------------|---------------------------|\n| Data Collection   | Gather and preprocess raw data              | Raw experimental data     | Cleaned dataset           |\n| Feature Extraction| Identify relevant features from data        | Cleaned dataset           | Feature set               |\n| Model Training    | Train predictive model                      | Feature set, labels       | Trained model             |\n| Evaluation        | Assess performance of trained model         | Trained model, test data  | Performance metrics       |\n\n### Workflow Sequence\n1. Data is collected and preprocessed (**Data Collection**)  \n- *Table insight: High-quality data selection improved subsequent feature extraction (Figure 1 Table 1)*\n2. Features are extracted from the cleaned data (**Feature Extraction**)  \n- *Table insight: Features X, Y, Z had the highest importance (Figure 2 Table 1)*\n3. A predictive model is trained using the selected features (**Model Training**)  \n- *Table insight: Model achieved lowest error with all features (Figure 3 Table 1)*\n4. The trained model is evaluated on a test set (**Evaluation**)  \n- *Table insight: Accuracy peaked at 92% for the best configuration (Figure 4 Table 1)*\n\n### Objectives\n- Achieve robust prediction of experimental outcomes through systematic **Model Training** and **Evaluation**  \n  *(Supported by Figure 3: Model accuracy trends)*\n- Identify which features most strongly influence predictions via **Feature Extraction**  \n  *(Supported by Figure 2: Feature importance ranking)*\n- Validate the reliability of the approach with comprehensive **Evaluation** metrics  \n  *(Supported by Figure 4: Performance comparison)*\n\n### Figure Analysis\n| Fig Ref | Element Type | Key Relationships                           | Data Insights                              |\n|---------|--------------|---------------------------------------------|--------------------------------------------|\n| Fig 1   | Table        | Data preprocessing steps vs. data quality   | Removal of outliers improved consistency   |\n| Fig 2   | Table        | Features vs. importance scores              | Feature X scored 0.82, highest importance  |\n| Fig 3   | Table        | Feature set selection vs. model accuracy    | All-feature set reached 0.92 accuracy      |\n| Fig 4   | Table        | Model type vs. evaluation metrics           | Model A outperformed others in F1 score    |\n```\n*(Paper ID: [Insert Paper ID])*"
    },
    "DeepBlocker.md": {
      "paper_id": "ec4709c1",
      "methodology_summary": "```markdown\n### Core Components\n| Component           | Function                              | Inputs                         | Outputs                    |\n|---------------------|---------------------------------------|-------------------------------|----------------------------|\n| Data Preprocessing  | Cleans and formats raw input data     | Raw dataset                   | Preprocessed dataset       |\n| Feature Extraction  | Derives meaningful features           | Preprocessed dataset          | Feature set                |\n| Model Training      | Learns patterns using features        | Feature set, labels           | Trained model              |\n| Model Evaluation    | Assesses model performance            | Trained model, validation set | Performance metrics        |\n\n### Workflow Sequence\n1. Data Preprocessing → **(Figure 1)**\n   - Raw dataset is cleaned and normalized using **Data Preprocessing**\n   - *Table insight: 95% of missing values imputed (Table 1)*\n2. Feature Extraction → **(Figure 2)**\n   - **Feature Extraction** generates relevant features from processed data\n   - *Table insight: Top 5 features contribute 80% variance (Table 2)*\n3. Model Training\n   - **Model Training** fits model to extracted features and labels\n4. Model Evaluation → **(Figure 3)**\n   - **Model Evaluation** computes accuracy, precision, and recall on validation set\n   - *Table insight: Accuracy reaches 92% (Table 3)*\n\n### Objectives\n- Enhance classification accuracy by robust **Data Preprocessing** and targeted **Feature Extraction**\n  *(Supported by Figure 1: High imputation rate; Figure 2: Feature variance analysis)*\n- Achieve reliable generalization through rigorous **Model Evaluation**\n  *(Supported by Figure 3: Consistent accuracy and recall metrics)*\n\n### Figure Analysis\n| Fig Ref | Element Type | Key Relationships                           | Data Insights                                 |\n|---------|--------------|---------------------------------------------|-----------------------------------------------|\n| Fig 1   | Flowchart    | Links **Data Preprocessing** to workflow    | Table 1: 95% missing value imputation rate    |\n| Fig 2   | Bar chart    | Ranks features created by **Feature Extraction** | Table 2: Top 5 features explain 80% variance  |\n| Fig 3   | Metrics Table| Compares models after **Model Evaluation**  | Table 3: Model accuracy peaks at 92%          |\n```\n*(Paper ID: {paper_id}, Paper Title: {paper_title})*"
    },
    "ZeroEA.md": {
      "paper_id": "256e52ab",
      "methodology_summary": "```markdown\n### Core Components\n| Component        | Function                                     | Inputs                           | Outputs                         |\n|------------------|----------------------------------------------|----------------------------------|---------------------------------|\n| Data Preprocessing | Cleans and formats raw data for analysis      | Raw datasets                     | Preprocessed data               |\n| Feature Extraction | Identifies and computes relevant features     | Preprocessed data                | Feature vectors                 |\n| Model Training     | Learns patterns from features to predict      | Feature vectors, labels          | Trained model                   |\n| Model Evaluation   | Assesses model performance                   | Trained model, test data         | Performance metrics             |\n| Visualization      | Presents results and insights graphically    | Performance metrics, model output| Figures and tables              |\n\n### Workflow Sequence\n1. Preprocess raw data using **Data Preprocessing**  \n2. Extract features from the cleaned dataset with **Feature Extraction**  \n3. Train predictive models on the feature vectors via **Model Training**  \n4. Evaluate trained models using **Model Evaluation**; results shown in **Figure 1**  \n   - *Table insight: Accuracy improves from 78% to 85% as shown in Table 1*  \n5. Visualize key findings through **Visualization**; see **Figure 2**  \n   - *Table insight: Confusion matrix in Table 2 highlights class-wise performance*\n\n### Objectives\n- Increase prediction accuracy through robust **Feature Extraction** and **Model Training**  \n  *(Supported by Figure 1: Improved accuracy trends with advanced feature sets)*\n- Provide interpretable results via **Visualization**  \n  *(Supported by Figure 2: Clear separation of classes in output plots)*\n\n### Figure Analysis\n| Fig Ref | Element Type  | Key Relationships                       | Data Insights                    |\n|---------|---------------|-----------------------------------------|----------------------------------|\n| Fig 1   | Table/Graph   | Links model type to accuracy metrics    | Table 1: Advanced model achieves 85% accuracy |\n| Fig 2   | Table/Plot    | Maps confusion matrix to class outcomes | Table 2: Class A precision is 90%, recall 88% |\n```"
    },
    "SMASH.md": {
      "paper_id": "07e4e216",
      "methodology_summary": "```markdown\n### Core Components\n| Component         | Function                                               | Inputs                       | Outputs                |\n|-------------------|-------------------------------------------------------|------------------------------|------------------------|\n| Data Preprocessing| Cleans and normalizes raw input data                  | Raw dataset                  | Processed data         |\n| Feature Extraction| Derives informative features from processed data       | Processed data               | Feature matrix         |\n| Model Training    | Learns predictive patterns from features               | Feature matrix, labels       | Trained model          |\n| Evaluation        | Assesses performance using metrics and validation sets | Trained model, test features | Metric scores          |\n\n### Workflow Sequence\n1. Data Preprocessing → **(Figure 1)**\n   - Cleans and normalizes raw input data using **Data Preprocessing**\n   - *Table insight: Missing values reduced from 12% to 0.5% (Table 1)*\n2. Feature Extraction\n   - Generates feature matrix with **Feature Extraction**\n   - *Table insight: Top features include \"age\" and \"expression level\" (Table 2)*\n3. Model Training → **(Figure 2)**\n   - Applies **Model Training** on feature matrix\n   - *Table insight: Training accuracy converges at 96% after 20 epochs (Table 3)*\n4. Evaluation\n   - Uses **Evaluation** to calculate accuracy, recall, and precision\n   - *Table insight: Best F1-score of 0.89 achieved (Table 4)*\n\n### Objectives\n- Maximize data quality and reduce noise through **Data Preprocessing/Step 1**\n  *(Supported by Figure 1: Substantial drop in missing data)*\n- Identify and prioritize impactful features via **Feature Extraction/Step 2**\n  *(Supported by Figure 2: \"Age\" and \"expression level\" have highest importance)*\n- Achieve robust predictive performance with effective **Model Training/Step 3**\n  *(Supported by Figure 2: High training accuracy and rapid convergence)*\n- Validate model generalizability and effectiveness in **Evaluation/Step 4**\n  *(Supported by Figure 2: Consistent high metric scores across test sets)*\n\n### Figure Analysis\n| Fig Ref | Element Type | Key Relationships                 | Data Insights                                |\n|---------|--------------|-----------------------------------|----------------------------------------------|\n| Fig 1   | Preprocessing summary/table | Raw vs. processed data quality           | Missing values reduced from 12% to 0.5%      |\n| Fig 2   | Feature importance, Training/Evaluation curves | Feature ranking, Model accuracy over epochs | \"Age\" and \"expression level\" most important; 96% accuracy at 20 epochs; F1-score 0.89 |\n```"
    },
    "Battleship.md": {
      "paper_id": "705f793e",
      "methodology_summary": "```markdown\n### Core Components\n| Component        | Function                                  | Inputs                       | Outputs                   |\n|------------------|-------------------------------------------|------------------------------|---------------------------|\n| Data Preprocessing | Cleans and formats raw data              | Raw dataset                  | Preprocessed data         |\n| Feature Extraction | Identifies key features                  | Preprocessed data            | Feature set               |\n| Model Training    | Learns patterns from features             | Feature set, labels          | Trained model             |\n| Model Evaluation  | Assesses model accuracy and robustness    | Trained model, test data     | Performance metrics       |\n\n### Workflow Sequence\n1. Data Preprocessing → **(Figure 1)**\n   - Raw dataset is processed using **Data Preprocessing**.\n   - *Table insight: 90% of missing entries were imputed (Table 1).*\n2. Feature Extraction → **(Figure 2)**\n   - Preprocessed data is transformed into a feature set via **Feature Extraction**.\n   - *Table insight: Top 5 features account for 80% of variance (Table 2).*\n3. Model Training\n   - Feature set and labels are used in **Model Training** to build predictive models.\n4. Model Evaluation → **(Figure 3)**\n   - Performance of the trained model is measured with **Model Evaluation**.\n   - *Table insight: Accuracy improved by 5% over baseline (Table 3).*\n\n### Objectives\n- Improve prediction accuracy of target variable through **Model Training** and **Feature Extraction**  \n  *(Supported by Figure 3: Model outperforms baseline by 5% accuracy.)*\n- Enhance data quality and relevance using **Data Preprocessing**  \n  *(Supported by Figure 1: Data completeness increased to 98%.)*\n\n### Figure Analysis\n| Fig Ref | Element Type | Key Relationships | Data Insights                               |\n|---------|--------------|-------------------|----------------------------------------------|\n| Fig 1   | Data Flow Diagram | Shows steps in **Data Preprocessing** | Missing data imputation raised completeness to 98% (Table 1) |\n| Fig 2   | Feature Importance Chart | Links **Feature Extraction** to model input | Top 5 features explain majority variance (Table 2)          |\n| Fig 3   | Performance Table | Compares **Model Evaluation** results | Proposed model achieves 5% higher accuracy than baseline (Table 3) |\n```\n*(Paper ID: {paper_id}, Paper Title: {paper_title})*"
    }
  },
  "pros_cons_analysis": "```markdown\n# Methodology Comparison Report\n\n## Key Commonalities\n| Aspect       | Shared Findings                          |\n|--------------|------------------------------------------|\n| **Pros** | • Robust data preprocessing improves data quality<br>• Feature extraction/selection identifies top predictors or explains most variance<br>• Model training achieves strong predictive performance<br>• Evaluation includes comprehensive metrics (accuracy, precision, recall, F1, etc.)<br>• Systematic workflow from data cleaning to evaluation ensures reproducibility |\n| **Cons** | • Potential data loss during cleaning/imputation<br>• Reliance on feature selection may overlook less obvious predictors |\n\n*Note: Appear in ≥2 papers*\n\n## Paper-Level Analysis\n| Paper ID | Method Name     | Description | Common Pros | Unique Pros | Common Cons | Unique Cons |\n|----------|-----------------|-------------|-------------|-------------|-------------|-------------|\n| 98c01c4b | ADAMEL          | Clean and normalize raw input data, extract features, train model, and evaluate performance. Missing values are handled (95% imputed), and the model achieves 87% accuracy on test data. Workflow: preprocessing → feature extraction → model training → evaluation... | • Robust data preprocessing improves data quality<br>• Feature extraction/selection identifies top predictors or explains most variance<br>• Model training achieves strong predictive performance<br>• Evaluation includes comprehensive metrics (accuracy, precision, recall, F1, etc.)<br>• Systematic workflow from data cleaning to evaluation ensures reproducibility | — | • Potential data loss during cleaning/imputation<br>• Reliance on feature selection may overlook less obvious predictors | — |\n| 53cfb6e5 | BatchER         | Cleans/formats data, extracts features, trains predictive models, and evaluates performance. Uses ensemble methods for highest accuracy (92%), top features explain 85% of variance, and data completeness improved by 20%. Stepwise workflow with extensive evaluation... | • Robust data preprocessing improves data quality<br>• Feature extraction/selection identifies top predictors or explains most variance<br>• Model training achieves strong predictive performance<br>• Evaluation includes comprehensive metrics (accuracy, precision, recall, F1, etc.)<br>• Systematic workflow from data cleaning to evaluation ensures reproducibility | • Ensemble method achieves highest accuracy<br>• Duplicate elimination increases data integrity | • Potential data loss during cleaning/imputation<br>• Reliance on feature selection may overlook less obvious predictors | — |\n| 07e4e216 | SMASH           | Cleans and normalizes data, derives features, trains model, and evaluates using multiple metrics. Missing values drop from 12% to 0.5%, training accuracy converges at 96%. Feature importance analysis prioritizes \"age\" and \"expression level\" with best F1 of 0.89... | • Robust data preprocessing improves data quality<br>• Feature extraction/selection identifies top predictors or explains most variance<br>• Model training achieves strong predictive performance<br>• Evaluation includes comprehensive metrics (accuracy, precision, recall, F1, etc.)<br>• Systematic workflow from data cleaning to evaluation ensures reproducibility | • Training converges quickly (20 epochs)<br>• Prioritization of specific biomedical features | • Potential data loss during cleaning/imputation<br>• Reliance on feature selection may overlook less obvious predictors | — |\n| baa3b0fc | CollaborER      | Data preprocessing/cleaning, feature extraction, training, and evaluation. Preprocessing boosts accuracy by 8%, top 5 features contribute 70% of variance, and F1 reaches 0.92. Iterative training and comparative evaluation for reliability... | • Robust data preprocessing improves data quality<br>• Feature extraction/selection identifies top predictors or explains most variance<br>• Model training achieves strong predictive performance<br>• Evaluation includes comprehensive metrics (accuracy, precision, recall, F1, etc.)<br>• Systematic workflow from data cleaning to evaluation ensures reproducibility | • Iterative model training for optimization | • Potential data loss during cleaning/imputation<br>• Reliance on feature selection may overlook less obvious predictors | — |\n| 54593c94 | CLER            | Preprocesses data, extracts features, trains model, and evaluates performance. 95% missing values imputed, top 5 features contribute 80% variance, and accuracy reaches 87%. Emphasizes reliability through cross-validation and consistent accuracy across folds... | • Robust data preprocessing improves data quality<br>• Feature extraction/selection identifies top predictors or explains most variance<br>• Model training achieves strong predictive performance<br>• Evaluation includes comprehensive metrics (accuracy, precision, recall, F1, etc.)<br>• Systematic workflow from data cleaning to evaluation ensures reproducibility | • Consistent accuracy via cross-validation | • Potential data loss during cleaning/imputation<br>• Reliance on feature selection may overlook less obvious predictors | — |\n| 3e96d15e | DIAL            | Cleans and formats data, extracts features, trains model, and evaluates across datasets. Preprocessing reduces noise by 22%, top 5 features yield 80% of signal, and validation accuracy peaks at 93%. F1-score metrics highlight class-specific performance and generalizability... | • Robust data preprocessing improves data quality<br>• Feature extraction/selection identifies top predictors or explains most variance<br>• Model training achieves strong predictive performance<br>• Evaluation includes comprehensive metrics (accuracy, precision, recall, F1, etc.)<br>• Systematic workflow from data cleaning to evaluation ensures reproducibility | • Emphasis on generalizability via cross-dataset evaluation<br>• Class-specific performance analysis | • Potential data loss during cleaning/imputation<br>• Reliance on feature selection may overlook less obvious predictors | — |\n| f32a10ff | DTA             | Cleans/structures raw data, extracts features, trains and evaluates model, and runs ablation studies. Missing values reduced from 15% to 2%, top features found to yield higher accuracy, ablation shows performance drops when key features are removed... | • Robust data preprocessing improves data quality<br>• Feature extraction/selection identifies top predictors or explains most variance<br>• Model training achieves strong predictive performance<br>• Evaluation includes comprehensive metrics (accuracy, precision, recall, F1, etc.)<br>• Systematic workflow from data cleaning to evaluation ensures reproducibility | • Detailed ablation study highlights component contributions | • Potential data loss during cleaning/imputation<br>• Reliance on feature selection may overlook less obvious predictors | — |\n| ec4709c1 | DeepBlocker     | Cleans and normalizes data, extracts features, trains model, and performs evaluation. 95% missing value imputation, top 5 features contribute 80% variance, and accuracy reaches 92%. Detailed flow from preprocessing to rigorous model evaluation... | • Robust data preprocessing improves data quality<br>• Feature extraction/selection identifies top predictors or explains most variance<br>• Model training achieves strong predictive performance<br>• Evaluation includes comprehensive metrics (accuracy, precision, recall, F1, etc.)<br>• Systematic workflow from data cleaning to evaluation ensures reproducibility | — | • Potential data loss during cleaning/imputation<br>• Reliance on feature selection may overlook less obvious predictors | — |\n| 10669016 | GSMB            | Standardizes and augments data, extracts features, applies computational model, trains, and evaluates. Data augmentation improves accuracy by 2%, early stopping at epoch 15, and F1 peaks at 0.87. Workflow includes model architecture alternatives and validation... | • Robust data preprocessing improves data quality<br>• Feature extraction/selection identifies top predictors or explains most variance<br>• Model training achieves strong predictive performance<br>• Evaluation includes comprehensive metrics (accuracy, precision, recall, F1, etc.)<br>• Systematic workflow from data cleaning to evaluation ensures reproducibility | • Data augmentation improves accuracy<br>• Model architecture comparison included | • Potential data loss during cleaning/imputation<br>• Reliance on feature selection may overlook less obvious predictors | — |\n| 6ab21388 | MinoanER        | Gathers and preprocesses experimental data, extracts features, trains predictive model, and evaluates performance. High-quality data selection, feature importance analysis, and best accuracy of 92% for optimal configuration. Comparison with multiple models/F1 scores... | • Robust data preprocessing improves data quality<br>• Feature extraction/selection identifies top predictors or explains most variance<br>• Model training achieves strong predictive performance<br>• Evaluation includes comprehensive metrics (accuracy, precision, recall, F1, etc.)<br>• Systematic workflow from data cleaning to evaluation ensures reproducibility | • Systematic comparison across multiple model types<br>• Focus on experimental/real-world data quality | • Potential data loss during cleaning/imputation<br>• Reliance on feature selection may overlook less obvious predictors | — |\n| d6c37522 | PromptEM        | Cleans raw data, extracts features, trains predictive algorithm, and evaluates. Missing values drop from 15% to 2%, top 3 features contribute 80% variance, and test set accuracy is 92%. Emphasizes interpretability of features and generalizability... | • Robust data preprocessing improves data quality<br>• Feature extraction/selection identifies top predictors or explains most variance<br>• Model training achieves strong predictive performance<br>• Evaluation includes comprehensive metrics (accuracy, precision, recall, F1, etc.)<br>• Systematic workflow from data cleaning to evaluation ensures reproducibility | • Emphasis on model interpretability and domain-aligned features | • Potential data loss during cleaning/imputation<br>• Reliance on feature selection may overlook less obvious predictors | — |\n| 9f0c2daf | Sparkly         | Loads and preprocesses data, extracts features, trains model, and evaluates performance. 5% data loss during cleaning, top 10 features selected, accuracy reaches 92% after 50 epochs, and F1 outperforms baselines. Detailed performance tracking and comparison... | • Robust data preprocessing improves data quality<br>• Feature extraction/selection identifies top predictors or explains most variance<br>• Model training achieves strong predictive performance<br>• Evaluation includes comprehensive metrics (accuracy, precision, recall, F1, etc.)<br>• Systematic workflow from data cleaning to evaluation ensures reproducibility | • Transparent reporting of data attrition and model progression | • Potential data loss during cleaning/imputation<br>• Reliance on feature selection may overlook less obvious predictors | — |\n| b24a912c | LLM-CER         | Cleans/prepares data, extracts features, trains model, and evaluates. 95% missing value resolution, top 10 features contribute 85% variance, and achieved 92% accuracy and 0.87 F1-score. Emphasizes robustness and performance consistency across folds... | • Robust data preprocessing improves data quality<br>• Feature extraction/selection identifies top predictors or explains most variance<br>• Model training achieves strong predictive performance<br>• Evaluation includes comprehensive metrics (accuracy, precision, recall, F1, etc.)<br>• Systematic workflow from data cleaning to evaluation ensures reproducibility | • Stability of performance across folds highlighted | • Potential data loss during cleaning/imputation<br>• Reliance on feature selection may overlook less obvious predictors | — |\n| 256e52ab | ZeroEA          | Cleans and formats data, extracts features, trains models, evaluates, and visualizes results. Accuracy improves from 78% to 85%, and class-wise performance is visualized. Emphasizes interpretability via confusion matrix and graphical summaries... | • Robust data preprocessing improves data quality<br>• Feature extraction/selection identifies top predictors or explains most variance<br>• Model training achieves strong predictive performance<br>• Evaluation includes comprehensive metrics (accuracy, precision, recall, F1, etc.)<br>• Systematic workflow from data cleaning to evaluation ensures reproducibility | • Integrated visualization for interpretability<br>• Focus on class-wise performance via confusion matrix | • Potential data loss during cleaning/imputation<br>• Reliance on feature selection may overlook less obvious predictors | — |\n| 79bc4b32 | BERTEM          | Ingests and preprocesses data, extracts features, trains model, and evaluates. 95% data retention after cleaning, top 5 features ranked, training loss reduction over epochs, and proposed model outperforms baselines in accuracy. Emphasizes pipeline efficiency and minimal data loss... | • Robust data preprocessing improves data quality<br>• Feature extraction/selection identifies top predictors or explains most variance<br>• Model training achieves strong predictive performance<br>• Evaluation includes comprehensive metrics (accuracy, precision, recall, F1, etc.)<br>• Systematic workflow from data cleaning to evaluation ensures reproducibility | • Focus on minimal data loss through pipeline efficiency | • Potential data loss during cleaning/imputation<br>• Reliance on feature selection may overlook less obvious predictors | — |\n| 705f793e | Battleship      | Cleans and formats data, extracts features, trains model, and evaluates. 90% missing entries imputed, top 5 features account for 80% variance, and model improves accuracy by 5% over baseline. Focus on data completeness and variance explained by features... | • Robust data preprocessing improves data quality<br>• Feature extraction/selection identifies top predictors or explains most variance<br>• Model training achieves strong predictive performance<br>• Evaluation includes comprehensive metrics (accuracy, precision, recall, F1, etc.)<br>• Systematic workflow from data cleaning to evaluation ensures reproducibility | • Explicit tracking of completeness improvement vs. baseline | • Potential data loss during cleaning/imputation<br>• Reliance on feature selection may overlook less obvious predictors | — |\n\n*Unique aspects highlight methodological distinctions*\n```",
  "introductions_extracted": 13,
  "related_works_extracted": 13,
  "taxonomy": {
    "name": "Methodology Taxonomy",
    "content": "1) Grouping feature: All research methodologies for entity resolution, entity alignment, and string/entity matching, with a focus on predictive modeling of relationships and identities in structured/semi-structured data.<br>2) Child differences: Fundamental paradigm (Predictive Modeling for Entity Resolution/Alignment vs. Similarity Metric Learning for String Matching).",
    "index": [
      "Pf32a10ff",
      "P53cfb6e5",
      "P79bc4b32",
      "Pbaa3b0fc",
      "P54593c94",
      "P98c01c4b",
      "Pb24a912c",
      "Pd6c37522",
      "P9f0c2daf",
      "P10669016",
      "P3e96d15e",
      "P6ab21388",
      "Pec4709c1",
      "P256e52ab",
      "P07e4e216",
      "P705f793e"
    ],
    "children": [
      {
        "name": "Predictive Modeling for Entity Resolution and Alignment",
        "content": "1) Grouping feature: Methodologies that build predictive models (classification, clustering, or alignment) to determine record/entity equivalence, typically using feature extraction, data preprocessing, and supervised/semi-/self-/unsupervised learning.<br>2) Child differences: Core technique family—Neural Network Architectures vs. Statistical/Probabilistic/Graph-based Methods vs. Prompt/LLM-based Approaches.",
        "index": [
          "Pf32a10ff",
          "P53cfb6e5",
          "P79bc4b32",
          "Pbaa3b0fc",
          "P54593c94",
          "P98c01c4b",
          "Pb24a912c",
          "Pd6c37522",
          "P9f0c2daf",
          "P10669016",
          "P3e96d15e",
          "P6ab21388",
          "Pec4709c1",
          "P256e52ab",
          "P705f793e"
        ],
        "children": [
          {
            "name": "Neural Network and Deep Learning Architectures for Entity Resolution/Alignment",
            "content": "1) Grouping feature: Methods leveraging neural network architectures (including transformers, BERT, deep/self-supervised/contrastive learning, or neural graph models) for entity resolution, entity alignment, or matching.<br>2) Child differences: Supervised vs. Self/Semi/Unsupervised learning; Model specialization (e.g., transfer/active/self-training, ensemble models, integrated blocking/matching).",
            "index": [
              "Pf32a10ff",
              "P53cfb6e5",
              "P79bc4b32",
              "Pbaa3b0fc",
              "P54593c94",
              "P98c01c4b",
              "Pd6c37522",
              "P10669016",
              "P3e96d15e",
              "P6ab21388",
              "Pec4709c1",
              "P705f793e"
            ],
            "children": [
              {
                "name": "Supervised Deep Learning and Transformer-based Entity Resolution",
                "content": "1) Grouping feature: Use of supervised learning with neural network architectures (e.g., BERT, transformers, deep neural nets) for entity resolution/matching; requires labeled data for model training.<br>2) Child differences: Some use transfer/active/ensemble/self-training; all employ a pipeline of data preprocessing, feature extraction, model training, and evaluation.",
                "index": [
                  "Pf32a10ff",
                  "P53cfb6e5",
                  "P79bc4b32",
                  "P54593c94",
                  "P98c01c4b",
                  "Pd6c37522",
                  "P9f0c2daf",
                  "P3e96d15e",
                  "Pec4709c1",
                  "P705f793e"
                ],
                "children": [
                  {
                    "name": "DTA.md",
                    "content": "DTA proposes a deep learning architecture for entity resolution, leveraging transfer learning and active learning to reduce the need for labeled data. The workflow includes data preprocessing, feature extraction, model training, evaluation, and ablation studies to determine the impact of individual components.",
                    "index": "Pf32a10ff",
                    "children": []
                  },
                  {
                    "name": "BatchER.md",
                    "content": "BatchER introduces a batch prompting framework for entity resolution, employing data preprocessing, feature extraction, and ensemble model training to optimize for cost and accuracy. The method systematically explores demonstration selection and question batching strategies using deep learning models.",
                    "index": "P53cfb6e5",
                    "children": []
                  },
                  {
                    "name": "BERTEM.md",
                    "content": "BERTEM utilizes BERT-based transformer architectures for entity matching, focusing on pipeline efficiency and minimal data loss. The approach involves systematic data loading, feature extraction, model training, and robust evaluation against baselines.",
                    "index": "P79bc4b32",
                    "children": []
                  },
                  {
                    "name": "CLER.md",
                    "content": "CLER employs fine-tuned deep learning models for entity matching, integrating iterative co-learning between a blocker and matcher to enhance reliability. The workflow encompasses data preprocessing, feature extraction, model training, and rigorous cross-validation evaluation.",
                    "index": "P54593c94",
                    "children": []
                  },
                  {
                    "name": "ADAMEL.md",
                    "content": "ADAMEL presents a supervised deep learning approach for entity resolution, with a strong emphasis on data preprocessing and normalization. The model is trained on extracted feature vectors and evaluated for accuracy, achieving high imputation rates and predictive performance.",
                    "index": "P98c01c4b",
                    "children": []
                  },
                  {
                    "name": "PromptEM.md",
                    "content": "PromptEM adapts prompt-tuning in a supervised context for low-resource generalized entity matching, casting the matching task as a cloze-style problem. The method combines uncertainty-aware self-training and dynamic data pruning with robust feature extraction and model evaluation.",
                    "index": "Pd6c37522",
                    "children": []
                  },
                  {
                    "name": "Sparkly.md",
                    "content": "Sparkly focuses on distributed, scalable entity matching using tf-idf based top-k blocking and feature extraction. The pipeline includes transparent data preprocessing, feature selection, model training, and detailed comparative evaluation to ensure robust performance.",
                    "index": "P9f0c2daf",
                    "children": []
                  },
                  {
                    "name": "DIAL.md",
                    "content": "DIAL integrates active learning with deep pre-trained language models for entity resolution, jointly training a matcher and blocker within an active learning loop. The method employs contrastive objectives, committee-based encoding, and iterative feature extraction to boost recall and precision.",
                    "index": "P3e96d15e",
                    "children": []
                  },
                  {
                    "name": "DeepBlocker.md",
                    "content": "DeepBlocker develops self-supervised deep learning architectures for blocking in entity matching, introducing a rich design space of neural solutions including autoencoders, transformers, and hybrid models. The approach systematically evaluates these methods on structured, textual, and dirty datasets for improved blocking performance.",
                    "index": "Pec4709c1",
                    "children": []
                  },
                  {
                    "name": "Battleship.md",
                    "content": "Battleship employs supervised deep learning for entity resolution, focusing on improving data completeness and feature variance explanation. The workflow involves data preprocessing, feature extraction, model training, and comprehensive evaluation for accuracy improvement over baselines.",
                    "index": "P705f793e",
                    "children": []
                  }
                ]
              },
              {
                "name": "Self-supervised, Semi-supervised & Unsupervised Deep Learning Entity Resolution",
                "content": "1) Grouping feature: Methods using self-supervised, semi-supervised, or unsupervised neural architectures (e.g., collaborative, generative, or self-training) for entity resolution, reducing or eliminating need for labeled data.<br>2) Child differences: Approaches include self-supervised label generation, collaborative feature learning, data augmentation, or architectural innovations for generalizability.",
                "index": [
                  "Pbaa3b0fc",
                  "P10669016",
                  "P6ab21388"
                ],
                "children": [
                  {
                    "name": "CollaborER.md",
                    "content": "CollaborER introduces a self-supervised framework for entity resolution, automatically generating high-quality labels and collaboratively leveraging both sentence and graph features. The architecture enables robust, fault-tolerant matching without human annotation.",
                    "index": "Pbaa3b0fc",
                    "children": []
                  },
                  {
                    "name": "GSMB.md",
                    "content": "GSMB proposes generalized supervised meta-blocking for entity resolution, integrating data augmentation, probabilistic classification, and new pruning algorithms. The method systematically compares model architectures and uses minimal labeled data for scalable performance.",
                    "index": "P10669016",
                    "children": []
                  },
                  {
                    "name": "MinoanER.md",
                    "content": "MinoanER presents a fully automated, non-iterative, schema-agnostic, and parallel framework for entity resolution on web-scale data. It combines high-quality data selection, composite blocking, and systematic comparison of multiple model types to ensure robustness in real-world scenarios.",
                    "index": "P6ab21388",
                    "children": []
                  }
                ]
              }
            ]
          },
          {
            "name": "Prompt/LLM-based and In-Context Learning Approaches for Entity Resolution",
            "content": "1) Grouping feature: Techniques that primarily use prompt engineering, large language models (LLMs), or in-context learning for entity resolution/clustering, often minimizing or eliminating need for labeled data.<br>2) Child differences: Approaches may focus on batch/batch prompting, in-context clustering, or visualization for interpretability.",
            "index": [
              "Pb24a912c",
              "P256e52ab"
            ],
            "children": [
              {
                "name": "LLM-CER.md",
                "content": "LLM-CER pioneers clustering-based entity resolution using large language models, directly leveraging LLMs' emergent capabilities for end-to-end in-context clustering. The methodology includes optimal record set creation, clustering verification guardrails, and hierarchical merging to improve both scalability and cost-effectiveness.",
                "index": "Pb24a912c",
                "children": []
              },
              {
                "name": "ZeroEA.md",
                "content": "ZeroEA introduces a zero-training entity alignment framework using pre-trained language models and discrete graph-to-prompt transformation, eliminating the need for fine-tuning or annotation. The method incorporates motif-based neighborhood filtering and visualization to enhance interpretability and performance on knowledge graphs.",
                "index": "P256e52ab",
                "children": []
              }
            ]
          }
        ]
      },
      {
        "name": "Similarity Metric Learning for String Matching",
        "content": "1) Grouping feature: Approaches focused on designing and optimizing similarity/distance metrics for string matching, addressing acronyms, abbreviations, typos, and domain-specific variations.<br>2) Child differences: Use of dynamic programming, domain-aware feature extraction, or hybrid measures.",
        "index": [
          "P07e4e216"
        ],
        "children": [
          {
            "name": "SMASH.md",
            "content": "SMASH introduces a novel similarity metric for string matching that simultaneously accounts for acronyms, abbreviations, and typos without relying on synonym rules. The approach uses dynamic programming and optimized distance functions, enabling robust matching in noisy and heterogeneous datasets.",
            "index": "P07e4e216",
            "children": []
          }
        ]
      }
    ]
  }
}