{
    "Introduction": {
        "content": "Entity Resolution (ER) is a fundamental data quality task that identifies and disambiguates real-world entities represented by different identifiers or descriptions across multiple records. In ER, a record denotes a specific data instance; an entity corresponds to a unique object represented by one or more potentially duplicate records; and attributes are the properties of each record, which may be numerical (e.g., price), categorical (e.g., brand), or textual (e.g., descriptions). ER aims to link and cluster these records, ensuring a clean and\n\n[^0]unified dataset [13, 15, 24, 55]. Traditional ER is computationally expensive, particularly for large datasets, as it often requires evaluating all possible pairs of records to detect duplicates. To reduce this cost, ER typically includes a pre-processing step, blocking or filtering [59], which partitions the dataset into (possibly overlapping) blocks, where records in different blocks are unlikely to refer to the same entity. The subsequent resolution phase, which involves matching and combining records, is the more resource-intensive step, ensuring that duplicates are grouped within the same cluster.\n\nExisting ER solutions can be broadly divided into three categories: crowdsourcing-based frameworks [74], machine learning (ML) and deep learning-oriented methods [23], and those leveraging large language models (LLMs) [43], such as GPT or LLAMA. Crowdsourcing frameworks rely on human intelligence to achieve highquality results but incur significant costs. For example, platforms like Amazon Mechanical Turk (AMT) typically charge around USD 0.02 per Human Intelligence Task (HIT) involving pairwise comparisons [77]. For a dataset containing 1,000 records, naive pairwise ER would require approximately $O\\left(10^{6}\\right)$ comparisons-amounting to around USD 20,000 costs. To mitigate such expenses, hybrid human-machine frameworks like CrowdER [77] employ automated techniques to eliminate obvious non-matches, reserving human effort for only the most uncertain pairs. Machine-based approaches, such as rule-based [4], ML-based [5], and deep learning-based [23] works, formulate pairwise matching as a classification problem. These models can be trained on labeled data to provide faster inference. Still, they require task-specific supervision and large amounts of labeled data, both of which are expensive and time-consuming to acquire. For instance, AMT charges USD 0.08 per labeling task [26], making the data annotation process costly and difficult. Recently emerged large language models, e.g., GPT-4, are pre-trained on massive corpora in a self-supervised manner [47] and have achieved remarkable results in a series of natural language processing and data science tasks like text generation [66], machine translation [25], data research and education [3]. General-purpose LLMs circumvent the need for expensive task-specific supervision and labeled data, thanks to simpler prompt-based interactions, making them easier to operate and iterate in downstream applications. Moreover, their lower costs-e.g., GPT-4o-mini charges only USD 0.15 per million tokens [1]-and superior performance on zero-shot tasks [38] make them an attractive alternative for entity resolution tasks.\n\nRecently, several works have pursued LLMs for ER. Narayan et al. [54] pioneered the exploration of GPT-3's potential for ER in a few-shot manner using task demonstrations. Their approach\n\n\n[^0]:    Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.\n    SKIMOD '26, May 31-June 5, 2026, Bengaluru, India\n    (C) 2026 Association for Computing Machinery.\n\n    ACM ISBN 978-1-4503-XXXX-X/18/06... \\$15.00\n    https://doi.org/XXXXXXX.XXXXXXX\n\n![img-0.jpeg](img-0.jpeg)\n\nFigure 1: (a) Ground truth ER result. (b) Pairwise matching and (c) Pairwise matching with batching (state-of-the-art) vs. (d) In-context clustering (this work) for ER. The combining phase is omitted for pairwise matching with batching [26], as it remains the same with pairwise matching [54].\nhas exhibited notable performance improvements compared to methods based on pre-trained language models (PLMs), e.g., Ditto [46]. ZeroMatch [89] takes a more advanced approach by using parameter-efficient fine-tuning, maintaining many pre-trained LLM variants from different domains, and selecting the most appropriate variant at inference time for zero-shot ER. Further advancements in prompt engineering for ER have been investigated in works such as [36, 53, 63, 80]. Li et al. [43] develop BoostER, a framework that applies LLMs to select the best answer from multiple different partitionings generated by existing ER tools. Their focus is on the next record pair selection problem, reducing the uncertainty of the current ER result. Additionally, batch prompting, which involves packing multiple pairwise questions into a single prompt to reduce the monetary cost of calling LLM APIs, has been studied in [26, 87].\n\nHowever, all state-of-the-art LLM-based ER methods rely on comparing record pairs and do not fully exploit the emergent capability of LLMs to directly cluster a set of records. Recent studies have shown that LLMs are surprisingly effective at clustering text data in a zero-shot or few-shot manner, preserving semantic meaning and context [73, 76, 88]. Given that entity resolution is essentially a clustering problem, LLMs' clustering capabilities can be directly leveraged to reduce both the time and monetary costs associated with end-to-end ER. To bridge this gap, we - for the first time - formulate and investigate the problem of in-context clustering-based entity resolution, that is, instructing an LLM to cluster different record sets to achieve the end-to-end ER task at a significantly lower cost and time. Consider the following example.\n\nExample 1. Figure 1(b) compares existing pairwise matching [54] with our proposed in-context clustering approach for an ER task. Figure 1(a) shows the records with ground-truth entity assignments, where each color represents an entity.\n\nPairwise questioning-based ER consists of two phases: matching and combining (excluding blocking for simplicity). Matching aims to determine if a pair of records refers to the same entity. Transitivity can reduce comparisons; for example, if records $a$ and $b$ are the same entity, and $b$ and $c$ are the same (or different), we can infer that $a$ and $c$ are also the same (or different). As shown in Figure 1(b), the matching phase concludes when all record pairs are compared explicitly, or their status is inferred through transitivity or anti-transitivity. In the combining phase, the final ER result is obtained by clustering records belonging to the same entity.\n\nIn contrast, our in-context clustering approach directly creates input record sets for clustering by the LLM, as depicted in Figure 1(d) (with a maximum size of 4). Records within each LLM cluster are considered the same entity (i.e., transitivity holds). For example, in the LLM-based clustering of the set $(1,2,5,7)$, records 1 and 2 are clustered together, indicating they are the same entity, while 2 and 5 belong to different entities (i.e., anti-transitivity), as reflected in their assignment to clusters $A=[1,2]$ and [5]. In cases where transitivity or anti-transitivity relationships are unknown, clusters are packed to form the next level of input sets, such as $(A, B, 5,6)$. This hierarchical process continues, merging clusters until all record pairs from different clusters satisfy anti-transitivity, yielding the final ER output.\n\nUnlike pairwise matching, which classifies two records at a time and heavily relies on transitivity, in-context clustering partitions multiple records simultaneously and hierarchically merges them. Each pairwise comparison requires an LLM API call, driving up both cost and time. In the worst case, the ER process requires comparing all pairs of records when they belong to different entities. As illustrated in Figure 1(b), pairwise matching requires 13 comparisons (i.e., 13 LLM API calls) for 8 records, even with the help of transitivity. Our method, however, requires only 5 record sets for clustering, leading to fewer LLM API calls, lower costs, and reduced running time. This efficiency becomes particularly significant for large-scale datasets, where processing costs and ER time are crucial.\n\nExample 2. An improved baseline, pairwise matching with batching [26], reduces costs by processing multiple pairwise questions in batches. With the same constraint of 4 records per LLM API call for a fair comparison (i.e., 2 pairwise questions in a batch), Figure 1(c) shows that batching reduces the number of API calls to 7, still higher than that of 5 by our in-context clustering approach. This inefficiency arises because the pairwise questioning format inherently introduces redundancy when conveying information to the LLM. For instance,\n\nexploring all possible relationships among four records require 6 pairwise questions, which necessitates 3 batches of 2 pairwise questions each. Even with transitivity and anti-transitivity among 4 records, the number of batches can only be reduced to 2. In contrast, our approach can capture all relationships among 4 records in a single API call, thus reducing the number of API calls. A key follow-up question is the highest possible information density (i.e., maximum set size constraint in our clustering solution) that the LLM can effectively process in a single API call ${ }^{1}$. This motivates our in-depth investigation in \u00a7 4.2.\n\nAlthough LLMs have demonstrated strong performance in ER via pairwise matching [26, 54], the application of in-context clustering for LLM-based ER remains largely unexplored. We observe that directly applying in-context clustering to ER, without a carefully designed mechanism, introduces several key challenges: (1) Clustering Performance Variability: The clustering performance of an LLM can vary significantly based on how the record sets are configured. Determining the optimal design for these sets-specifically in terms of size, diversity, and the ordering of records-is crucial. For instance, if the record set is too large, the LLM's performance may degrade due to long context lengths [44], whereas a smaller set size will result in excessive LLM API calls, thereby increasing the monetary cost. (2) Hallucination Risks: LLMs are prone to hallucinations, where the model generates outputs that may not be grounded in the input data [33]. As a result, the outputs of in-context clustering may not always be accurate. Addressing this challenge requires the development of effective result verification strategies, such as implementing LLM \"guardrails\" and record set regeneration. (3) Efficient Result Combination: Combining the outputs of the in-context clustering efficiently presents another challenge. The process involves balancing the trade-off between the quality of the end-to-end ER result and the number of LLM API calls, which directly affects the monetary cost. To address these challenges, we extensively explore the design space and introduce several novel algorithms that optimize the creation of input record sets, the validation of LLM clustering outputs, and the merging of clustering results.\n\nOur empirical evaluations reveal that in-context clustering generally performs best when each record set contains 9 records drawn from 4 distinct entities, with roughly equal representation per entity. For domain-specific datasets, however, the optimal configuration tends to involve fewer records and entities, as factors such as attribute type and noise can significantly impact clustering quality. Performance further improves when records from the same entity appear consecutively within the set. Building on these insights, we propose an effective record set creation strategy (\u00a7 5.2) that balances set size, diversity, and intra-set variation to maximize clustering performance while minimizing monetary cost. Across nine real-world datasets, our method LLM-CER (LLM-powered Clustering-based ER) yields substantial improvements-up to 150\\% in Accuracy (ACC) and 10\\% in FP-measure-while reducing API calls by up to $5 \\times$ and maintaining comparable monetary cost to the most cost-effective baseline (\u00a7 6.2). Additionally, scalability tests confirm that our method remains both effective and efficient as dataset size increases (\u00a7 6.5).\nContribution. Our main contributions are summarized below.\n\n[^0]- We are the first to apply a clustering-based approach using LLMs for entity resolution, exploring the design space of clusteringbased LLM ER (\u00a7 4).\n- We identify four key factors-set size, set diversity, set variation, and record order-and analyze their impact on the performance of in-context clustering (\u00a7 4.2).\n- We propose the Next Record Set Creation algorithm, which effectively addresses the four identified factors. To mitigate potential LLM misclassifications, we design a Misclustering Detection Guardrail and a Record Set Regeneration strategy for further refinement of results (\u00a7 5.2). Our experimental findings demonstrate that MDG incurs $10 \\%$ time overhead, while improving the FP-measure by up to $75 \\%$ (\u00a7 6.4).\n- We propose a Hierarchical Cluster Merging approach that generates record sets hierarchically and performs efficient merging, improving the quality of cluster merging and overall entity resolution performance (\u00a7 5.3).\n- We empirically evaluate our clustering-based end-to-end ER algorithm on nine real-world datasets. The results show that our method significantly improves ER quality, achieving up to 150\\% higher ACC, reduces API calls by up to $5 \\times$, while maintaining competitive cost efficiency compared to the most cost-effective baseline and demonstrating strong scalability (\u00a7 6).\n",
        "title": [
            "## 1 INTRODUCTION"
        ],
        "summary": "This section introduces the challenges of entity resolution (ER), reviews traditional and modern approaches\u2014including crowdsourcing, machine learning, and large language models (LLMs)\u2014and highlights the inefficiency of current pairwise matching strategies. The authors propose a novel LLM-powered in-context clustering approach that directly clusters records, offering significant reductions in computational cost and API calls compared to pairwise and batched methods, while overcoming issues such as clustering performance variability and LLM hallucinations through new algorithms for record set creation, result validation, and hierarchical merging. Empirical results across nine datasets demonstrate that their method, LLM-CER, achieves marked improvements in accuracy and efficiency, and the section outlines their key contributions in advancing clustering-based ER with LLMs."
    },
    "Problem Definition": {
        "content": "We first introduce our novel record set for in-context clustering and its key factors (\u00a74.1), followed by experiments on how these\nfactors impact clustering on individual record sets (\u00a74.2). Based on these results, we formally define the end-to-end ER problem (\u00a74.3).\nDifferent from pairwise input schemes used by recent LLM-based ER solutions [26, 43], we combine the advantages of LLMs and innovatively propose to input a record set. In particular, we pack a number of records in a set as the input prompt and ask the LLM to output a clustering of the records in this set. Such an in-context clustering scheme can make good use of the LLM's ability to process batch data [87], as well as cluster text data preserving semantic similarity [88]. Direct clustering of larger record sets reduces the number of questions to be prepared, subsequently minimizing the number of LLM API calls, the number of LLM tokens, and thereby obtaining high-quality answers at a lower cost and time.\n\nGiven a set of records as input to the LLM, the goal is to partition the records into clusters. We refer to this paradigm as \"in-context clustering\". Three key factors may influence the performance of the LLM within this clustering-based scheme: set size, set diversity, and set variation. The definitions are given below.\ni) Set size: the number of records in an input set.\nii) Set diversity: the number of distinct entities (i.e., clusters) within an input set.\niii) Set variation: the variability in cluster sizes within an input set, quantified by the coefficient of variation:\n\n$$\n\\operatorname{variation}(S)=\\frac{\\sigma(S)}{\\mu(S)}\n$$\n\nwhere $\\sigma(S)$ and $\\mu(S)$ denote the standard deviation and mean of the cluster sizes, respectively, in the input set $S$. This metric measures the degree of variability in cluster sizes relative to the mean.\n\nAdditionally, our follow-up experimental results demonstrate the importance of ordering similar records sequentially inside a record set, this enhances an LLM's in-context clustering performance by facilitating better context differentiation [8]. Similar findings were also demonstrated by prior RAG work [49], which shows that ordering semantically similar sentences together improves LLM output.\n\nFigure 3 presents an example of an LLM prompt for our novel in-context clustering-based entity resolution. Instead of pairwise ER, the prompt explicitly asks to cluster the records in an input set.\n\nExample 3. As shown in Figure 3, we use an LLM to group a record set containing 9 records, thus the set size is 9. The set diversity of this record set is 3 , since it contains three distinct entities (clusters), which are represented by three different colors. The mean of cluster sizes in this record set is $(3+3+3) / 3=3$, while the standard deviation of cluster sizes in the record set is $\\sqrt{\\frac{(3-3)^{2}+(3-3)^{2}+(3-3)^{2}}{3}}=0$, indicating a set variation of 0 , i.e., no variation in relation to three cluster sizes.\nWe examine the impact of varying set size $\\left(S_{s}\\right)$, set diversity $\\left(S_{d}\\right)$, and set variation $\\left(S_{v}\\right)$ on the LLM's performance for in-context clustering of individual record sets. We further explore how the order of records in a set affects the LLM's performance by implementing: sequential record order, where records belonging to the same entity are placed consecutively in the record set; and random record order, where the records are randomly placed. These\n\n![img-2.jpeg](img-2.jpeg)\n\nFigure 4: Clustering performance vs. set size and variation\n\n![img-3.jpeg](img-3.jpeg)\n\nFigure 5: Clustering performance vs. set diversity, ordering\n\nanalyses help us identify the optimal information density and structure that LLMs can handle within a single API call, hence guiding our design for creating effective record sets and fully utilizing the knowledge provided by blocking techniques (\u00a7 5).\n\nWe define three levels of set variations ($S_B$) to represent different degrees of imbalance within the record sets: balanced ($S_B < 0.3$), relatively balanced ($0.3 \\leq S_B \\leq 0.7$), and unbalanced ($S_B > 0.7$). For each fixed $S_B$, $S_d$, $S_0$, and record ordering, we uniformly at random select 200 questions (i.e., record sets) to query the LLM, provided that the total number of questions available under the given parameter settings exceeds 200. The ground truth for the questions corresponds to the correct clustering of the records within each record set. More detailed experimental settings, evaluation metrics, and dataset descriptions can be found in \u00a7 6.\n\nFigure 4 illustrates the in-context clustering performance of the LLM on individual record sets under varying $S_B$ and $S_0$. The results indicate that LLM performance consistently improves as set variation decreases, suggesting that clustering is more accurate when records are evenly distributed among entities within a set. This trend holds across all datasets and evaluation metrics, regardless of the set size. Moreover, the LLM maintains relatively stable and robust performance across balanced, relatively balanced, and unbalanced settings as the set size increases from 4 to 9. However, a marked decline in clustering accuracy is observed once the set size exceeds 9 or 10. This degradation can be attributed to the increased cognitive load placed on the model: larger sets require the LLM to process and compare more records simultaneously, which dilutes semantic focus and heightens the risk of clustering errors. Additionally, managing coherent contextual relationships across more records becomes increasingly challenging, further reducing accuracy. Across different set variation settings, the trends in FP-measure remain largely consistent, as this metric reflects the alignment between predicted clusters and ground truth labels. Taken together, these results suggest that the optimal configuration for in-context clustering occurs at a set size of 9, and that minimizing variation within each set is key to maximizing LLM performance.\n\nFixing the optimal values of $S_B$ and $S_0$ based on previous experiments, we further investigate the impact of $S_d$ and the ordering of records within record sets on the performance of LLM-based in-context clustering (Figure 5). We notice that when $S_d$ is set to 4, the in-context clustering performance of the LLM is better across all datasets in terms of FP-measure compared to other diversity settings. Thus, we consider $S_d = 4$ as our optimal diversity setting. With 4 entities, the clustering task provides enough granularity to differentiate entities while avoiding excessive sparsity or overlap in record distribution. In contrast, fewer entities (e.g., 2 or 3) might result in overly homogeneous clusters that fail to capture subtle differences, while a larger number of entities (e.g., 5) might introduce higher complexity and noise, negatively impacting the LLM's ability to identify distinct clusters accurately. Meanwhile, it is observed that the LLM achieves better in-context clustering performance when the sequential record order is used. This improvement can be attributed to the sequential arrangement of similar records, which likely enhances the coherence of contextual information presented to the LLM. By maintaining entity-related records in close proximity, this ordering reduces potential context-switching overhead and provides a more structured input sequence [49], thereby enabling the model to better discern subtle patterns and relationships essential for accurate clustering.\n\nIn conclusion, for the LLM's in-context clustering performance, our optimal configuration includes a set size of 9, a balanced distribution ($S_B$ close to 0), a set diversity of 4, and the sequential record order. Larger set sizes should be avoided, as the LLM's performance under extremely unbalanced ratios deteriorates rapidly beyond this point. Conversely, smaller set sizes should also be avoided, as maximizing the set size is crucial for reducing API call frequency, improving efficiency and minimizing token usage and cost. More analyses about the choice of key factors can be found in \u00a7 6.3.\nWe are now ready to define our main problems.\n\n**Problem 1. (Next Record Set Selection).** This problem is relevant for an LLM's in-context clustering during the entity matching phase. Given a set of records $\\mathcal{R}$, the current clustering result $\\mathbb{C}$, and predefined constraints on the set size ($S_B$), diversity ($S_d$), and variation ($S_0$), the Next Record Set Selection problem identifies a subset of records $\\mathcal{R}^* \\subseteq \\mathcal{R}$ satisfying the constraints ($S_B, S_d, S_0$) with the objective of minimizing the number of LLM API calls by leveraging the identified transitivity and anti-transitivity relationships.\n\n**Problem 2. (Cluster Combination).** Given the current clustering result $\\mathbb{C}$ and the LLM's in-context clustering output $\\mathbb{C}^*$ derived from a record set $\\mathcal{R}^*$, the Cluster Combination problem updates $\\mathbb{C}$ by combining the clusters in $\\mathbb{C}$ based on the merge decisions of the LLM over $\\mathbb{C}^*$, leveraging the identified transitivity and anti-transitivity relationships. This problem is relevant in combining phase to gradually refine the LLM's clustering outputs and obtain the final ER result.\n\nThe number of record sets generated by our algorithm determines the number of LLM API calls. Since each LLM API call roughly consists of the same number of tokens (e.g., see Figure 3), and an LLM's monetary cost and inference time are determined by the total number of input and output tokens, minimizing the number of LLM API calls also reduces the number of LLM tokens, monetary cost, as well as the end-to-end ER time. We discuss our solutions in the following section. In particular, the next record set selection is considered in \u00a75.2, while cluster combination is detailed in \u00a75.3. Notice that the existing pairwise ER scheme is a special case of our in-context clustering-based ER paradigm when the record set\n\n![img-4.jpeg](img-4.jpeg)\n\nFigure 6: overview of our end-to-end entity resolution solution LLM-CER\nsize $S_{x}=2$. Therefore, our problems and solutions generalize the state-of-the-art ER approaches.\n",
        "title": [
            "## 4 PROBLEM_ IN-CONTEXT CLUSTERING",
            "### 4.1 Record Set and Key Factors",
            "### 4.2 Empirical Evaluations for the Key Factors",
            "### 4.3 Problem Statements for End-to-end ER"
        ],
        "summary": "This section introduces a novel in-context clustering approach for entity resolution (ER) using large language models (LLMs), where sets of records\u2014rather than pairs\u2014are provided as input for clustering. The authors detail key factors affecting clustering performance: set size, diversity, variation, and record ordering, and experimentally determine that optimal results occur with balanced sets of size nine, diversity of four, and sequential ordering. They formally define two core problems\u2014Next Record Set Selection and Cluster Combination\u2014that structure the end-to-end ER process, aiming to minimize API calls, reduce costs, and maintain accuracy. The proposed paradigm generalizes prior pairwise methods and sets the stage for more efficient, scalable LLM-driven ER solutions."
    },
    "Methodology": {
        "content": "We present an overview of our solution, LLM-powered Clusteringbased ER (LLM-CER), before delving into details. Our key technical contributions are: (1) an effective Next Record Set Creation (NRS) algorithm for constructing optimal record sets for in-context clustering from initial blocks generated by standard blocking techniques.; (2) a guardrail technique, Misclustering Detection Guardrail (MDG) for assessing and subsequently improving the accuracy of the LLM's in-context clustering outputs; and (3) an efficient heuristic algorithm, Cluster Merge (CMR) for merging the in-context clustering outputs. Putting them together, we systematically explore the design space of clustering-based entity resolution powered by LLMs.\n\nAs illustrated in Figure 6, LLM-CER begins with blocking to generate initial candidate blocks (Section 5.1), which are then processed by the NRS algorithm (Algorithm 1) to construct record sets for in-context clustering by an LLM (Section 5.2). The clusterings are assessed and refined by the MDG algorithm (Algorithm 2), which identifies and corrects likely misclusterings. Improved clusters are then hierarchically merged using the CMR algorithm (Algorithm 3), which repeatedly invokes LLM-based clustering over newly formed record sets while leveraging transitivity and anti-transitivity (Section 5.3). This process continues until a termination condition is met, producing the final entity resolution result (Section 5.4).\nWe begin by introducing the filtering and blocking strategies used as a pre-processing step in the ER pipeline. Given a collection of records $\\mathcal{R}$, a na\u00efve approach compares all pairs, incurring a computational complexity of $\\mathrm{O}\\left(|\\mathcal{R}|^{2}\\right)$. To mitigate this, filtering and blocking techniques are employed [34, 51, 61]. Blocking groups similar records into blocks, while filtering typically applies similarity joins to identify candidate matches for each record. Although not the focus of our technical contributions, this work is the first to explore their trade-offs in the context of (1) LLM-based ER, which leverages large language models for ER [26, 43], and (2) clusteringbased ER, where resolution is achieved by iteratively clustering subsets of records rather than pairwise comparisons. We consider the following filtering and blocking strategies.\nFiltering-based Block Creation. Similarity joins identify records with similarity scores above a threshold using metrics like Jaccard similarity [6, 7]. While effective for detecting near-duplicates, pairwise comparisons incur $\\mathrm{O}\\left(|\\mathcal{R}|^{2}\\right)$ complexity, making filtering computationally expensive. To address this, positional filtering [84] prunes candidate records by leveraging token ordering, reducing complexity to $\\mathrm{O}(|\\mathcal{R}| \\cdot k)$ when $k$ candidates are allowed per record.\n\nThe similarity threshold $b_{t}$ is set empirically by maximizing the F1-score of clustering on a validation dataset [28], iterating thresholds from 0.05 to 0.95 in 0.05 increments. When ground truth is unavailable, we exploit LLM-based clustering results over a certain number of record subsets as ground truth.\nLocality Sensitive Hashing-based Blocking. Distributed representations effectively capture semantic similarities (e.g., \"p53\" and \"cancer\") in ER tasks [30]. While cosine similarity can measure embedding proximity, its $\\mathrm{O}\\left(|\\mathcal{R}|^{2}\\right)$ complexity becomes impractical for large datasets. Locality-sensitive hashing (LSH) [23] hashes similar embeddings into identical buckets with high probability, reducing complexity to $\\mathrm{O}(|\\mathcal{R}| \\cdot k)$, where $k$ is the number of buckets. However, the stochastic hash functions may co-locate dissimilar records, introducing false positives. To mitigate this issue, we retain only pairs with similarity exceeding a threshold $b_{t}$ to purify the initial blocks, where $b_{t}$ is determined via the aforementioned approach.\nCanopy Blocking [50]. Two thresholds $b_{s}$ and $m_{s}$, with $b_{s} \\geq m_{s}$, are utilized to balance efficiency and accuracy. A computationally inexpensive metric (e.g., inverted index-based edit distance on a single attribute) assigns record pairs with similarity $>b_{s}$ to the same block. Pairs with similarity $>m_{s}$ are grouped into overlapping canopies. Within these canopies, a refined metric (e.g., Jaccard similarity across all attributes) computes pairwise similarities. Matching pairs trigger transitive block merging: If records from two blocks match, the blocks are combined iteratively until convergence. While initial canopy formation retains quadratic complexity, its reliance on lightweight metrics makes it orders of magnitude faster than refined similarity computations [50]. We tune $b_{s}$ and $m_{s}$ empirically on validation datasets, or when ground truth is unavailable, derive labels via LLM-based clustering (as described earlier).\nRemark. We employ LSH as the default blocking method based on empirical results (\u00a76.4). We adopt the above filtering and blocking techniques for their simplicity and compatibility with our framework. As our primary technical contributions lie in the in-context clustering for matching (\u00a75.2) and the merging strategy for combining clusters (\u00a75.3), we leave the integration of advanced blocking methods-such as progressive blocking [27]-to future studies.\nWe develop novel in-context clustering for the matching phase.\nNext Record Set Creation Algorithm. Consider the records $\\mathcal{B}_{\\text {remain }}$ in a block that are not used in forming a record set yet. We design an algorithm NRS (Algorithm 1) to create the next record set with $\\mathcal{B}_{\\text {remain }}$. If $\\left|\\mathcal{B}_{\\text {remain }}\\right|$ is smaller than a predefined set size $\\left(S_{x}\\right)$, the next record set is created by grouping similar records sequentially, which improves an LLM's in-context clustering performance by facilitating better context differentiation, as shown in our experiment (\u00a74.2) (Lines 2-6). Otherwise, the next record set generation\n\nis optimized based on the empirical findings from $\\S 4.2$. Our results show that an LLM's in-context clustering accuracy depends on the record set design. In particular, we select an optimal record set size $S_{s}$, diversity $S_{d}$, and variation $S_{v}$ based on experimental results. We then apply the elbow method and $k$-means [18] to perform a preliminary clustering of records in $\\mathcal{B}_{\\text {remain }}$, while also assessing its diversity $k$. Next, we construct the next record set while ensuring the set size constraint $S_{s}$. For the record set, we minimize $S_{v}$ as much as possible and aim to meet the $S_{d}$ requirement (Lines 8-17).\n\nExperimental results indicate that the optimal values are $S_{s}=9$ and $S_{d}=4$, while LLM-based in-context clustering achieves better performance when $S_{v}$ is minimized, ideally approaching zero.\nMitigating In-context Clustering Errors of LLMs. In practice, LLMs are prone to hallucination [33], leading to seemingly plausible, yet factually incorrect contents. As a result, LLM-based incontext clustering outcomes for record sets may not be entirely reliable. To ensure high accuracy of the output, we further devise a misclustering detection guardrail (MDG) algorithm to verify the clustering results of the LLM, as detailed in Algorithm 2. We first define some key terms.\n\nDefinition 1. (Inter- and Intra-cluster Similarity) Given a similarity function $F(\\cdot)$ to measure the similarity between record pairs and a known record $r$, the intra-cluster similarity of $r$ is defined as the minimum similarity between $r$ and other records within the same cluster as $r$. The inter-cluster similarity of $r$ is defined as the maximum similarity between $r$ and records from other clusters.\n\nSpecifically, for any cluster from the in-context clustering output, if the intra-cluster similarity of a record inside it is lower than its inter-cluster similarity, Algorithm 2 indicates the record to be misclustered; and hence, the result is not acceptable. When computing similarity between records or clusters, our choice of similarity function is closely linked to the block creation method. For example, when using Filtering-based Block Creation, we employ Jaccard similarity to measure record similarity. In contrast, with LSH-based Block Creation, we assess similarity through cosine similarity applied to record embeddings. By default, we use cosine similarity for record embeddings unless otherwise specified. The overall time complexity of the MDG algorithm is $\\mathrm{O}\\left(S_{s}^{2}\\right)$, where $S_{s}$ is typically 9 . Given that $S_{s}$ is small and the computation is highly parallelizable, the actual runtime overhead remains low in practice.\nRecord Set Regeneration. For each misclustered record $r$, our end-to-end algorithm LLM-CER identifies the cluster with the highest inter-cluster similarity to $r$, and relocates it immediately after that cluster within the record set. This targeted adjustment leaves all other records unchanged and aims to bring semantically similar records closer together inside the record set, resulting in an overall time complexity of $\\mathrm{O}\\left(S_{s}\\right)$. As shown in $\\S 4.2$, LLM's clustering accuracy improves when records from the same entity appear consecutively. Next, we conduct in-context clustering with this more sequentially-ordered record set, enhancing the LLM's performance significantly, which is evident in our empirical results (\u00a76.4).\nLeveraging the optimal set size $S_{s}=9$ as stated earlier, each block is divided into several record sets (e.g., a block of size 27 is split into 3 record sets). Each record set is then in-context clustered independently by the LLM. For example, if record set $A$ is clustered\n\nAlgorithm 1 Next record set creation (NRS)\nInput: remaining records in a block $\\mathcal{B}_{\\text {remain }}, S_{s}, S_{d}, S_{v}$\nOutput: record set $\\mathcal{R}_{\\text {set }}$\n$\\mathcal{R}_{\\text {set }} \\leftarrow \\emptyset$\nif $\\left|\\mathcal{B}_{\\text {remain }}\\right| \\leq S_{s}$ then\n$r_{\\text {pre }} \\leftarrow$ first element in $\\mathcal{B}_{\\text {remain }}$\nwhile $\\left|\\mathcal{B}_{\\text {remain }}\\right|>0$ do\nadd $r_{\\text {pre }}$ to $R_{\\text {set }}$ and remove it from $\\mathcal{B}_{\\text {remain }}$\n$r_{\\text {pre }} \\leftarrow$ update $r_{\\text {pre }} \\mathrm{w} /$ its most similar record from $\\mathcal{B}_{\\text {remain }}$\nend while\nelse\n$k \\leftarrow$ compute diversity of $\\mathcal{B}_{\\text {remain }}$ using the elbow method\n$\\mathbb{B}_{\\text {remain, } k} \\leftarrow$ perform $k$-means on $\\mathcal{B}_{\\text {remain }}$\ntarget $_{\\text {size }} \\leftarrow\\left\\lfloor S_{s} / S_{d}\\right\\rfloor$\nfor $\\mathcal{B}_{\\text {remain, } i}$ in $\\mathbb{B}_{\\text {remain, } k}$ do\nif $\\left|\\mathcal{R}_{\\text {set }}\\right|<S_{s}$ and $\\left|\\mathcal{B}_{\\text {remain, } i}\\right| \\geq$ target $_{\\text {size }}$ then select target $_{\\text {size }}$ records from $\\mathcal{B}_{\\text {remain, } i}$ and add to $\\mathcal{R}_{\\text {set }}$ delete those records from $\\mathcal{B}_{\\text {remain }}$\nend if\nend for\nwhile $\\left|\\mathcal{R}_{\\text {set }}\\right|<S_{s}$ do\n$r_{\\text {set }} \\leftarrow$ find record in $\\mathcal{B}_{\\text {remain }}$ to least increase in $S_{v}$ (via Eq. 1) add $r_{\\text {set }}$ to $\\mathcal{R}_{\\text {set }}$ and delete from $\\mathcal{B}_{\\text {remain }}$\nend while\norder similar records together in $\\mathcal{R}_{\\text {set }}$ (similar to Lines 3-6)\nend if\nreturn $\\mathcal{R}_{\\text {set }}$\n\nAlgorithm 2 Misclustering Detection Guardrail (MDG)\nInput: In-context clustering result $\\mathrm{C}_{\\text {set }}$ of a record set\nOutput: whether in-context clustering result is acceptable (True/False)\nfor cluster $C_{i}$ in $\\mathrm{C}_{\\text {set }}$ do\nfor record $r_{j}$ in $C_{i}$ do\nif intra-cluster sim. of $r_{j}<$ inter-cluster sim. of $r_{j}$ then\nreturn False\nend if\nend for\nend for\nreturn True\ninto subsets $\\left\\{A_{1}, A_{2}, A_{3}\\right\\}$, record set $B$ into $\\left\\{B_{1}, B_{2}\\right\\}$, and record set $C$ into $\\left\\{C_{1}, C_{2}, C_{3}, C_{4}, C_{5}\\right\\}$, it becomes necessary to merge subsets across record sets to obtain the final partition.\n\nTo achieve this, we note that clusters within the same record set (e.g., $A_{1}$ and $A_{2}$ ) are already disjoint (i.e., belong to different entity groups) due to the LLM's in-context clustering and hence exhibit non-transitive relationships. However, clusters from different record sets (e.g., $A_{1}$ and $B_{1}$ ) may belong to the same entity group. Merging clusters across different record sets can be formulated as a $K$-dimensional maximum matching problem (analogous to a 3-dimensional maximum matching problem, which is proven to be NP-Hard [16]), where $K$ is the number of record sets.\n\nProblem 3 (Cluster Merging). Given $K$ record sets and their in-context clustering outputs, the Cluster Merging problem treats each output cluster as a \"new\" individual record for the next round and packs them optimally to construct record sets for the LLM's next round of in-context clustering. The problem is subject to the following conditions: (1) Each cluster (i.e., the corresponding \"new\" individual record) is selected exactly once in the next round of record sets to\n\n![img-5.jpeg](img-5.jpeg)\n\nFigure 7: An example of the next round of record sets creation. For simplicity, consider that each of the record sets $A, B, C, D$ generates three in-context clusters: $A=\\left\\{A_{1}, A_{2}, A_{3}\\right\\}$, $B=\\left\\{B_{1}, B_{2}, B_{3}\\right\\}$, etc. Assume that $S_{s} \\geq 4$ and $S_{d}=1$ for the sake of brevity. Then, the next round of record sets should pack the most similar clusters from $A, B, C$, and $D$, forming three new sets for the next round as follows: $\\left\\{A_{1}, B_{2}, C_{2}, D_{1}\\right\\},\\left\\{A_{2}, B_{1}, C_{1}, D_{2}\\right\\}$, and $\\left\\{A_{3}, B_{3}, C_{3}, D_{3}\\right\\}$. Each cluster appears exactly once in the next round of record sets to minimize the overall LLM API calls. Clusters from the same record set (e.g., both $A_{1}$ and $A_{2}$ ) are not packed together in the same record set of the next round owing to anti-transitivity.\nminimize the total number of LLM API calls. (2) Clusters from the same record set are never packed together in the same record set of the next round owing to anti-transitivity. (3) The next round of record sets should also ensure the set size constraint $S_{s}$, set diversity $S_{d}$, and minimize $S_{v}$ as much as possible.\n\nThe complexity of evaluating all combinations in the cluster merging problem is $\\mathrm{O}(K \\cdot n!)$, where $n$ is the average number of clusters per record set. This results in an exponential time cost, making it impractical for real-world applications with large blocks and multiple record sets.\n\nTo address this issue, we propose a heuristic Cluster Merge (CMR) algorithm (Algorithm 3) that avoids exhaustive enumeration. Specifically, for each cluster of a given record set, we identify its most similar cluster from the next record set based on a defined similarity measure. For instance, consider two record sets $A$ and $B$ with their in-context clustering results $A=\\left\\{A_{1}, A_{2}, A_{3}\\right\\}, B=\\left\\{B_{1}, B_{2}\\right\\}$. If $A_{1}$ is most similar to $B_{2}$, these two clusters are packed together. This process is repeated iteratively across all record sets (e.g., finding a cluster of the next record set $C$ that is most similar to the group $A_{1} B_{2}$ ), gradually forming a new record set for the next round of in-context clustering, while satisfying $S_{x}, S_{d}$, and $S_{v}$ requirements. Figure 7 shows an example of our cluster merging procedure that constructs record sets for the next round of clustering.\n\nFor simplicity, assume $K \\leq S_{s}$, where $K$ denotes the number of record sets in the current round, and $S_{s}$ is the optimal record set size. For ease of understanding, we also assume that each of these record sets generates $n$ in-context clusters by the LLM. Algorithm 3 outlines the process of constructing record sets for the next round\n\nAlgorithm 3 Cluster Merge (CMR): Depicting the creation of one record set for the next round\nInput: $K\\left(\\leq S_{s}\\right)$ record sets $\\left\\{\\mathcal{R}_{1}, \\ldots, \\mathcal{R}_{K}\\right\\}$ each with $n$ clusters; $S_{x}, S_{d}, S_{v}$ Output: one record set $\\mathcal{R}_{\\text {next }}$ for the next round with size $K$\nfor $i \\leftarrow 1$ to $K$ do\n$\\mathbb{C}_{i} \\leftarrow n$ output clusters of record set $\\mathcal{R}_{i}$\nreplace each cluster $C \\in \\mathbb{C}_{i}$ by a representative record $r_{C} \\in C$\nend for\n$\\mathcal{R}_{\\text {next }} \\leftarrow \\phi$\nfor $j \\leftarrow 1$ to $S_{d}$ do\n$w \\leftarrow(j-1) \\cdot\\left\\lceil K / S_{d}\\right\\rceil+1$\nselect a previously unselected cluster $C \\in \\mathbb{C}_{w}$\ninsert $C$ (i.e., its representative record $r_{C}$ ) into $\\mathcal{R}_{\\text {next }}$\nfor $i \\leftarrow(j-1) \\cdot\\left\\lceil K / S_{d}\\right\\rceil+2$ to $\\min \\left\\{j \\cdot\\left\\lceil K / S_{d}\\right\\rceil, K\\right\\}$ do\nfind cluster $C^{\\prime} \\in \\mathbb{C}_{i}$, previously unselected \\& most similar to $C$ insert $C^{\\prime}$ (i.e., its representative record $r_{C^{\\prime}}$ ) into $\\mathcal{R}_{\\text {next }}$\nend for\nend for\nreturn $\\mathcal{R}_{\\text {next }}$\nof in-context clustering. Given $K$ record sets, the algorithm first generates clusters for each record set using the LLM, then replaces each cluster with a representative \"new\" record. This replacement is possible because the records within a cluster represent the same entity and satisfy transitivity. The representative record can either be uniformly chosen at random or selected as the one with the smallest distance from the average embedding of the records in that cluster (Lines 1-3). Then, it partitions the $K$ record sets into groups of size $\\left\\lceil K / S_{d}\\right\\rceil$, iterating over these groups to construct a record set $\\mathcal{R}_{\\text {next }}$ for the next round (Lines 5-6). For each group, it selects an unselected cluster $C$ from the first record set of the group and uses its representative record as the initial record (Lines 7-8). Subsequently, for each remaining record set in the group, it identifies the most similar unselected cluster $C^{\\prime}$ to the previously selected cluster $C$ and adds its representative record to $\\mathcal{R}$ next (Lines 9-11). Finally, the resulting record set is returned for the next round of in-context clustering (Line 12). This process also ensures that (1) similar elements of $\\mathcal{R}$ next are sequentially ordered; and (2) we meet the $S_{d}$ requirement, while minimizing $S_{v}$ as much as possible.\n\nConsidering an average of $n$ clusters from a record set, the complexity of comparing clusters between two record sets is $\\mathrm{O}\\left(n^{2}\\right)$. Since there are $K$ record sets in the current round, the total complexity of CMR algorithm is $\\mathrm{O}\\left(K \\cdot n^{2}\\right)$ in the current round. This represents a significant reduction from the original exponential complexity $\\mathrm{O}\\left(K \\cdot n!\\right)$. The maximum value of $n$ can be up to the record set size, which is typically 9 in our experiments. This is significantly smaller than the total number of records $|\\mathcal{R}|$, leading to a reduced complexity in real-world usage. Moreover, the total hierarchy layers remain limited as observed in our experiments and summarized in Table 3 (\u00a75.2).\nAlgorithm 4 presents our end-to-end entity resolution framework outlining the key steps, given the input set of records $\\mathcal{R}$. The algorithm begins by applying blocking functions to $\\mathcal{R}$, generating initial set of blocks $\\mathbb{B}(\\S 5.1)$, after which we build a disconnected graph based on it to keep track of similar records. For each block in $\\mathbb{B}$, record sets are generated using the NRS algorithm (Algorithm 1), they are in-context clustered leveraging an LLM, with acceptable outputs verified by the MDG algorithm (Algorithm 2) given in $\\S 5.2$. Subsequently, the clustering outputs of these record sets are merged\nInput: A set of records $\\mathcal{R}$\nOutput: final partition $\\mathbb{C}$ of $\\mathcal{R}$\n$1: \\mathbb{B} \\leftarrow$ apply blocking functions on $\\mathcal{R}$ and get initial blocks\n2: generate record sets for each block $\\mathcal{B} \\in \\mathbb{B}$ using NRS algorithm\n3: in-context clustering of record sets with LLM\n4: check correctness of in-context clustering by MDG algorithm\n5: record sets regeneration to improve in-context clustering if needed\n6: while True do\n7: generate record sets for next round using Cluster Merge algorithm\n8: in-context clustering of record sets with LLM\n9: check correctness of in-context clustering by MDG algorithm\n10: record sets regeneration to improve in-context clustering\n11: if Exit condition then\n12: Break\n13: end if\n14: end while\n15: Use each singleton cluster to reconstruct the final partition $\\mathbb{C}$\n16: return $\\mathbb{C}$\nhierarchically using the CMR algorithm (Algorithm 3) and next rounds of in-context clustering (\u00a75.3).\nExit Condition. We keep generating hierarchical record sets for in-context clustering and subsequent cluster merging. The process continues until no more clusters can be merged owing to anti-transitivity. Here, anti-transitivity is identified by in-context clustering of the record sets. In particular, consider a current round when the in-context clustering outputs only singleton clusters, i.e., each cluster having only one element from the current round. The exit condition is thus satisfied. A naive method for the \"final check\" is to conduct pairwise comparisons of all singleton clusters to ensure that, indeed, no more merging is feasible, which costs quadratic time in the number of singleton clusters. During this final check, we, however, adopt a more efficient method by packaging multiple singleton clusters in a record set to reduce the number of LLM API calls, which is discussed in the following theoretical analysis.\n\nFinally, we use each singleton cluster to reconstruct the full partition $\\mathbb{C}=\\left\\{\\mathcal{C}_{1}, \\ldots, \\mathcal{C}_{n}\\right\\}$ and complete the ER process.\nTheoretical Analysis. We analyze the number of LLM API calls required by our LLM-CER method, focusing on a single block of $m$ records. To highlight the efficiency benefits, we consider two extreme cases that capture the boundaries of practical scenarios. (1) All records belong to same entity. For each hierarchy level, the number of records is reduced by a factor of the record set size $\\left(S_{s}\\right)$. Hence, the total number of LLM API calls equals $\\left(\\left\\lceil m / S_{s}\\right\\rceil+\\left\\lceil\\left\\lceil m / S_{s}\\right\\rceil / S_{s}\\right\\rceil+\\ldots+1\\right) \\approx\\left\\lceil m /\\left(S_{s}-1\\right)\\right\\rceil$. For larger $S_{s}$, the series converges quickly, and the total number of LLM API calls reduces significantly, justifying the superiority of our in-context clustering approach over pairwise comparisons (whose $S_{s}=2$ ) in the end-to-end ER. Specifically in this case, the API calls reduce from $O(m)$ in pairwise with transitivity to only $O\\left(\\frac{m}{S_{s}}\\right)$ in LLM-CER. (2) All records belong to different entities. For hierarchy level 0 , there are $p=\\left\\lceil m / S_{s}\\right\\rceil$ record sets, resulting in $p$ LLM API calls. For simplicity, assume $m=p \\cdot S_{s}$, i.e., each record set has exactly $S_{s}$ records. Since all records are unique, each record forms a singleton cluster, satisfying the clustering exit condition immediately. However, a final disambiguation step is still needed to verify cross-set matches. This involves comparing each record to all others outside its record set, resulting in $\\frac{m\\left(m-S_{s}\\right)}{2}$ comparisons. Using our in-context framework, we batch these comparisons into new record sets. Each set\n\nTable 1: Dataset statistics: Rec. stands for records, ent. represents entities, attr. denotes attributes, $E_{d}$ indicates entity dispersion[10] ( $=\\#$ Rec./\\#Ent.). The 'T' in Attr. Types denotes 'Textual', 'N' represents 'Numeric', 'C' indicates 'Categorical', the number after the attribute category indicates the count of attributes of the corresponding type.\n\n| Datasets | Domain | \\#Rec. | \\#Ent. | $E_{d}$ | \\#Attr. <br> per Rec. | Attr. <br> Types |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| Alaska | Product | 12 k | 1.48 k | $\\approx 8$ | 9 | T(9) |\n| AS | Geo | 2.26 k | 0.33 k | $\\approx 7$ | 1 | T(1) |\n| Song | Music | 4.85 k | 1.19 k | $\\approx 3$ | 7 | T(4), N(3) |\n| Music- <br> 20K | Music | 19.3 k | 10 k | $\\approx 2$ | 6 | $\\begin{gathered} \\mathrm{T}(4), \\mathrm{N}(1) \\\\ \\mathrm{C}(1) \\end{gathered}$ |\n| DBLP- <br> Google | Citation | 7.63 k | 2.35 k | $\\approx 3$ | 4 | T(3), N(1) |\n| Cora | Citation | 1.29 k | 0.11 k | $\\approx 12$ | 12 | T(12) |\n| Cite- <br> ser | Citation | 9.13 k | 2.49 k | $\\approx 4$ | 6 | $\\begin{gathered} \\mathrm{T}(4), \\mathrm{N}(1) \\\\ \\mathrm{C}(1) \\end{gathered}$ |\n| Amazon- <br> Google | Software | 2.16 k | 0.99 k | $\\approx 2$ | 3 | T(2), N(1) |\n| Walmart- <br> Amazon | Electronics | 1.81 k | 0.85 k | $\\approx 2$ | 5 | $\\begin{gathered} \\mathrm{T}(3), \\mathrm{N}(1) \\\\ \\mathrm{C}(1) \\end{gathered}$ |\n\nof size $S_{s}$ enables $\\frac{S_{s}\\left(S_{s}-1\\right)}{2}$ pairwise comparisons, meaning we require $\\frac{m\\left(m-S_{s}\\right)}{S_{s}\\left(S_{s}-1\\right)}$ LLM API calls for the final check. Including level 0 , the total number of calls is $\\frac{m}{S_{s}}+\\frac{m\\left(m-S_{s}\\right)}{S_{s}\\left(S_{s}-1\\right)}=\\frac{m(m-1)}{S_{s}\\left(S_{s}-1\\right)}$, which again is much smaller than the $\\frac{m(m-1)}{2}$ pairwise calls needed in conventional methods. This yields a non-trivial reduction by a factor of $\\mathrm{O}\\left(S_{s}^{2}\\right)$. Since all practical scenarios are between these two extreme cases, our theoretical analysis demonstrates that our design significantly reduces the LLM API calls compared to pairwise ER.\nAn ER algorithm receives an input set of records $\\mathcal{R}$ and returns a partition of them: $\\mathbb{C}=\\left\\{C_{1}, C_{2}, \\ldots, C_{n}\\right\\}$, such that $C_{i} \\cap C_{j}=\\emptyset$ for all $i, j$, and $\\cup_{i} C_{i}=\\mathcal{R}$. Each $C_{i}$ is called a cluster or a group of $\\mathcal{R}$ and represents a distinct real-world entity. If two records $r_{i}, r_{j}$ refer to the same (different) entity, they are denoted as $r_{i}=r_{j}\\left(r_{i} \\neq r_{j}\\right)$.\n\n| Prompt for LLM-based in context clustering ER |  |\n| :--: | :--: |\n| You are a worker with rich experience performing Entity Resolution tasks. Following are some duplicated records. Group these records by entity, placing the 2Ds of records that belong to the same entity together. Return the result as a 2D array, where each row corresponds to a different entity and contains the 2Ds of the records identified as belonging to that entity. Here are the records: <br> Record one: - - - - none: Alice, Criterion, age 25, email: alice.johnson@gmail.com, phone: none <br> Record two: - - - - none:A Criterion, age 25, email: alice.johnson@gmail.com, phone: $123-456-7890$ Record three: - - - - none: A C, age 25, email: alice.j@google.com, phone: $123-456-7890$ Record four: - - - - none: Hence Lee, age: 23, email: 14.@google.com, phone: none Record five: - - - - none: N Lee, age: 23, email: 14.@google.com, phone: $188-888-8888$ Record six: - - - - none: Hence L, age: none, email: 14.@google.com, phone: $188-888-8888$ Record seven: - - - - none: Mike Vills, age: 22, email: @Vills@Tummel.com, phone: $166-666-6666$ Record eight: - - - - none: - - - - none: - - - - none: - - - - none: - - - - none: - - - - none: - - - - |  |\n|  |  |\n|  |  |\n\nFigure 3: Example zero-shot prompt for in-context clustering\nThere are three key steps in entity resolution, blocking/ filtering, matching, and combining [15, 29, 58]. Blocking and filtering aim to separate almost dissimilar records and keep similar records in the same block to reduce the number of comparisons in the subsequent matching step and make the algorithm more efficient [60]. Matching determines whether records in the same block refer to the same entity. Finally, combining creates the final clusters of entities by inferring indirect matching relations following the matching step.\n\nIn pairwise ER, a pairwise similarity function is used over each pair of records to find the matching pairs. In this case, each pair is referred to as a question posed to the similarity function. An ER algorithm generally obeys transitivity and anti-transitivity rules, which can be employed in the matching and combining phases to further reduce the number of questions [4].\nTransitivity. Given three records $r_{1}, r_{2}$, and $r_{3}$, if $r_{1}=r_{2}$ and $r_{2}=r_{3}$, then we have $r_{1}=r_{3}$.\nAnti-transitivity. Given three records $r_{1}, r_{2}$, and $r_{3}$, if $r_{1}=r_{2}$ and $r_{2} \\neq r_{3}$, then we have $r_{1} \\neq r_{3}$.\n\nA clustering $\\mathbb{C}$ of the input set of records $\\mathcal{R}$ is transitively closed. For instance, if a pairwise similarity function decides $r_{1}=r_{2}$ and $r_{2}=r_{3}$, then we can infer $r_{1}=r_{3}$ without employing the similarity function on the pair $\\left(r_{1}, r_{3}\\right)$. This reduces the number of questions.\nDeep neural network-based models have long been effective as similarity functions for record matching, typically taking a pair of records as input and predicting whether they refer to the same realworld entity [35, 42, 56]. However, these methods demand large quantities of high-quality, task-specific labeled data, which can be costly to obtain. In contrast, large language models (LLMs)-pretrained on massive text corpora via self-supervision-have recently demonstrated strong zero-shot performance on ER matching tasks [26, 36, 43, 54, 64, 80]. Their extensive prior knowledge and superior classification capabilities [8, 12, 71] make them well suited to entity disambiguation. Figure 2 illustrates a typical LLM prompt for pairwise ER. Our approach adopts a purely zero-shot strategy: rather than fine-tuning or providing in-context examples, it relies entirely on the LLM's internal representations to assess record similarity. Consequently, the underlying similarity function remains latent, rather than implemented as an explicit, hand-crafted metric.\nWe discuss the background on the entity resolution (ER) problem (\u00a73.1), followed by how pre-trained LLMs can be adapted for ER in a zero-shot prompting paradigm (\u00a73.2).\nJiajie Fu<br>Zhejiang University<br>jiajiefu@zju.edu.cn<br>Sharad Mehrotra<br>University of California, Irvine<br>sharad@ics.uci.edu\n\nHaitong Tang<br>Zhejiang University<br>tht@zju.edu.cn<br>Xiangyu Ke<br>Zhejiang University<br>xiangyu.ke@zju.edu.cn\n\nArijit Khan<br>Aalborg University<br>arijitk@cs.aau.dk<br>Yunjun Gao<br>Zhejiang University<br>gaoyj@zju.edu.cn\n",
        "title": [
            "## 5 SOLUTION_ END-TO-END ER",
            "### 5.1 Filtering and Blocking",
            "### 5.2 In-context Clustering of a Record Set",
            "### 5.3 Hierarchical Cluster Merging",
            "### 5.4 End-to-End ER Solution",
            "## Algorithm 4 End-to-end ER",
            "### 3.1 Entity Resolution",
            "### 3.2 LLM_ Adaptation Techniques",
            "## 3 PRELIMINARIES",
            "# In-context Clustering-based Entity Resolution with Large Language Models_ A Design Space Exploration"
        ],
        "summary": "This section introduces LLM-CER, a novel framework for entity resolution that leverages large language models (LLMs) for clustering-based matching, detailing its core components: an optimized Next Record Set Creation (NRS) algorithm for constructing effective in-context clustering batches, a Misclustering Detection Guardrail (MDG) to identify and correct LLM errors, and a heuristic Cluster Merge (CMR) algorithm for efficiently combining clusters across rounds. The approach employs various blocking and filtering strategies to reduce computational complexity, uses in-context clustering to improve LLM performance, and hierarchically merges clusters while exploiting transitivity and anti-transitivity rules. Theoretical analysis demonstrates that LLM-CER significantly reduces the number of LLM API calls compared to traditional pairwise methods, making it scalable for large datasets. The section also provides detailed algorithmic steps, practical considerations for parameter tuning, and empirical justifications for design choices."
    },
    "Related Work": {
        "content": "ER problems can be broadly classified into deduplication (dirty ER) and record linkage (clean-clean ER). The former partitions a single record collection into multiple entity clusters. The latter involves matching records from two separate, typically overlapping but duplicate-free, collections (e.g., two tables) and identifying pairs of records that refer to the same entity [39]. Dirty ER is generally more challenging than clean-clean ER due to inherent data quality issues, such as spelling errors, missing values, and noise. In contrast, clean-clean ER typically involves tables that are already cleaned and standardized [14]. This work focuses on the dirty ER problem, while our solution is also applicable to the clean-clean ER problem by considering the union of records across tables as a single collection. We categorize related work as follows.\nPLM and LLM-based Entity Resolution. Before the advent of large language models, pre-trained transformer language models (PLMs) were commonly used for ER tasks [41]. DeepBlocker [72] evaluates various deep learning methods, including SBERT (Sentence BERT) [68] for blocking. Ditto [46] exploits PLMs such as BERT [22], DistilBERT [70], or RoBERTa [48] for clean-clean ER. JointBERT develops a dual-objective training method for BERT, combining binary matching and multi-class classification, to predict both match/non-match decisions and entity identifiers in training pairs [62]. RobEM aims to enhance the robustness of PLM-based entity matching models [67].\n\nPLM-based ER approaches typically require task-specific labeled data and fine-tuning, which are resource-intensive. More recently, LLMs, a.k.a. foundation models, have demonstrated strong performance in data cleaning and integration tasks, including ER, at a lower cost. This is primarily due to the more straightforward prompt-based interactions without requiring task-specific model retraining or labeled data. State-of-the-art LLM-based ER approaches are discussed in $\\S 1$. Among them, [26, 80, 87] are the closest to\n\n\n[^0]:    ${ }^{1}$ The number of relationships to explore grows quadratically with the set size.\n\nours. Both [26, 87] introduce batch prompting, where multiple pairwise questions are packaged into a single batch for the LLM. However, our approach differs by packing multiple records into a set for direct, in-context clustering via the LLM, allowing for more efficient and scalable ER. Furthermore, ComEM [80] employs advanced prompts such as \"match\", \"compare\", or \"select\" to explore interactions across multiple records beyond pairwise questions. However, unlike ours, it does not leverage the direct clustering capacity of LLMs for a record set. As pairwise questioning is a special case of our in-context clustering (with set size $=2$ ), our framework generalizes the state-of-the-art ER approaches, offering improved cost-effectiveness and efficiency.\n\nBesides, our work LLM-CER differs from existing LLM-based ER methods in its comprehensive end-to-end pipeline, which includes blocking/filtering (except for [26]), and addresses critical issues like LLM hallucinations, which are not explicitly handled before.\nCrowdsourcing-based Entity Resolution. A key concern in crowdsourced entity resolution (ER) is minimizing the number of questions posed to workers, which directly impacts the overall cost. Transitivity is commonly used to reduce the number of questions in [75, 78]. Closer to our approach, CrowdER [77] develops a clustering-based method where crowd workers are directly tasked with clustering a set of records. Various models for selecting high-quality questions have been developed in [9, 82]. ZenCrowd combines algorithmic and crowdsourcing-based matching techniques within a probabilistic framework [21], while Roomba uses a decision-theoretic approach to select matches to confirm based on their utility [37]. More recent works [32, 74, 79, 85] consider crowd errors in ER tasks. Corleone crowdsources the entire ER workflow, including blocking rules learning [31], and Falcon scales Corleone using RDBMS-style query execution over a Hadoop cluster [19].\n\nAlthough CrowdER [77] provides record sets to human taskers for direct clustering, there are fundamental differences between their approach and our in-context clustering using LLMs. (1) CrowdER aims to minimize the number of record sets required (equivalently the number of HITs), given a predefined set size, to cover and resolve all uncertain record pairs in a block. In contrast, we adopt a hierarchical approach for record set creation. Unlike CrowdER, which generates all record sets at once and covers all uncertain record pairs, our method incrementally generates record sets across multiple layers. In the initial layers, we focus on generating non-overlapping sets that cover all records, without immediately resolving all uncertain pairs. As we progress to higher layers, we leverage transitivity and anti-transitivity to identify and reduce unnecessary record sets. This approach minimizes the total number of sets, thus reducing the number of LLM API calls and ultimately lowering monetary costs. According to our experimental results over real-world datasets and considering the same set size as well as the same blocking approach (e.g., LSH [23]), the number of record sets (equivalently the number of HITs) needed for CrowdER can be $2-5 \\times$ higher than the number of record sets (equivalently the number of LLM API calls) required by ours. (2) Our combining approach differs from CrowdER's in several key ways. The initial record sets generated by our algorithm are non-overlapping. Based on clustering results via the LLM, we conduct hierarchical record sets generation and further in-context clustering to merge those clusters. Our hierarchical record sets generation process maintains\n![img-1.jpeg](img-1.jpeg)\n\nFigure 2: Example zero-shot prompt for pairwise ER\nthe optimal record set configuration, it also leverages transitivity and anti-transitivity to reduce the number of LLM API calls. In summary, direct and robust LLM-based clustering is employed both in our matching and combining phases. In contrast, CrowdER allows overlaps of records in different HITs, and their cluster merging process relies on these overlaps to indirectly facilitate merging through transitivity. This could suffer due to human errors as two clusters could be incorrectly merged. (3) Finally, our approach includes a guardrail mechanism to assess the quality of the LLM's clustering results. This allows us to identify and discard incorrect clustering results and regenerate better record sets. In contrast, CrowdER assumes that crowdsourcing results are always accurate, it lacks a built-in process to validate or modify the clustering outcomes. These differences ensure that our method not only minimizes the number of record sets (equivalently the number of LLM API calls) but also maintains higher quality of the final ER results.\nFiltering and Blocking. Blocking is a critical preprocessing step in ER, aimed at reducing computational overhead by partitioning records into groups and ensuring that only potentially matching records are compared. Blocking methods could be classified as rulebased, sorting-based, and hash-based. Rule-based methods organize tuples by applying fixed keys or decision rules created by experts or derived from heuristics [65]. Sorting-based methods cluster records by rapidly sorting them according to their textual similarities, which are determined using different similarity functions [40]. Hash-based approaches utilize hashing techniques, such as Min-Hashing and Locality-Sensitive Hashing (LSH), to place records into different buckets [23]. Filtering complements blocking by eliminating record pairs that are guaranteed not to match, thus focusing comparisons only on the remaining pairs to improve computational efficiency. Prominent filtering techniques are categorized into prefix-based, partition-based, and tree-based. We refer to [59] for details.\n",
        "title": [
            "## 2 RELATED WORK"
        ],
        "summary": "This section reviews related work in entity resolution (ER), distinguishing between deduplication (dirty ER) and record linkage (clean-clean ER), and positions the current work as focused on the more challenging dirty ER scenario while being adaptable to clean-clean tasks. It surveys PLM- and LLM-based ER methods, highlighting that recent LLM approaches, particularly those leveraging in-context clustering, offer greater efficiency and scalability compared to batch or pairwise prompting, and introduces the authors\u2019 comprehensive pipeline that addresses issues like hallucination and incorporates blocking/filtering. The section also compares LLM-based clustering with crowdsourcing-based ER, emphasizing the advantages of the proposed hierarchical, transitivity-aware, and quality-controlled approach over previous methods such as CrowdER in terms of reduced cost and improved accuracy. Additionally, it summarizes standard filtering and blocking techniques used to enhance ER efficiency by narrowing down candidate record pairs."
    },
    "Experiment": {
        "content": "In this section, we empirically evaluate our proposed algorithms as well as the competing approaches. All methods are implemented in Python and executed on a Linux server with 2.20 GHz CPU and 128GB of RAM.\nDatasets. We utilize nine real-world entity resolution (ER) datasets spanning various domains, including e-commerce products, academic citations, geography, and music. Details of these datasets are provided in Table 1. Specifically, CORA is a text-based dataset comprising duplicate references to scientific publications. For this study, we use the structured CSV version provided by [57], with the raw data accessible online ${ }^{2}$. Alaska [17] is an e-commerce product dataset collected from multiple websites and previously featured in the SIGMOD 2020 programming contest [20]. DBLP-Google [52] and Citeseer-DBLP belong to the citation domain, containing bibliographic duplicates sourced from DBLP, Google Scholar, and Citeseer. The Song dataset represents the music domain and includes music information from various platforms, with variations in attributes such as duration and release year. Both Song and Citeseer-DBLP can\n\n[^0]\n[^0]:    ${ }^{2}$ https://www.gabormelli.com/RKB/CORA_Citation_Benchmark_Task.\n\nbe found here. The $A S$ and Music 20K datasets are widely recognized for Dirty ER benchmarks [69]. Lastly, we consider AmazonGoogle [52] and Walmart-Amazon [52] datasets, originating from the Software and Electronics domains, respectively. These domains are more specialized and require deeper domain knowledge, making the corresponding ER tasks more complex than others.\nEvaluation Metrics. We select several widely-used metrics to evaluate the in-context clustering results, as well as the final ER clustering results. These metrics are FP-measure (a variant of the Fmeasure), Accuracy (ACC), Normalized Mutual Information (NMI), and Adjusted Rand Score (ARI) [2, 11, 81, 86]. All experiments are repeated three times to ensure robustness, and we report the average values over three runs. Due to space limitations, we present results for ACC and FP-measure in the main paper.\n\nThe definition of ACC is as follows: Let $\\forall=\\left\\{M_{1}, M_{2}, \\ldots, M_{m}\\right\\}$ represent the ground truth clusters and $\\mathbb{X}=\\left\\{X_{1}, X_{2}, \\ldots, X_{n}\\right\\}$ denote the predicted clusters. The total number of records in both sets is $|\\Re|=\\bigcup_{i}\\left|X_{i}\\right|=\\bigcup_{j}\\left|M_{j}\\right|$. The accuracy (ACC) is defined as:\n\n$$\n\\begin{gathered}\n\\mathrm{ACC}=\\frac{\\text { CorrectCount }}{|\\Re|} \\\\\n\\text { CorrectCount }=\\sum_{X_{j} \\in \\mathbb{X}} \\sum_{x \\in X_{j}} \\mathbb{I}\\left(\\exists y \\in M_{j}^{\\prime}, y=x\\right)\n\\end{gathered}\n$$\n\nwhere $\\forall^{\\prime}=\\left\\{M_{1}^{\\prime}, \\ldots, M_{m}^{\\prime}\\right\\}$ is obtained by reordering $\\forall$ based on their intersection sizes with the predicted clusters, and $\\mathbb{I}(\\cdot)$ is the indicator function that equals to 1 if the condition satisfies.\n\nFP-measure evaluates clustering results from the perspective of homogeneity and stability. It is the harmonic mean of purity and inverse-purity.\n\n$$\n\\begin{gathered}\n\\text { purity }(\\mathbb{X}, \\forall)=\\sum_{X_{i} \\in \\mathbb{X}} \\frac{\\left|X_{i}\\right|}{\\mid \\Re|} \\max _{M_{i} \\in \\forall} \\operatorname{Overlap}\\left(X_{i}, M_{i}\\right) \\\\\n\\text { inverse-purity }(\\mathbb{X}, \\forall)=\\sum_{M_{i} \\in \\forall} \\frac{\\left|M_{i}\\right|}{\\mid \\Re|} \\max _{X_{i} \\in \\mathbb{X}} \\operatorname{Overlap}\\left(M_{i}, X_{i}\\right) \\\\\n\\operatorname{Overlap}\\left(X_{i}, M_{i}\\right):=\\frac{\\left|X_{i} \\cap M_{i}\\right|}{\\left|X_{i}\\right|}\n\\end{gathered}\n$$\n\nFinally, we derive the FP-measure:\n\n$$\n\\text { FP-measure }(\\mathbb{X}, \\forall)=\\frac{2}{1 / \\text { purity }(\\mathbb{X}, \\forall)+1 / \\text { inverse-purity }(\\mathbb{X}, \\forall)}\n$$\nWe use gpt-40-mini for end-to-end tests and set the temperature parameter to zero. For embedding generation in blocking, we adopt widely-used all-MiniLM-L6-v2 model ${ }^{4}$.\n6.2.1 In-context clustering vs. pairwise questioning. We compare the performance of our in-context clustering-based ER $\\left(S_{s}=9\\right)$ with the pairwise matching method $\\left(S_{s}=2\\right)$ to demonstrate the effectiveness and efficiency of in-context clustering that can process multiple records simultaneously. For fair comparison, we also adopt our guardrail strategy to mitigate LLM-induced errors for pairwise matching as stated in $\\S 5.2$. Table 2 shows that when $S_{s}$ is set to 9 , the final ER results achieved by our algorithm are nearly identical or even better than those of the pairwise matching approach. On\n\nTable 2: Comparison with pairwise matching-based ER\n\n|  Metrics | Cora |  | Alaska |  | AS |   |\n| --- | --- | --- | --- | --- | --- | --- |\n|   | $S_{s}=2$ <br> (pairwise) | $S_{s}=9$ <br> (clustering) | $S_{s}=2$ <br> (pairwise) | $S_{s}=9$ <br> (clustering) | $S_{s}=2$ <br> (pairwise) | $S_{s}=9$ <br> (clustering)  |\n|  ACC | 0.68 | $\\mathbf{0 . 9 6}$ | 0.81 | $\\mathbf{0 . 8 2}$ | $\\mathbf{0 . 7 8}$ | $\\mathbf{0 . 7 0}$  |\n|  FP-measure | 0.67 | $\\mathbf{0 . 7 1}$ | 0.78 | $\\mathbf{0 . 7 9}$ | 0.60 | $\\mathbf{0 . 6 3}$  |\n|  Cost (USD) | 0.67 | $\\mathbf{0 . 0 3}$ | 0.43 | $\\mathbf{0 . 1 3}$ | 0.00 | $\\mathbf{0 . 0 2}$  |\n|  Tokens (M) | 3.45 | $\\mathbf{0 . 1 2}$ | 2.29 | $\\mathbf{0 . 7 3}$ | 0.35 | $\\mathbf{0 . 0 7}$  |\n|  Time (min) | 297.27 | $\\mathbf{5 . 4 2}$ | 241.51 | $\\mathbf{3 9 . 3 7}$ | 77.2 | $\\mathbf{8 . 0 1}$  |\n|  $\\#$ API Calls (K) | 30.23 | $\\mathbf{0 . 2 8}$ | 24.54 | $\\mathbf{2 . 0 4}$ | 7.85 | $\\mathbf{0 . 4 1}$  |\n\nTable 3: Number of records set in each hierarchy level\n\n|  Dataset | Level 0 | Level 1 | Level 2 | Level 3 | Level 4 | Level 5  |\n| --- | --- | --- | --- | --- | --- | --- |\n|  Cora | 183 | 76 | 15 | 5 | $/$ | $/$  |\n|  Alaska | 1312 | 604 | 101 | 20 | 5 | 1  |\n|  AS | 251 | 107 | 48 | 6 | 1 | $/$  |\n\nthe other hand, our in-context clustering significantly reduces the number of LLM API calls by $12-108 \\times$, token consumption by 3$28 \\times$, monetary cost by $3-22 \\times$, and end-to-end ER time by $6-55 \\times$, compared to pairwise matching.\n\nTable 3 illustrates the distribution of the number of record sets across hierarchy levels, providing insights into our hierarchical clustering approach. For all datasets, the number of record sets decreases significantly from Level 0 to higher levels, and are nearly completed before Level 5, demonstrating the effectiveness of the hierarchical structure in progressively merging similar entities. Larger datasets generally require more hierarchy levels for processing, as evidenced by Alaska (the largest dataset) extending to Level 5, while Cora (the smallest dataset) completes by Level 3. 6.2.2 Comparison with state-of-the-art. We compare our LLM-CER framework with several recent LLM-based ER methods to highlight its effectiveness and efficiency.\n\nBooster [43]: Booster employs traditional blocking to generate multiple candidate partitions and iteratively queries the LLM using a next-question selection: identifying record pairs that are most informative in distinguishing between different partitioning. The final output is one of the partitionings with the highest probability. Unlike Booster, which is limited to selecting from predefined partitions, our method integrates blocking with hierarchical clustering and misclustering correction to produce refined clusters beyond the initial partitions-resulting in higher flexibility and accuracy.\n\nBQ [26]: BQ reduces LLM usage by batching multiple pairwise match questions into a single prompt, allowing contextual information to improve prediction quality across questions. It also uses an enhanced demonstration selection strategy for better few-shot generalization. Each batch of $k$ records forms $\\frac{k}{2}$ pairwise comparisons. In contrast, our method with set size $k$ performs clustering over the entire set, avoiding up to $\\frac{k(k-1)}{2}$ comparisons, yielding significantly fewer API calls and lower monetary cost.\n\nCrowdER+LLM [77]: CrowdER+LLM follows a clustering-based design that filters irrelevant pairs and generates record sets to resolve ambiguous cases, merging clusters via transitive closure. Our approach improves on this by incrementally constructing record sets and leveraging both transitivity and anti-transitivity to prune unnecessary sets. For a fair comparison, we adopt their record grouping strategy but replace crowdsourcing with LLM-based clustering, using the same blocking and record set size.\n\nTable 4: End-to-end comparison of our in-context clusteringbased ER with state-of-the-art LLM-based ER\n\n|  Dataset | Metrics | LLM-CER | Booster | BQ | CrowdER +LLM  |\n| --- | --- | --- | --- | --- | --- |\n|  Alaska | ACC | 0.82 | 0.71 | 0.33 | 0.68  |\n|   | FP | 0.79 | 0.55 | 0.49 | 0.62  |\n|   | Cost ( $\\$$ ) | 0.15 | 0.02 | 1.55 | 0.42  |\n|   | Tokens (M) | 0.73 | 0.19 | 5.59 | 2.04  |\n|   | Time (s) | 2374.2 | 2450.1 | 8798.9 | 6547.2  |\n|   | # API Calls | 2043 | 2606 | 8035 | 5845  |\n|  AS | ACC | 0.70 | 0.62 | 0.54 | 0.52  |\n|   | FP | 0.63 | 0.62 | 0.51 | 0.50  |\n|   | Cost ( $\\$$ ) | 0.02 | 0.01 | 0.29 | 0.11  |\n|   | Tokens (M) | 0.07 | 0.03 | 0.34 | 0.37  |\n|   | Time (s) | 480.6 | 622.9 | 925.5 | 2356.2  |\n|   | # API Calls | 413 | 723 | 842 | 2084  |\n|  Song | ACC | 0.72 | 0.52 | 0.59 | 0.52  |\n|   | FP | 0.78 | 0.68 | 0.67 | 0.64  |\n|   | Cost ( $\\$$ ) | 0.06 | 0.02 | 0.77 | 0.12  |\n|   | Tokens (M) | 0.22 | 0.11 | 1.98 | 0.43  |\n|   | Time (s) | 933.2 | 903.3 | 2581.5 | 1856.3  |\n|   | # API Calls | 668 | 921 | 2338 | 1247  |\n|  Music | ACC | 0.71 | 0.59 | 0.60 | 0.62  |\n|   | FP | 0.61 | 0.60 | 0.54 | 0.55  |\n|   | Cost ( $\\$$ ) | 0.19 | 0.02 | 2.18 | 0.39  |\n|   | Tokens (M) | 0.90 | 0.15 | 8.96 | 1.82  |\n|   | Time (s) | 2388.4 | 2585.1 | 17515.8 | 4562.3  |\n|   | # API Calls | 3859 | 3915 | 17365 | 7782  |\n|  DG | ACC | 0.81 | 0.56 | 0.62 | 0.72  |\n|   | FP | 0.70 | 0.68 | 0.63 | 0.65  |\n|   | Cost ( $\\$$ ) | 0.07 | 0.02 | 1.12 | 0.34  |\n|   | Tokens (M) | 0.37 | 0.18 | 3.92 | 1.79  |\n|   | Time (s) | 1552.4 | 2552.2 | 6052.2 | 7456.3  |\n|   | # API Calls | 1285 | 3085 | 6456 | 6504  |\n|  Cora | ACC | 0.90 | 0.75 | 0.62 | 0.51  |\n|   | FP | 0.71 | 0.60 | 0.56 | 0.61  |\n|   | Cost ( $\\$$ ) | 0.03 | 0.01 | 1.45 | 0.07  |\n|   | Tokens (M) | 0.12 | 0.06 | 4.23 | 0.29  |\n|   | Time (s) | 325.5 | 605.4 | 4085.3 | 598.5  |\n|   | # API Calls | 279 | 698 | 4882 | 483  |\n|  Cite-\nseer | ACC | 0.88 | 0.72 | 0.64 | 0.60  |\n|   | FP | 0.95 | 0.78 | 0.79 | 0.69  |\n|   | Cost ( $\\$$ ) | 0.03 | 0.01 | 0.63 | 0.08  |\n|   | Tokens (M) | 0.13 | 0.05 | 1.64 | 0.37  |\n|   | Time (s) | 1360.8 | 1583.2 | 6228.9 | 3895.6  |\n|   | # API Calls | 1302 | 2169 | 6420 | 3858  |\n|  AmazonGoogle | ACC | 0.71 | 0.58 | 0.53 | 0.50  |\n|   | FP | 0.64 | 0.55 | 0.50 | 0.48  |\n|   | Cost ( $\\$$ ) | 0.02 | 0.01 | 0.62 | 0.09  |\n|   | Tokens (M) | 0.07 | 0.03 | 0.86 | 0.42  |\n|   | Time (s) | 465.6 | 785.2 | 1658.2 | 1985.2  |\n|   | # API Calls | 432 | 998 | 1895 | 2025  |\n|  WalmartAmazon | ACC | 0.61 | 0.50 | 0.42 | 0.51  |\n|   | FP | 0.56 | 0.48 | 0.41 | 0.50  |\n|   | Cost ( $\\$$ ) | 0.02 | 0.01 | 0.59 | 0.08  |\n|   | Tokens (M) | 0.06 | 0.03 | 0.68 | 0.39  |\n|   | Time (s) | 375.8 | 475.2 | 1498.5 | 3895.6  |\n|   | # API Calls | 398 | 825 | 1585 | 1958  |\n\nImplementation. For BQ, we follow [26] by including 8 demonstrations per prompt and applying the same blocking strategy as ours. Each prompt contains 5 pairwise questions (i.e., 10 records), approximately matching our 9 -record clustering prompts for a fair workload comparison. As BQ requires labeled pairs for demonstrations, we include their annotation cost following the same accounting method. For Booster, we apply our blocking method with tuned parameters. We evaluate all methods using ACC and FP-measure. End-to-end Performance. As shown in Table 4, the following observations can be made: (1) Our LLM-CER method consistently outperforms all baselines across datasets in both clustering quality (ACC and FP-measure) and operational efficiency (API calls\n\nTable 5: Optimal values vs. attribute count $\\left(A_{i t}\\right)$ and attribute types. The \"T\" denotes \"Textual\", \"N\" represents \"Numeric\", \"C\" indicates 'Categorical'. Inc. denotes 'Included'\n\n|  Dataset | $A_{i t}$ | $S_{s}$ | $S_{d}$  |\n| --- | --- | --- | --- |\n|  Cora | 4 | 9 | 3  |\n|   | 8 | 9 | 4  |\n|   | 12 | 9 | 4  |\n|  Alaska | 3 | 9 | 4  |\n|   | 6 | 9 | 4  |\n|   | 9 | 9 | 4  |\n\n|  Dataset | Inc. type | $S_{s}$ | $S_{d}$  |\n| --- | --- | --- | --- |\n|  T, N, C | 7 | 3 |   |\n|  N, C | 12 | 4 |   |\n|  T, C | 8 | 3 |   |\n|  T, N | 8 | 4 |   |\n|  Cite-\nseer | T, N, C | 9 | 4  |\n|   | N, C | 8 | 4  |\n|   | T, C | 9 | 4  |\n|   | T, N | 9 | 4  |\n\nand runtime), while maintaining competitive monetary cost. (2) Booster selects from multiple blocking-based partitionings using an LLM-driven scoring mechanism but does not refine these partitions. While it may choose a relatively strong candidate, its inability to correct or merge partitions limits its clustering quality: its ACC and FP scores remain suboptimal. In terms of monetary cost, however, Booster benefits from minimal token consumption, as it only selects the best partitioning without further modifying it. (3) BQ performs exhaustive pairwise matching within blocks using few-shot prompts, incurring substantial API and token costs $\\sim 5 \\sim 35 \\times$ higher than ours-even after applying transitivity and anti-transitivity. Although demonstrations slightly improve pairwise accuracy, the lack of result verification often leads to incorrect merges, resulting in the lowest ACC and FP scores. Furthermore, BQ requires labeled examples for few-shot prompting, adding additional annotation cost, whereas our method operates entirely without supervision. (4) CrowdER+LLM uses clustering and achieves the second-best accuracy. However, its HIT design permits overlapping records and lacks alignment with LLM-optimized record set formats (\u00a7 2), resulting in greater API calls and monetary cost. Moreover, like BQ, it lacks verification for LLM outputs, limiting its clustering quality compared to our LLM-CER approach. Remark. Some of the competing methods are also complementary to ours. For example, BQ's batch processing can be used in our case to batch clustering of multiple record sets simultaneously. Similarly, Booster's best partitioning selection can serve as an effective pre-processing step, providing high-quality initial blocks that our algorithm can refine for enhanced accuracy. These complementarities underscore the potential for integrating strengths from different methods to advance ER performance.\nIn this section, we analyze how dataset characteristics-namely the number of attributes, attribute types, and overall dataset dif-ficulty-affect the optimal configuration of key parameters and the performance of our end-to-end ER framework. To ensure fair evaluation, we retain critical attributes (e.g., title) across all settings. Impact of Attribute Count. To isolate the effect of increasing attribute count, we use the Cora and Alaska datasets, which contain many single-type (textual) attributes. As shown in Table 5, the optimal values for $S_{s}$ and $S_{d}$ remain largely stable. For Cora, $S_{s}$ stays at 9 across all attribute counts, while $S_{d}$ increases slightly from 3 to 4 . For Alaska, both values remain fixed at 9 and 4 , respectively.\n\nTable 6: End-to-end ER performance vs. attribute count\n\n| Dataset | Metrics | $A_{n}=4$ | $A_{n}=8$ | $A_{n}=12$ |\n| :--: | :--: | :--: | :--: | :--: |\n| Cora | ACC | 0.82 | 0.85 | 0.90 |\n|  | FP | 0.66 | 0.67 | 0.71 |\n|  | Cost (\\$) | 0.02 | 0.03 | 0.03 |\n|  | Tokens (M) | 0.05 | 0.09 | 0.12 |\n|  | Time (min) | 5.04 | 5.21 | 5.43 |\n|  | API Calls | 288 | 283 | 279 |\n| Alaska | ACC | 0.74 | 0.77 | 0.82 |\n|  | FP | 0.74 | 0.75 | 0.79 |\n|  | Cost (\\$) | 0.06 | 0.11 | 0.15 |\n|  | Tokens (M) | 0.26 | 0.51 | 0.73 |\n|  | Time (min) | 37.54 | 38.24 | 39.57 |\n|  | API Calls | 2064 | 2055 | 2043 |\n\nTable 7: End-to-end ER performance vs. attribute types\n\n| Dataset | Metrics | Original | w/o <br> Textual | w/o <br> Numeric | w/o <br> Categorical |\n| :--: | :--: | :--: | :--: | :--: | :--: |\n| Walmart- <br> Amazon | ACC | 0.61 | 0.72 | 0.66 | 0.60 |\n|  | FP | 0.56 | 0.66 | 0.58 | 0.54 |\n|  | Cost (\\$) | 0.02 | 0.01 | 0.02 | 0.02 |\n|  | Tokens (M) | 0.06 | 0.04 | 0.05 | 0.06 |\n|  | Time (min) | 6.25 | 5.89 | 6.02 | 6.36 |\n|  | API Calls | 398 | 374 | 393 | 409 |\n| Cite- <br> seer | ACC | 0.88 | 0.82 | 0.86 | 0.86 |\n|  | FP | 0.95 | 0.90 | 0.92 | 0.93 |\n|  | Cost (\\$) | 0.03 | 0.02 | 0.03 | 0.03 |\n|  | Tokens (M) | 0.13 | 0.11 | 0.12 | 0.12 |\n|  | Time (min) | 22.68 | 20.98 | 21.88 | 22.03 |\n|  | API Calls | 1302 | 1331 | 1312 | 1314 |\n\nThis suggests that, for single-type textual datasets, the parameter configuration is robust to changes in attribute count.\n\nBuilding on these configurations, we conduct end-to-end ER experiments (Table 6). Results show that increasing attribute count consistently improves clustering performance: for Cora, ACC improves from 0.84 to 0.90 and FP-measure from 0.66 to 0.71 when moving from 4 to 12 attributes. Similar trends are observed for Alaska. These improvements persist with MDG, indicating that richer attribute sets enhance record distinguishability. While token usage increases moderately ( 0.05 M to 0.12 M for Cora), the accuracy gains justify retaining all attributes in single-type datasets.\nImpact of Attribute Types and Dataset Difficulty. To assess the role of attribute types, we use Walmart-Amazon and Citeseer, both containing textual, numerical, and categorical fields. Controlled ablations omit one attribute type at a time to evaluate its contribution.\n\nAs shown in Table 5, the simpler Citeseer dataset exhibits stable parameter values, with only a minor drop when textual attributes are removed-likely due to increased reliance on less informative modalities. In contrast, the more complex Walmart-Amazon dataset shows notable deviations. When all attributes are retained, optimal values are $S_{s}=7$ and $S_{d}=3$, lower than the typical 9 and 4 , indicating higher clustering difficulty. Removing textual attributes increases $S_{s}$ and $S_{d}$ to 12 and 4 , suggesting that pruning noisy fields simplifies the task and enables larger input sets. Excluding numerical or categorical attributes also shifts $S_{s}$ to 8 , with corresponding adjustments in $S_{d}$. These results highlight that attribute type plays a critical role in complex domains, where noisy or redundant fields may obscure true entity boundaries.\n\nEnd-to-end results (Table 6) further confirm this. For WalmartAmazon, removing noisy textual attributes-while preserving key\n\nTable 8: Effect of MDG algorithm and record set regeneration on end-to-end performance\n\n| Metrics | Cora |  | Alaska |  | AS |  |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n|  | w/o MDG | w/ MDG | w/o MDG | w/ MDG | w/o MDG | w/ MDG |\n| ACC | 0.60 | 0.90 | 0.35 | 0.82 | 0.52 | 0.70 |\n| FP-measure | 0.58 | 0.71 | 0.47 | 0.79 | 0.52 | 0.63 |\n| Cost (USD) | 0.03 | 0.03 | 0.14 | 0.15 | 0.02 | 0.02 |\n| Tokens (M) | 0.11 | 0.12 | 0.66 | 0.73 | 0.06 | 0.07 |\n| Time (min) | 5.04 | 5.42 | 36.88 | 39.57 | 7.35 | 8.01 |\n| \\# API Calls (K) | 0.26 | 0.28 | 1.85 | 2.04 | 0.37 | 0.41 |\n\nfields like title-boosts ACC from 0.61 to 0.72 and FP-measure from 0.56 to 0.66 , with token usage dropping from 0.06 M to 0.04 M . This is due to common extraction errors (e.g., 'brand' values in 'name', or 'leather red' misclassified as 'color') that introduce significant noise. Eliminating these sources of noise allows the model to focus on cleaner signals, enhancing clustering quality. Similar observations are reported in [52]. Excluding numerical attributes yields a modest ACC gain (to 0.66), while removing categorical attributes slightly degrades performance, suggesting categorical fields offer informative structure. In contrast, Citeseer suffers across all ablations, reflecting the complementary value of all attribute types in well-structured datasets. These findings underscore the importance of selective attribute pruning for noisy, domain-specific datasets, and comprehensive attribute inclusion for simpler, structured ones.\nThe results in Table 8 show that the incorporation of the Misclustering Detection Guardrail (MDG) algorithm leads to improvements in ACC and FP-measure. For instance, ACC increases by up to $50 \\%$ and FP-measure by $22 \\%$ on Cora. This supports the claim that MDG reduces the occurrence of misclassifications in the ER process.\n\nMoreover, these performance gains are achieved with minimal additional computational cost. The total cost, token usage, processing time, and number of API calls remain almost unchanged or only slightly increase with the addition of MDG. This suggests that MDG enhances ER performance without incurring a significant resource overhead, making it an efficient approach for improving entity resolution in practical applications.\nTo assess the scalability of our end-to-end ER algorithm, we conduct experiments on progressively larger subsets of the publicly available Music dataset [69], containing 10K, 20K, 30K, and 50K records. These subsets preserve consistent entity dispersion, enabling a controlled evaluation of how performance and resource usage evolve with scale. We also compare our method with BQ [26] (introduced in $\\S 6.2 .2$ ), a state-of-the-art baseline designed for efficient LLM-based ER. To ensure fairness, all experiments are subject to a maximum execution time limit of 24 hours.\n\nFigure 8 shows that our LLM-CER consistently maintains high FP-measure as the dataset size increases, demonstrating strong robustness to scale. In contrast, BQ experiences a notable accuracy loss. Regarding efficiency, our method exhibits a predictable and modest increase in both cost and the number of API calls as data size grows, whereas BQ incurs substantially higher overheads. Execution time for our method scales linearly from approximately 20 minutes on Music 10 K to just over 100 minutes on Music 50 K . BQ\n\n![img-6.jpeg](img-6.jpeg)\n\nFigure 8: End-to-end performance vs. \\#Records\nbecomes prohibitively slow on larger datasets, particularly failing to complete on the Music $50 K$ dataset within the time limit.\n",
        "title": [
            "## 6 EXPERIMENTAL RESULTS",
            "### 6.1 Experimental Setup",
            "### 6.2 Performance on End-to-End ER",
            "### 6.3 Impact of Dataset Characteristics",
            "### 6.4 Impact of LLM Guardrails",
            "### 6.5 Scalability Test"
        ],
        "summary": "This section presents a comprehensive empirical evaluation of the proposed LLM-CER algorithm for entity resolution (ER) across nine diverse real-world datasets, comparing it to leading LLM-based and clustering-based ER methods. The authors detail the datasets, experimental setup, and evaluation metrics, then demonstrate that in-context clustering achieves comparable or superior accuracy to pairwise questioning while dramatically reducing computational cost, token usage, and API calls. LLM-CER consistently outperforms baselines in both clustering quality (ACC, FP-measure) and efficiency, and the study further analyzes how dataset characteristics (attribute count and type) influence parameter tuning and ER performance. The addition of the Misclustering Detection Guardrail (MDG) significantly boosts accuracy with negligible overhead, and scalability tests show that LLM-CER maintains high performance and efficiency as dataset size increases, unlike competing methods that degrade or become impractical at scale."
    }
}