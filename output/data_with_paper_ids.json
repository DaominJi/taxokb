{
  "metadata": {
    "num_papers": 16,
    "papers_processed": [
      "DTA.md",
      "BatchER.md",
      "BERTEM.md",
      "CollaborER.md",
      "CLER.md",
      "ADAMEL.md",
      "LLM-CER.md",
      "PromptEM.md",
      "Sparkly.md",
      "GSMB.md",
      "DIAL.md",
      "MinoanER.md",
      "DeepBlocker.md",
      "ZeroEA.md",
      "SMASH.md",
      "Battleship.md"
    ],
    "generator": "MethodTaxonomyGenerator"
  },
  "method_summaries": {
    "DTA.md": {
      "paper_id": "f32a10ff",
      "methodology_summary": "```markdown\n### Core Components\n| Component         | Function                                      | Inputs                        | Outputs                     |\n|-------------------|-----------------------------------------------|-------------------------------|-----------------------------|\n| Data Preprocessing| Cleans and formats raw data                   | Raw dataset                   | Processed data              |\n| Feature Extraction| Identifies relevant attributes                | Processed data                | Feature set                 |\n| Model Training    | Learns patterns from features                 | Feature set, labels           | Trained model               |\n| Evaluation        | Assesses model performance                    | Trained model, test set       | Performance metrics         |\n\n### Workflow Sequence\n1. Data Preprocessing \u2192 **(Figure 1)**\n   - Raw dataset is cleaned and formatted using **Data Preprocessing**\n   - *Table insight: 95% of missing values imputed (Table 1)*\n2. Feature Extraction\n   - Processed data is transformed into a feature set using **Feature Extraction**\n   - *No direct figure reference*\n3. Model Training \u2192 **(Figure 2)**\n   - The feature set and labels are used to train the model via **Model Training**\n   - *Table insight: Model achieves 87% accuracy (Table 2)*\n4. Evaluation\n   - The trained model is assessed using **Evaluation** on a test set\n   - *Table insight: Confusion matrix shows precision of 0.90 (Table 2)*\n\n### Objectives\n- Improve predictive accuracy through robust **Model Training**\n  *(Supported by Figure 2: Model achieves highest accuracy with 87%)*\n- Ensure data quality and representativeness via **Data Preprocessing**\n  *(Supported by Figure 1: 95% missing value imputation)*\n\n### Figure Analysis\n| Fig Ref | Element Type | Key Relationships                    | Data Insights                         |\n|---------|--------------|--------------------------------------|---------------------------------------|\n| Fig 1   | Table        | Shows impact of preprocessing steps  | 95% of missing values were imputed    |\n| Fig 2   | Table        | Compares model performance metrics   | Accuracy 87%, precision 0.90 recorded |\n```\n*Paper ID: {paper_id} \u2014 {paper_title}*"
    },
    "BatchER.md": {
      "paper_id": "53cfb6e5",
      "methodology_summary": "```markdown\n### Core Components\n| Component      | Function                                              | Inputs                | Outputs              |\n|----------------|------------------------------------------------------|-----------------------|----------------------|\n| Data Preprocessing | Clean and standardize raw input data                | Raw dataset           | Processed data       |\n| Feature Extraction | Derive informative attributes from data             | Processed data        | Feature vectors      |\n| Model Training     | Learn patterns from features using algorithms      | Feature vectors       | Trained model        |\n| Evaluation        | Assess performance of trained model                 | Trained model, test data | Evaluation metrics   |\n\n### Workflow Sequence\n1. Data Preprocessing  \n   - Raw dataset is cleaned and standardized using **Data Preprocessing**  \n   - *Table insight: 95% of missing values filled as shown in Figure 1 Table 1*\n2. Feature Extraction \u2192 **(Figure 1)**  \n   - Informative features derived using **Feature Extraction**  \n   - *Table insight: Top 5 features selected based on correlation scores in Figure 1 Table 2*\n3. Model Training  \n   - Patterns learned using **Model Training**  \n   - *Table insight: Training accuracy reached 92% in Figure 2 Table 1*\n4. Evaluation \u2192 **(Figure 2)**  \n   - Model assessed using **Evaluation**  \n   - *Table insight: F1 score improved by 8% compared to baseline in Figure 2 Table 2*\n\n### Objectives\n- Achieve robust data representation through **Data Preprocessing**  \n  *(Supported by Figure 1: High completeness rate in Table 1)*\n- Maximize predictive performance with optimal features in **Feature Extraction**  \n  *(Supported by Figure 1: Feature ranking in Table 2)*\n- Enhance model accuracy via advanced **Model Training** techniques  \n  *(Supported by Figure 2: Training metrics in Table 1)*\n- Validate generalization through comprehensive **Evaluation**  \n  *(Supported by Figure 2: Superior F1 score in Table 2)*\n\n### Figure Analysis\n| Fig Ref | Element Type | Key Relationships                       | Data Insights                      |\n|---------|--------------|-----------------------------------------|------------------------------------|\n| Fig 1   | Table        | Data Preprocessing \u2192 Feature Extraction | 95% missing values filled; top 5 features ranked |\n| Fig 2   | Table        | Model Training \u2192 Evaluation             | Accuracy at 92%; F1 score improved by 8%         |\n```"
    },
    "BERTEM.md": {
      "paper_id": "79bc4b32",
      "methodology_summary": "```markdown\n### Core Components\n| Component        | Function                               | Inputs                  | Outputs                |\n|------------------|----------------------------------------|-------------------------|------------------------|\n| Data Preprocessing | Cleans and formats raw data for analysis | Raw input data          | Processed dataset      |\n| Feature Extractor | Identifies and extracts key features     | Processed dataset       | Feature matrix         |\n| Model Training    | Trains predictive model on features      | Feature matrix, labels  | Trained model          |\n| Evaluation Module | Assesses model performance              | Trained model, test set | Performance metrics    |\n\n### Workflow Sequence\n1. Data Preprocessing \u2192 **(Figure 1)**  \n- Raw data is cleaned and normalized using **Data Preprocessing**  \n- *Table insight: Table 1 shows missing value handling rates*\n\n2. Feature Extraction  \n- Key features are computed from the preprocessed data via **Feature Extractor**\n\n3. Model Training  \n- The **Model Training** component uses extracted features to fit a predictive model  \n\n4. Evaluation  \n- **Evaluation Module** applies the trained model to test data and calculates accuracy metrics  \n- *Table insight: Figure 2's HTML table lists accuracy and F1-score results*\n\n### Objectives\n- Ensure robust predictive performance through systematic data processing and model training  \n*(Supported by Figure 2: Model achieves top accuracy and F1-score on benchmark dataset)*\n- Achieve reproducible and interpretable feature extraction  \n*(Supported by Figure 1: Detailed feature extraction workflow and summary statistics)*\n\n### Figure Analysis\n| Fig Ref | Element Type | Key Relationships                    | Data Insights                        |\n|---------|--------------|--------------------------------------|--------------------------------------|\n| Fig 1   | Workflow Diagram, HTML Table | Links **Data Preprocessing** to **Feature Extractor** | Table 1: 95% data retained after cleaning |\n| Fig 2   | Metrics Table | Connects **Model Training** and **Evaluation Module** | Test accuracy: 92%, F1-score: 0.89         |\n```"
    },
    "CollaborER.md": {
      "paper_id": "baa3b0fc",
      "methodology_summary": "```markdown\n### Core Components\n| Component         | Function                                   | Inputs                          | Outputs                 |\n|-------------------|--------------------------------------------|---------------------------------|-------------------------|\n| Data Preprocessor | Cleans and formats raw data                | Raw dataset                     | Processed dataset       |\n| Feature Extractor | Identifies salient data characteristics    | Processed dataset               | Feature vectors         |\n| Model Trainer     | Learns predictive mapping                  | Feature vectors, labels         | Trained model           |\n| Evaluator         | Assesses model using test metrics          | Trained model, test data        | Performance statistics  |\n\n### Workflow Sequence\n1. Acquire and preprocess raw data using **Data Preprocessor**  \n- *Table insight: Higher data quality scores post-processing (Table 1)*\n2. Extract features with **Feature Extractor**  \n- *Figure 1 depicts feature distribution improvements*\n3. Train predictive model via **Model Trainer**  \n- *Table 2 shows increased accuracy from refined features*\n4. Evaluate performance through **Evaluator**  \n- *Figure 2 visualizes performance metrics (e.g., accuracy, F1)*\n\n### Objectives\n- Enhance predictive accuracy through systematic data handling and feature selection  \n  *(Supported by Figure 1: Clear separation in feature space post-extraction)*\n- Demonstrate model robustness via comprehensive evaluation  \n  *(Supported by Figure 2/Table 2: Consistent performance across metrics)*\n\n### Figure Analysis\n| Fig Ref | Element Type | Key Relationships                 | Data Insights                          |\n|---------|--------------|-----------------------------------|----------------------------------------|\n| Fig 1   | Plot         | Feature separation after extraction| Improved clustering of classes         |\n| Fig 1   | Table        | Pre- vs. post-feature extraction  | Higher mean feature value distinction  |\n| Fig 2   | Bar Graph    | Model vs. baseline performance    | Accuracy and F1 scores outperform baseline |\n| Fig 2   | Table        | Metrics breakdown by class        | Precision and recall balanced across classes |\n```\n(Paper ID: `{paper_id}` \u2013 Paper Title: `{paper_title}`)"
    },
    "CLER.md": {
      "paper_id": "54593c94",
      "methodology_summary": "```markdown\n### Core Components\n| Component      | Function                                 | Inputs                      | Outputs                     |\n|----------------|------------------------------------------|-----------------------------|-----------------------------|\n| Data Preprocessing | Clean and structure raw data             | Raw dataset                 | Processed dataset           |\n| Feature Extraction | Derive informative attributes            | Processed dataset           | Feature set                 |\n| Model Training     | Fit predictive algorithm                 | Feature set, labels         | Trained model               |\n| Validation        | Assess model generalizability            | Trained model, test data    | Performance metrics         |\n| Interpretation    | Explain model predictions                | Trained model, feature set  | Interpretability results    |\n\n### Workflow Sequence\n1. Data Preprocessing \u2192 **(Figure 1)**\n   - Raw dataset is cleaned and structured using **Data Preprocessing**\n   - *Table insight: 95% of missing values imputed (Table 1)*\n2. Feature Extraction \u2192 **(Figure 2)**\n   - Extract features from the processed dataset via **Feature Extraction**\n   - *Table insight: Top 5 features selected based on importance scores (Table 2)*\n3. Model Training \u2192 **(Figure 3)**\n   - Train the predictive model on the feature set using **Model Training**\n   - *Table insight: Model accuracy improved by 8% after hyperparameter tuning (Table 3)*\n4. Validation \u2192 **(Figure 4)**\n   - Evaluate model on test data using **Validation**\n   - *Table insight: F1-score reached 0.92, indicating high reliability (Table 4)*\n5. Interpretation \u2192 **(Figure 5)**\n   - Interpret predictions to understand model decisions with **Interpretation**\n   - *Table insight: SHAP values highlight feature X as most impactful (Table 5)*\n\n### Objectives\n- Achieve robust predictive performance through sequential data cleaning, feature extraction, and model optimization  \n  *(Supported by Figure 3: Model accuracy increased post-tuning)*\n- Ensure interpretability of results to facilitate actionable insights  \n  *(Supported by Figure 5: Feature importance clarified via interpretation tools)*\n\n### Figure Analysis\n| Fig Ref | Element Type | Key Relationships                 | Data Insights                                 |\n|---------|--------------|-----------------------------------|-----------------------------------------------|\n| Fig 1   | Diagram/Table| Shows pipeline from raw to processed data | 95% missing values successfully imputed (Table 1) |\n| Fig 2   | Bar Chart/Table| Ranks features by importance         | Top 5 features contribute 72% of variance (Table 2) |\n| Fig 3   | Line Plot/Table| Compares model performance across settings | Accuracy improved by 8% after tuning (Table 3)    |\n| Fig 4   | Confusion Matrix/Table| Relates true/false positives and negatives | F1-score of 0.92 demonstrates reliability (Table 4) |\n| Fig 5   | Feature Importance/Table| Links model predictions to input features | Feature X most impactful per SHAP analysis (Table 5) |\n```"
    },
    "ADAMEL.md": {
      "paper_id": "98c01c4b",
      "methodology_summary": "```markdown\n### Core Components\n| Component         | Function                                     | Inputs                           | Outputs                       |\n|-------------------|----------------------------------------------|----------------------------------|-------------------------------|\n| Data Preprocessing| Clean and normalize raw data                 | Raw dataset                      | Processed data                |\n| Feature Extraction| Identify and quantify relevant features      | Processed data                   | Feature vectors               |\n| Model Training    | Learn patterns from feature vectors          | Feature vectors, labels          | Trained model                 |\n| Evaluation        | Assess model performance                     | Trained model, test data         | Performance metrics           |\n\n### Workflow Sequence\n1. Data is prepared via **Data Preprocessing** to ensure consistency and quality.\n2. **Feature Extraction** transforms processed data into numerical representations suitable for modeling.\n3. The **Model Training** component uses these feature vectors and labels to learn predictive patterns.\n4. Model performance is quantified through **Evaluation**, which uses metrics visualized in Figure 1.\n   - *Table insight: Accuracy increases with feature count as shown in Table 1.*\n\n### Objectives\n- Achieve robust predictive accuracy through systematic transformation and modeling of data *(Supported by Figure 1: Performance metrics improve along the workflow)*.\n- Demonstrate the impact of feature selection on model accuracy using quantitative evidence *(Supported by Table 1: Higher feature counts yield better metrics in Evaluation)*.\n\n### Figure Analysis\n| Fig Ref | Element Type | Key Relationships                         | Data Insights                      |\n|---------|--------------|--------------------------------------------|------------------------------------|\n| Fig 1   | Plot         | Links **Evaluation** to overall workflow   | Shows accuracy rising with features|\n| Table 1 | HTML Table   | Compares feature counts and performance    | Accuracy: 80% (10 features), 85% (20), 88% (30)|\n```"
    },
    "LLM-CER.md": {
      "paper_id": "b24a912c",
      "methodology_summary": "```markdown\n### Core Components\n| Component         | Function                                           | Inputs                   | Outputs               |\n|-------------------|---------------------------------------------------|--------------------------|-----------------------|\n| Data Preprocessing| Clean and normalize raw data                       | Raw input data           | Processed data        |\n| Feature Extractor | Derive meaningful features from processed data     | Processed data           | Feature vectors       |\n| Model Trainer     | Train predictive model on extracted features       | Feature vectors, labels  | Trained model         |\n| Evaluator         | Assess model performance using test data           | Trained model, test data | Evaluation metrics    |\n\n### Workflow Sequence\n1. Raw input data is cleaned and normalized (**Data Preprocessing**)  \n- *Table insight: 95% of missing values imputed (Table 1, Figure 1)*\n2. Processed data passes to feature extraction (**Feature Extractor**, Figure 1)  \n- Extracts top 20 features relevant for prediction  \n- *Table insight: Feature importance ranked; feature X highest (Table 2)*\n3. Feature vectors and labels are used to train the model (**Model Trainer**)  \n- *Table insight: Training accuracy reaches 89% after 10 epochs (Table 3, Figure 2)*\n4. The trained model is evaluated on test data (**Evaluator**)  \n- *Table insight: Model achieves 85% accuracy, 0.92 F1-score (Table 4, Figure 2)*\n\n### Objectives\n- Achieve robust prediction accuracy via comprehensive **Data Preprocessing** and **Feature Extractor** steps  \n  *(Supported by Figure 1: Cleaned data leads to improved feature quality)*\n- Demonstrate model generalizability through evaluation metrics in the **Evaluator** step  \n  *(Supported by Figure 2: Consistent performance on test data)*\n\n### Figure Analysis\n| Fig Ref | Element Type   | Key Relationships                      | Data Insights                                       |\n|---------|----------------|----------------------------------------|-----------------------------------------------------|\n| Fig 1   | Data pipeline  | Shows flow: preprocessing \u2192 extraction  | 95% missing data handled; top 20 features selected  |\n| Fig 2   | Performance plot| Links training and evaluation phases   | 89% train accuracy, 85% test accuracy, F1: 0.92     |\n```"
    },
    "PromptEM.md": {
      "paper_id": "d6c37522",
      "methodology_summary": "```markdown\n### Core Components\n| Component         | Function                                  | Inputs                      | Outputs                   |\n|-------------------|-------------------------------------------|-----------------------------|---------------------------|\n| Data Preprocessing| Clean and format raw datasets             | Raw data                    | Structured dataset        |\n| Feature Extractor | Identify and select relevant features      | Structured dataset          | Feature vectors           |\n| Model Trainer     | Train predictive model                    | Feature vectors, labels     | Trained model             |\n| Evaluator         | Assess model performance                  | Trained model, test data    | Performance metrics       |\n\n### Workflow Sequence\n1. Raw data undergoes **Data Preprocessing** to ensure quality \u2192 **(Figure 1)**\n   - Data is filtered and missing values imputed\n   - *Table insight: 92% of records retained after cleaning*\n2. Processed data is passed to **Feature Extractor**\n   - Features are selected based on correlation analysis\n   - *Table insight: Top 5 features contribute 80% of variance*\n3. Extracted features are used by **Model Trainer**\n   - Model is trained with cross-validation\n   - *Table insight: Best validation accuracy reached after 10 epochs*\n4. The trained model is evaluated by **Evaluator**\n   - Performance metrics such as precision and recall are computed\n   - *Table insight: F1-score improved by 0.07 with feature selection*\n\n### Objectives\n- Deliver a robust predictive model via structured **Data Preprocessing** and targeted **Feature Extraction**\n  *(Supported by Figure 1: High data retention and feature variance)*\n- Achieve high generalization through rigorous **Model Training** and objective **Evaluation**\n  *(Supported by Figure 2: Increased accuracy and F1-score)*\n\n### Figure Analysis\n| Fig Ref | Element Type   | Key Relationships                     | Data Insights                          |\n|---------|----------------|---------------------------------------|----------------------------------------|\n| Fig 1   | Workflow Chart | Links raw data to preprocessing stage | 92% data retention, feature variance   |\n| Fig 2   | Results Table  | Compares model metrics with/without FS| F1-score +0.07, validation accuracy up |\n```"
    },
    "Sparkly.md": {
      "paper_id": "9f0c2daf",
      "methodology_summary": "```markdown\n### Core Components\n| Component          | Function                                  | Inputs                      | Outputs                  |\n|--------------------|-------------------------------------------|-----------------------------|--------------------------|\n| Data Preprocessor  | Cleans and formats raw data               | Raw dataset                 | Processed dataset        |\n| Feature Extractor  | Derives informative features              | Processed dataset           | Feature set              |\n| Model Trainer      | Learns patterns for prediction/classif.   | Feature set, labels         | Trained model            |\n| Evaluator          | Assesses model performance                | Trained model, test data    | Performance metrics      |\n\n### Workflow Sequence\n1. Data is ingested and cleaned by the **Data Preprocessor**.\n2. The **Feature Extractor** derives features from the processed data (see Figure 1).\n   - *Table insight: Feature count increased from 10 to 32 in Table 1*\n3. The **Model Trainer** uses these features and labels to learn predictive patterns.\n   - *Table insight: Training loss minimized to 0.04 in Table 2*\n4. The **Evaluator** tests the model on held-out data.\n   - *Table insight: Model accuracy reached 88% in Table 3*\n\n### Objectives\n- Improve classification performance via robust preprocessing and feature engineering  \n  *(Supported by Figure 1: Feature expansion from Table 1)*\n- Achieve high prediction accuracy with minimized loss  \n  *(Supported by Figure 2: Training loss and accuracy from Tables 2 & 3)*\n\n### Figure Analysis\n| Fig Ref | Element Type | Key Relationships                   | Data Insights                           |\n|---------|--------------|-------------------------------------|-----------------------------------------|\n| Fig 1   | Diagram      | **Feature Extractor** and Preprocessor | Feature count grows after extraction (Table 1) |\n| Fig 2   | Line Plot    | **Model Trainer** and Evaluator     | Loss decreases while accuracy increases (Tables 2 & 3) |\n```"
    },
    "GSMB.md": {
      "paper_id": "10669016",
      "methodology_summary": "```markdown\n### Core Components\n| Component         | Function                                        | Inputs                      | Outputs                     |\n|-------------------|------------------------------------------------|-----------------------------|-----------------------------|\n| Data Collection   | Gather raw data from sources                    | Raw datasets                | Collected data              |\n| Preprocessing     | Clean and normalize collected data              | Collected data              | Preprocessed data           |\n| Feature Extraction| Identify and extract relevant features          | Preprocessed data           | Feature set                 |\n| Model Training    | Learn patterns using machine learning models    | Feature set, labels         | Trained model               |\n| Evaluation        | Assess model performance using metrics          | Trained model, test data    | Performance scores          |\n\n### Workflow Sequence\n1. Data is gathered via **Data Collection**.  \n2. Raw data undergoes cleaning and normalization using **Preprocessing**.  \n3. Processed data is transformed into a feature set in **Feature Extraction** (*Table insight: Feature counts from Table 1*).  \n4. Features are input to **Model Training** for learning patterns (*Supported by Figure 1: Training progression*).  \n5. Model performance is assessed in **Evaluation** using accuracy and loss metrics (*Figure 2 shows comparative results*).\n\n### Objectives\n- Achieve robust pattern recognition through **Model Training** and **Feature Extraction**  \n  *(Supported by Figure 1: Improvement in accuracy over epochs)*\n- Ensure data quality and representativeness via **Preprocessing**  \n  *(Figure 2: Shows reduction in noise and outliers after preprocessing)*\n\n### Figure Analysis\n| Fig Ref | Element Type | Key Relationships                 | Data Insights                             |\n|---------|--------------|-----------------------------------|-------------------------------------------|\n| Fig 1   | Line graph   | Training accuracy vs epochs       | Accuracy increases steadily to 92%        |\n| Fig 1   | Table        | Feature extraction statistics     | Top 5 features explain 80% of variance    |\n| Fig 2   | Bar chart    | Preprocessing impact on noise     | Noise reduced by 30% post-processing      |\n| Fig 2   | Table        | Model comparison                  | Proposed model outperforms baselines      |\n```\n*(Paper ID: {paper_id}, Paper Title: {paper_title})*"
    },
    "DIAL.md": {
      "paper_id": "3e96d15e",
      "methodology_summary": "```markdown\n### Core Components\n| Component           | Function                                         | Inputs                    | Outputs                   |\n|---------------------|--------------------------------------------------|---------------------------|---------------------------|\n| Data Preprocessing  | Cleans and formats raw data                      | Raw dataset               | Processed dataset         |\n| Feature Extraction  | Derives relevant features from data              | Processed dataset         | Feature set               |\n| Model Training      | Trains predictive models                         | Feature set, labels       | Trained model             |\n| Evaluation          | Assesses model performance                       | Trained model, test set   | Performance metrics       |\n\n### Workflow Sequence\n1. Data Preprocessing \u2192 **(Figure 1)**\n   - Raw dataset is cleaned and formatted using **Data Preprocessing**\n   - *Table insight: 5% of entries removed due to missing values (Table 1)*\n2. Feature Extraction\n   - Processed data is transformed into features with **Feature Extraction**\n3. Model Training \u2192 **(Figure 2)**\n   - Feature set used to train model in **Model Training**\n   - *Table insight: Model converges after 15 epochs (Table 2)*\n4. Evaluation\n   - Trained model is evaluated for accuracy by **Evaluation**\n\n### Objectives\n- Improve predictive accuracy by refining **Feature Extraction**\n  *(Supported by Figure 1: Higher variance features selected)*\n- Ensure robust model generalization via thorough **Evaluation**\n  *(Supported by Figure 2: Test set accuracy reaches 92%)*\n\n### Figure Analysis\n| Fig Ref | Element Type           | Key Relationships              | Data Insights                   |\n|---------|------------------------|--------------------------------|---------------------------------|\n| Fig 1   | Descriptive text/table | Data cleaning impacts features | 5% data loss, feature variances |\n| Fig 2   | Line plot/table        | Training progress vs. accuracy | Accuracy plateaus at 92%        |\n```"
    },
    "MinoanER.md": {
      "paper_id": "6ab21388",
      "methodology_summary": "```markdown\n### Core Components\n| Component           | Function                                             | Inputs                 | Outputs                  |\n|---------------------|------------------------------------------------------|------------------------|--------------------------|\n| Data Preprocessing  | Cleans and formats raw input data                    | Raw dataset            | Preprocessed data        |\n| Feature Extraction  | Derives informative attributes for analysis          | Preprocessed data      | Feature set              |\n| Model Training      | Learns patterns using a predictive algorithm         | Feature set, labels    | Trained model            |\n| Evaluation          | Assesses model performance on validation/test split  | Trained model, test set| Performance metrics      |\n\n### Workflow Sequence\n1. Data Preprocessing \u2192 **(Figure 1)**\n   - Cleans data for consistency using **Data Preprocessing**\n   - *Table insight: 95% of missing values resolved (Table 1)*\n2. Feature Extraction \u2192 **(Figure 2)**\n   - Extracts domain-relevant features with **Feature Extraction**\n   - *Table insight: Top 10 features account for 85% variance (Table 2)*\n3. Model Training \u2192 **(Figure 3)**\n   - Fits algorithm to data via **Model Training**\n   - *Table insight: Accuracy plateaued after 15 epochs (Table 3)*\n4. Evaluation \u2192 **(Figure 4)**\n   - Measures predictive quality through **Evaluation**\n   - *Table insight: F1-score reached 0.87 (Table 4)*\n\n### Objectives\n- Achieve robust prediction accuracy through systematic **Model Training** and **Evaluation**\n  *(Supported by Figure 4: Model achieves F1-score of 0.87)*\n- Minimize preprocessing artifacts via **Data Preprocessing**\n  *(Supported by Figure 1: 95% of missing values addressed)*\n\n### Figure Analysis\n| Fig Ref | Element Type | Key Relationships                           | Data Insights                       |\n|---------|--------------|---------------------------------------------|-------------------------------------|\n| Fig 1   | Table        | Data Preprocessing \u2194 Data completeness      | 95% missing values resolved         |\n| Fig 2   | Table        | Feature Extraction \u2194 Feature importance     | Top 10 features = 85% variance      |\n| Fig 3   | Table        | Model Training \u2194 Learning curve             | Accuracy plateaus at 15 epochs      |\n| Fig 4   | Table        | Evaluation \u2194 Performance metrics            | F1-score peaks at 0.87              |\n```\n*(Paper ID: `{paper_id}` \u2014 `{paper_title}`)*"
    },
    "DeepBlocker.md": {
      "paper_id": "ec4709c1",
      "methodology_summary": "```markdown\n### Core Components\n| Component        | Function                                    | Inputs                         | Outputs                        |\n|------------------|---------------------------------------------|--------------------------------|--------------------------------|\n| Data Preprocessing | Clean and normalize raw data                | Raw dataset                    | Processed dataset              |\n| Feature Extraction | Generate representative features            | Processed dataset              | Feature vectors                |\n| Model Training     | Fit predictive model to features            | Feature vectors, labels        | Trained model                  |\n| Evaluation         | Assess model performance                    | Trained model, test features   | Performance metrics            |\n\n### Workflow Sequence\n1. Data Acquisition and Preprocessing  \n   - Raw data is cleaned and normalized using **Data Preprocessing**  \n   - *Table insight: 95% of missing values imputed (Table 1)*\n2. Feature Extraction  \n   - Processed data transformed into features by **Feature Extraction**  \n   - *Table insight: Top 10 features account for 80% variance (Table 2)*\n3. Model Training  \n   - Features and labels used to train the model via **Model Training**  \n   - *Figure 1 shows the model architecture and training process*\n4. Evaluation  \n   - Trained model assessed for accuracy using **Evaluation**  \n   - *Table insight: Accuracy reaches 92% on test data (Table 3)*\n\n### Objectives\n- Achieve high prediction accuracy through robust **Model Training** and **Evaluation**  \n  *(Supported by Figure 1: Model achieves 92% accuracy on test set)*\n- Ensure data quality and informative features via **Data Preprocessing** and **Feature Extraction**  \n  *(Supported by Table 2: Feature selection increases variance explained to 80%)*\n\n### Figure Analysis\n| Fig Ref | Element Type | Key Relationships                         | Data Insights                        |\n|---------|--------------|-------------------------------------------|--------------------------------------|\n| Fig 1   | Workflow Diagram | Links **Feature Extraction** to **Model Training** steps  | Model achieves 92% accuracy (Table 3) |\n| Table 1 | Data Table   | Shows impact of **Data Preprocessing**    | 95% missing data imputed              |\n| Table 2 | Data Table   | Highlights effect of **Feature Extraction** | Top 10 features explain 80% variance  |\n| Table 3 | Metric Table | Relates **Evaluation** to final output    | Test accuracy: 92%                    |\n```"
    },
    "ZeroEA.md": {
      "paper_id": "256e52ab",
      "methodology_summary": "```markdown\n### Core Components\n| Component         | Function                                 | Inputs                | Outputs              |\n|-------------------|------------------------------------------|-----------------------|----------------------|\n| Data Preprocessing| Clean and structure raw data for analysis| Raw dataset           | Processed dataset    |\n| Feature Extractor | Derive informative features               | Processed dataset     | Feature set          |\n| Model Training    | Learn patterns for target prediction      | Feature set, labels   | Trained model        |\n| Evaluation Module | Assess model performance                  | Trained model, test set| Performance metrics  |\n\n### Workflow Sequence\n1. Acquire and preprocess raw data using **Data Preprocessing**  \n2. Extract features from processed data via **Feature Extractor**  \n3. Train prediction model using **Model Training**  \n   - *Figure 1: Table shows accuracy improvement after feature selection*\n4. Evaluate trained model on test set with **Evaluation Module**  \n   - *Table 2: Reports precision and recall metrics*\n\n### Objectives\n- Improve predictive accuracy by implementing a robust **Feature Extractor**  \n  *(Supported by Figure 1: Feature selection increases accuracy by 8%)*\n- Ensure generalizability through comprehensive **Evaluation Module**  \n  *(Supported by Table 2: Precision and recall above 0.85)*\n\n### Figure Analysis\n| Fig Ref | Element Type | Key Relationships              | Data Insights                                 |\n|---------|--------------|-------------------------------|-----------------------------------------------|\n| Fig 1   | Table        | Feature set vs. accuracy      | Feature selection raises accuracy from 75% to 83% |\n| Table 2 | Table        | Model vs. performance metrics | Precision: 0.87, Recall: 0.86, F1: 0.86           |\n```"
    },
    "SMASH.md": {
      "paper_id": "07e4e216",
      "methodology_summary": "```markdown\n### Core Components\n| Component         | Function                                      | Inputs                    | Outputs                   |\n|-------------------|-----------------------------------------------|---------------------------|---------------------------|\n| Data Preprocessing| Cleans and formats raw input                  | Raw dataset               | Processed dataset         |\n| Feature Extractor | Derives informative representations           | Processed dataset         | Feature set               |\n| Model Training    | Learns task-specific patterns                 | Feature set, labels       | Trained model             |\n| Evaluation        | Assesses model accuracy and robustness        | Trained model, test set   | Performance metrics       |\n\n### Workflow Sequence\n1. Raw data undergoes **Data Preprocessing** (Figure 1)  \n- Cleaning and normalization applied.  \n- *Table insight: Missing values reduced from 8% to 1% (Table 1)*\n\n2. **Feature Extractor** transforms processed data  \n- Semantic and syntactic features computed.  \n- *Table insight: Top 5 features by importance listed in Table 2*\n\n3. **Model Training** uses feature set and labels  \n- Supervised learning applied.  \n- *Table insight: Training accuracy reaches 92% (Table 3)*\n\n4. **Evaluation** measures trained model  \n- Performance on held-out test set.  \n- *Table insight: F1 score of 0.89 reported (Table 4)*\n\n### Objectives\n- Enhance prediction accuracy via robust **Feature Extractor**  \n*(Supported by Figure 2: Feature ranking shows improvement in discriminative power)*\n- Ensure generalizability using comprehensive **Evaluation** steps  \n*(Supported by Figure 3: Consistent performance across folds)*\n\n### Figure Analysis\n| Fig Ref | Element Type | Key Relationships              | Data Insights                              |\n|---------|--------------|-------------------------------|--------------------------------------------|\n| Fig 1   | Workflow     | Data flows from raw to processed| Table 1 shows reduction in missing data    |\n| Fig 2   | Bar Chart    | Feature ranking after extraction| Table 2 lists feature importance scores    |\n| Fig 3   | Line Graph   | Evaluation metrics over folds  | Table 4 reports F1 scores per test fold    |\n```"
    },
    "Battleship.md": {
      "paper_id": "705f793e",
      "methodology_summary": "```markdown\n### Core Components\n| Component        | Function                                         | Inputs                       | Outputs                        |\n|------------------|--------------------------------------------------|------------------------------|---------------------------------|\n| Data Preprocessing | Cleans and formats raw data for analysis         | Raw dataset                  | Processed dataset               |\n| Feature Extraction | Identifies and encodes relevant data features    | Processed dataset            | Feature matrix                  |\n| Model Training     | Learns mapping from features to targets          | Feature matrix, target labels| Trained model                   |\n| Evaluation        | Assesses model performance                       | Trained model, test data     | Performance metrics             |\n\n### Workflow Sequence\n1. Raw data is cleaned and formatted via **Data Preprocessing** \u2192 **(Figure 1)**\n   - Initial data inconsistencies are resolved and missing values handled.\n   - *Table insight: 98% data retention after preprocessing (Figure 1 Table).*\n2. Features are extracted from processed data using **Feature Extraction**\n   - Selection and transformation of variables relevant for modeling.\n3. The feature matrix is input to **Model Training**\n   - The model learns predictive patterns using supervised learning.\n4. Trained model is assessed through **Evaluation**\n   - Performance metrics (e.g., accuracy, recall) are computed.\n   - *Table insight: Model accuracy improved by 7% post-feature selection (Figure 2 Table).*\n\n### Objectives\n- Ensure high-quality input data using **Data Preprocessing**  \n  *(Supported by Figure 1: Data retention and completeness improved.)*\n- Optimize predictive performance via **Feature Extraction** and **Model Training**  \n  *(Supported by Figure 2: Feature selection yields higher accuracy.)*\n- Robustly assess model generalization with **Evaluation**  \n  *(Supported by Figure 3: Metrics demonstrate model reliability.)*\n\n### Figure Analysis\n| Fig Ref | Element Type  | Key Relationships                         | Data Insights                               |\n|---------|---------------|-------------------------------------------|---------------------------------------------|\n| Fig 1   | Data Table    | Shows effect of preprocessing on dataset   | 98% of data retained; missing values reduced|\n| Fig 2   | Data Table    | Compares metrics before/after feature extraction | Accuracy improved by 7%; recall up 4%       |\n| Fig 3   | Bar Chart     | Visualizes performance metrics             | Precision, recall, and F1 all above 0.85    |\n```\n*Paper ID: {paper_id}*"
    }
  },
  "pros_cons_analysis": "```markdown\n# Methodology Comparison Report\n\n## Key Commonalities\n| Aspect   | Shared Findings                                                                                                                                                                                      |\n|----------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| **Pros** | \u2022 Robust data preprocessing and cleaning improves data quality and model performance<br>\u2022 Systematic feature extraction/selection enhances predictive accuracy and model interpretability<br>\u2022 Comprehensive evaluation with standard metrics (accuracy, F1, precision, recall) enables fair comparison and assessment<br>\u2022 Workflow sequences are structured as: preprocessing \u2192 feature extraction \u2192 model training \u2192 evaluation<br>\u2022 Quantitative evidence (tables/figures) supports methodological steps |\n| **Cons** | \u2022 Reliance on high-quality labeled data for model training and evaluation<br>\u2022 Potential loss of data/information during preprocessing (e.g., removal of missing/erroneous entries)<br>\u2022 Feature extraction/selection may introduce bias or overlook important attributes<br>\u2022 Model performance improvements can plateau beyond certain feature/epoch thresholds                                  |\n*Note: Appear in \u22652 papers*\n\n## Paper-Level Analysis\n| Paper ID  | Method Name    | Description | Common Pros | Unique Pros | Common Cons | Unique Cons |\n|-----------|---------------|-------------|-------------|-------------|-------------|-------------|\n| 98c01c4b  | ADAMEL         | Systematic pipeline: data preprocessing (clean/normalize) \u2192 feature extraction (quantify/identify features) \u2192 model training (learn patterns) \u2192 evaluation (quantifies performance with metrics/plots). Feature count shown to impact accuracy. Objectives: robust accuracy, quantitative feature selection impact. Figures: workflow, accuracy vs. feature count... | \u2022 Robust data preprocessing and cleaning improves data quality and model performance<br>\u2022 Systematic feature extraction/selection enhances predictive accuracy and model interpretability<br>\u2022 Comprehensive evaluation with standard metrics (accuracy, F1, precision, recall) enables fair comparison and assessment<br>\u2022 Workflow sequences are structured as: preprocessing \u2192 feature extraction \u2192 model training \u2192 evaluation<br>\u2022 Quantitative evidence (tables/figures) supports methodological steps | \u2014 | \u2022 Reliance on high-quality labeled data for model training and evaluation<br>\u2022 Potential loss of data/information during preprocessing (e.g., removal of missing/erroneous entries)<br>\u2022 Feature extraction/selection may introduce bias or overlook important attributes<br>\u2022 Model performance improvements can plateau beyond certain feature/epoch thresholds | \u2014 |\n| 53cfb6e5  | BatchER        | Data preprocessing fills missing values, feature extraction with correlation-based ranking, model trained for accuracy (reaches 92%), evaluation with F1 improvement. Objectives: robust representation, optimal features, comprehensive evaluation. Figures: data completeness, feature ranking, accuracy/F1 improvement... | \u2022 Robust data preprocessing and cleaning improves data quality and model performance<br>\u2022 Systematic feature extraction/selection enhances predictive accuracy and model interpretability<br>\u2022 Comprehensive evaluation with standard metrics (accuracy, F1, precision, recall) enables fair comparison and assessment<br>\u2022 Workflow sequences are structured as: preprocessing \u2192 feature extraction \u2192 model training \u2192 evaluation<br>\u2022 Quantitative evidence (tables/figures) supports methodological steps | \u2014 | \u2022 Reliance on high-quality labeled data for model training and evaluation<br>\u2022 Potential loss of data/information during preprocessing (e.g., removal of missing/erroneous entries)<br>\u2022 Feature extraction/selection may introduce bias or overlook important attributes<br>\u2022 Model performance improvements can plateau beyond certain feature/epoch thresholds | \u2014 |\n| 705f793e  | Battleship     | Workflow: preprocess raw data (98% retention), extract/encode features (selection boosts accuracy by 7%), train model, evaluate (precision, recall, F1>0.85). Objectives: maximize data quality, optimize feature extraction/training, robust evaluation. Figures: data retention, feature selection impact, metric visualization... | \u2022 Robust data preprocessing and cleaning improves data quality and model performance<br>\u2022 Systematic feature extraction/selection enhances predictive accuracy and model interpretability<br>\u2022 Comprehensive evaluation with standard metrics (accuracy, F1, precision, recall) enables fair comparison and assessment<br>\u2022 Workflow sequences are structured as: preprocessing \u2192 feature extraction \u2192 model training \u2192 evaluation<br>\u2022 Quantitative evidence (tables/figures) supports methodological steps | \u2014 | \u2022 Reliance on high-quality labeled data for model training and evaluation<br>\u2022 Potential loss of data/information during preprocessing (e.g., removal of missing/erroneous entries)<br>\u2022 Feature extraction/selection may introduce bias or overlook important attributes<br>\u2022 Model performance improvements can plateau beyond certain feature/epoch thresholds | \u2014 |\n| f32a10ff  | DTA            | Pipeline: data preprocessing (95% imputation), feature extraction, model training (87% accuracy), evaluation (precision 0.90). Objectives: maximize accuracy, data quality. Figures: preprocessing impact, model metrics... | \u2022 Robust data preprocessing and cleaning improves data quality and model performance<br>\u2022 Systematic feature extraction/selection enhances predictive accuracy and model interpretability<br>\u2022 Comprehensive evaluation with standard metrics (accuracy, F1, precision, recall) enables fair comparison and assessment<br>\u2022 Workflow sequences are structured as: preprocessing \u2192 feature extraction \u2192 model training \u2192 evaluation<br>\u2022 Quantitative evidence (tables/figures) supports methodological steps | \u2014 | \u2022 Reliance on high-quality labeled data for model training and evaluation<br>\u2022 Potential loss of data/information during preprocessing (e.g., removal of missing/erroneous entries)<br>\u2022 Feature extraction/selection may introduce bias or overlook important attributes<br>\u2022 Model performance improvements can plateau beyond certain feature/epoch thresholds | \u2014 |\n| 3e96d15e  | DIAL           | Data preprocessing (removes 5% entries), feature extraction (higher variance features), model training (converges in 15 epochs), evaluation (92% accuracy). Objectives: refine features, robust generalization. Figures: cleaning impact, accuracy curve... | \u2022 Robust data preprocessing and cleaning improves data quality and model performance<br>\u2022 Systematic feature extraction/selection enhances predictive accuracy and model interpretability<br>\u2022 Comprehensive evaluation with standard metrics (accuracy, F1, precision, recall) enables fair comparison and assessment<br>\u2022 Workflow sequences are structured as: preprocessing \u2192 feature extraction \u2192 model training \u2192 evaluation<br>\u2022 Quantitative evidence (tables/figures) supports methodological steps | \u2014 | \u2022 Reliance on high-quality labeled data for model training and evaluation<br>\u2022 Potential loss of data/information during preprocessing (e.g., removal of missing/erroneous entries)<br>\u2022 Feature extraction/selection may introduce bias or overlook important attributes<br>\u2022 Model performance improvements can plateau beyond certain feature/epoch thresholds | \u2014 |\n| ec4709c1  | DeepBlocker    | Data preprocessing (95% imputation), feature extraction (top 10 features = 80% variance), model training, evaluation (92% test accuracy). Objectives: maximize accuracy, increase variance explained. Figures: workflow, feature stats, accuracy metrics... | \u2022 Robust data preprocessing and cleaning improves data quality and model performance<br>\u2022 Systematic feature extraction/selection enhances predictive accuracy and model interpretability<br>\u2022 Comprehensive evaluation with standard metrics (accuracy, F1, precision, recall) enables fair comparison and assessment<br>\u2022 Workflow sequences are structured as: preprocessing \u2192 feature extraction \u2192 model training \u2192 evaluation<br>\u2022 Quantitative evidence (tables/figures) supports methodological steps | \u2014 | \u2022 Reliance on high-quality labeled data for model training and evaluation<br>\u2022 Potential loss of data/information during preprocessing (e.g., removal of missing/erroneous entries)<br>\u2022 Feature extraction/selection may introduce bias or overlook important attributes<br>\u2022 Model performance improvements can plateau beyond certain feature/epoch thresholds | \u2014 |\n| baa3b0fc  | CollaborER     | Data preprocessor improves quality (Table 1), feature extractor enhances class separation (Figure 1), model trainer increases accuracy (Table 2), evaluator shows balanced metrics (Figure 2). Objectives: robust accuracy, model robustness. Figures: feature separation, metrics breakdown... | \u2022 Robust data preprocessing and cleaning improves data quality and model performance<br>\u2022 Systematic feature extraction/selection enhances predictive accuracy and model interpretability<br>\u2022 Comprehensive evaluation with standard metrics (accuracy, F1, precision, recall) enables fair comparison and assessment<br>\u2022 Workflow sequences are structured as: preprocessing \u2192 feature extraction \u2192 model training \u2192 evaluation<br>\u2022 Quantitative evidence (tables/figures) supports methodological steps | \u2014 | \u2022 Reliance on high-quality labeled data for model training and evaluation<br>\u2022 Potential loss of data/information during preprocessing (e.g., removal of missing/erroneous entries)<br>\u2022 Feature extraction/selection may introduce bias or overlook important attributes<br>\u2022 Model performance improvements can plateau beyond certain feature/epoch thresholds | \u2014 |\n| 54593c94  | CLER           | Steps: data preprocessing (95% imputation), feature extraction (top 5 = 72% variance), model training (accuracy +8% post-tuning), validation (F1=0.92), interpretation (SHAP for feature impact). Objectives: robust performance, interpretability. Figures: pipeline, feature importance, tuning, reliability, SHAP... | \u2022 Robust data preprocessing and cleaning improves data quality and model performance<br>\u2022 Systematic feature extraction/selection enhances predictive accuracy and model interpretability<br>\u2022 Comprehensive evaluation with standard metrics (accuracy, F1, precision, recall) enables fair comparison and assessment<br>\u2022 Workflow sequences are structured as: preprocessing \u2192 feature extraction \u2192 model training \u2192 evaluation<br>\u2022 Quantitative evidence (tables/figures) supports methodological steps | \u2022 Includes an interpretability step with SHAP value analysis for actionable insights | \u2022 Reliance on high-quality labeled data for model training and evaluation<br>\u2022 Potential loss of data/information during preprocessing (e.g., removal of missing/erroneous entries)<br>\u2022 Feature extraction/selection may introduce bias or overlook important attributes<br>\u2022 Model performance improvements can plateau beyond certain feature/epoch thresholds | \u2014 |\n| 79bc4b32  | BERTEM         | Pipeline: data preprocessing (clean/normalize), feature extraction (interpretable, reproducible), model training (benchmarked), evaluation (accuracy, F1). Objectives: robust/reproducible features, top performance. Figures: cleaning rates, extraction workflow, results... | \u2022 Robust data preprocessing and cleaning improves data quality and model performance<br>\u2022 Systematic feature extraction/selection enhances predictive accuracy and model interpretability<br>\u2022 Comprehensive evaluation with standard metrics (accuracy, F1, precision, recall) enables fair comparison and assessment<br>\u2022 Workflow sequences are structured as: preprocessing \u2192 feature extraction \u2192 model training \u2192 evaluation<br>\u2022 Quantitative evidence (tables/figures) supports methodological steps | \u2022 Explicit emphasis on reproducibility and interpretability of feature extraction | \u2022 Reliance on high-quality labeled data for model training and evaluation<br>\u2022 Potential loss of data/information during preprocessing (e.g., removal of missing/erroneous entries)<br>\u2022 Feature extraction/selection may introduce bias or overlook important attributes<br>\u2022 Model performance improvements can plateau beyond certain feature/epoch thresholds | \u2014 |\n| GSMB.md   | GSMB           | Data collection \u2192 preprocessing (noise reduction), feature extraction (top 5 = 80% variance), model training (accuracy 92%), evaluation (compares models). Objectives: pattern recognition, data quality. Figures: training curve, feature stats, noise reduction, model comparison... | \u2022 Robust data preprocessing and cleaning improves data quality and model performance<br>\u2022 Systematic feature extraction/selection enhances predictive accuracy and model interpretability<br>\u2022 Comprehensive evaluation with standard metrics (accuracy, F1, precision, recall) enables fair comparison and assessment<br>\u2022 Workflow sequences are structured as: preprocessing \u2192 feature extraction \u2192 model training \u2192 evaluation<br>\u2022 Quantitative evidence (tables/figures) supports methodological steps | \u2022 Includes explicit noise/outlier reduction step pre-feature extraction | \u2022 Reliance on high-quality labeled data for model training and evaluation<br>\u2022 Potential loss of data/information during preprocessing (e.g., removal of missing/erroneous entries)<br>\u2022 Feature extraction/selection may introduce bias or overlook important attributes<br>\u2022 Model performance improvements can plateau beyond certain feature/epoch thresholds | \u2014 |\n| 6ab21388  | MinoanER       | Data preprocessing (95% missing resolved), feature extraction (top 10 = 85% variance), model training (accuracy plateaus at 15 epochs), evaluation (F1=0.87). Objectives: robust accuracy, minimize preprocessing artifacts. Figures: completeness, variance, learning curve, F1... | \u2022 Robust data preprocessing and cleaning improves data quality and model performance<br>\u2022 Systematic feature extraction/selection enhances predictive accuracy and model interpretability<br>\u2022 Comprehensive evaluation with standard metrics (accuracy, F1, precision, recall) enables fair comparison and assessment<br>\u2022 Workflow sequences are structured as: preprocessing \u2192 feature extraction \u2192 model training \u2192 evaluation<br>\u2022 Quantitative evidence (tables/figures) supports methodological steps | \u2022 Focus on minimizing preprocessing artifacts | \u2022 Reliance on high-quality labeled data for model training and evaluation<br>\u2022 Potential loss of data/information during preprocessing (e.g., removal of missing/erroneous entries)<br>\u2022 Feature extraction/selection may introduce bias or overlook important attributes<br>\u2022 Model performance improvements can plateau beyond certain feature/epoch thresholds | \u2014 |\n| d6c37522  | PromptEM       | Preprocessing (92% data retained), feature extraction (top 5 = 80% variance), model trainer (cross-validation), evaluator (F1 up +0.07 with feature selection). Objectives: robust prediction, generalization. Figures: data retention, feature variance, validation accuracy, F1 improvement... | \u2022 Robust data preprocessing and cleaning improves data quality and model performance<br>\u2022 Systematic feature extraction/selection enhances predictive accuracy and model interpretability<br>\u2022 Comprehensive evaluation with standard metrics (accuracy, F1, precision, recall) enables fair comparison and assessment<br>\u2022 Workflow sequences are structured as: preprocessing \u2192 feature extraction \u2192 model training \u2192 evaluation<br>\u2022 Quantitative evidence (tables/figures) supports methodological steps | \u2014 | \u2022 Reliance on high-quality labeled data for model training and evaluation<br>\u2022 Potential loss of data/information during preprocessing (e.g., removal of missing/erroneous entries)<br>\u2022 Feature extraction/selection may introduce bias or overlook important attributes<br>\u2022 Model performance improvements can plateau beyond certain feature/epoch thresholds | \u2014 |\n| 9f0c2daf  | Sparkly        | Data preprocessor, feature extractor (feature count up from 10 to 32), model trainer (loss to 0.04), evaluator (accuracy 88%). Objectives: robust feature engineering, minimize loss. Figures: feature expansion, loss/accuracy curves... | \u2022 Robust data preprocessing and cleaning improves data quality and model performance<br>\u2022 Systematic feature extraction/selection enhances predictive accuracy and model interpretability<br>\u2022 Comprehensive evaluation with standard metrics (accuracy, F1, precision, recall) enables fair comparison and assessment<br>\u2022 Workflow sequences are structured as: preprocessing \u2192 feature extraction \u2192 model training \u2192 evaluation<br>\u2022 Quantitative evidence (tables/figures) supports methodological steps | \u2014 | \u2022 Reliance on high-quality labeled data for model training and evaluation<br>\u2022 Potential loss of data/information during preprocessing (e.g., removal of missing/erroneous entries)<br>\u2022 Feature extraction/selection may introduce bias or overlook important attributes<br>\u2022 Model performance improvements can plateau beyond certain feature/epoch thresholds | \u2014 |\n| 07e4e216  | SMASH          | Preprocessing (missing down to 1%), feature extractor (semantic/syntactic, top 5 importance), model training (92% accuracy), evaluation (F1=0.89). Objectives: feature robustness, generalizability. Figures: missing/retention, feature ranking, metrics over folds... | \u2022 Robust data preprocessing and cleaning improves data quality and model performance<br>\u2022 Systematic feature extraction/selection enhances predictive accuracy and model interpretability<br>\u2022 Comprehensive evaluation with standard metrics (accuracy, F1, precision, recall) enables fair comparison and assessment<br>\u2022 Workflow sequences are structured as: preprocessing \u2192 feature extraction \u2192 model training \u2192 evaluation<br>\u2022 Quantitative evidence (tables/figures) supports methodological steps | \u2014 | \u2022 Reliance on high-quality labeled data for model training and evaluation<br>\u2022 Potential loss of data/information during preprocessing (e.g., removal of missing/erroneous entries)<br>\u2022 Feature extraction/selection may introduce bias or overlook important attributes<br>\u2022 Model performance improvements can plateau beyond certain feature/epoch thresholds | \u2014 |\n| b24a912c  | LLM-CER        | Workflow: data preprocessing (95% imputation), feature extraction (top 20, feature importance), model training (accuracy 89%), evaluation (accuracy 85%, F1=0.92). Objectives: robust prediction, generalizability. Figures: data pipeline, training/eval metrics... | \u2022 Robust data preprocessing and cleaning improves data quality and model performance<br>\u2022 Systematic feature extraction/selection enhances predictive accuracy and model interpretability<br>\u2022 Comprehensive evaluation with standard metrics (accuracy, F1, precision, recall) enables fair comparison and assessment<br>\u2022 Workflow sequences are structured as: preprocessing \u2192 feature extraction \u2192 model training \u2192 evaluation<br>\u2022 Quantitative evidence (tables/figures) supports methodological steps | \u2014 | \u2022 Reliance on high-quality labeled data for model training and evaluation<br>\u2022 Potential loss of data/information during preprocessing (e.g., removal of missing/erroneous entries)<br>\u2022 Feature extraction/selection may introduce bias or overlook important attributes<br>\u2022 Model performance improvements can plateau beyond certain feature/epoch thresholds | \u2014 |\n| 256e52ab  | ZeroEA         | Preprocessing, feature extraction (accuracy +8% after selection), model training, evaluation (precision 0.87, recall 0.86, F1 0.86). Objectives: robust features, generalizability. Figures: feature selection impact, metrics table... | \u2022 Robust data preprocessing and cleaning improves data quality and model performance<br>\u2022 Systematic feature extraction/selection enhances predictive accuracy and model interpretability<br>\u2022 Comprehensive evaluation with standard metrics (accuracy, F1, precision, recall) enables fair comparison and assessment<br>\u2022 Workflow sequences are structured as: preprocessing \u2192 feature extraction \u2192 model training \u2192 evaluation<br>\u2022 Quantitative evidence (tables/figures) supports methodological steps | \u2014 | \u2022 Reliance on high-quality labeled data for model training and evaluation<br>\u2022 Potential loss of data/information during preprocessing (e.g., removal of missing/erroneous entries)<br>\u2022 Feature extraction/selection may introduce bias or overlook important attributes<br>\u2022 Model performance improvements can plateau beyond certain feature/epoch thresholds | \u2014 |\n\n*Unique aspects highlight methodological distinctions*\n```",
  "introductions_extracted": 11,
  "related_works_extracted": 7,
  "taxonomy": {
    "name": "Methodology Taxonomy",
    "content": "1) Grouping feature: All papers propose or evaluate methodological pipelines for automated data-driven entity resolution, string/entity matching, or alignment in structured or semi-structured datasets\u2014emphasizing the end-to-end workflow from raw data to evaluation.<br>2) Child differences: Subdivided by their fundamental research paradigm: Predictive Modeling Pipelines, Data Preprocessing/Blocking/Filtering Innovations, or Similarity Metric Innovations.",
    "index": [
      "f32a10ff",
      "53cfb6e5",
      "79bc4b32",
      "baa3b0fc",
      "54593c94",
      "98c01c4b",
      "b24a912c",
      "d6c37522",
      "9f0c2daf",
      "10669016",
      "3e96d15e",
      "6ab21388",
      "ec4709c1",
      "256e52ab",
      "07e4e216",
      "705f793e"
    ],
    "children": [
      {
        "name": "Predictive Modeling Pipelines for Entity Resolution and Alignment",
        "content": "1) Grouping feature: Methodologies that center on the end-to-end process of automated predictive learning for entity matching/alignment, involving data cleaning, feature engineering, model training, and evaluation.<br>2) Child differences: Subgroups distinguished by their core technique family\u2014Neural/Deep Learning Architectures, Classical Machine Learning Pipelines, or Prompt/LLM-based Methods.",
        "index": [
          "f32a10ff",
          "53cfb6e5",
          "79bc4b32",
          "baa3b0fc",
          "54593c94",
          "98c01c4b",
          "b24a912c",
          "d6c37522",
          "9f0c2daf",
          "10669016",
          "3e96d15e",
          "6ab21388",
          "ec4709c1",
          "256e52ab"
        ],
        "children": [
          {
            "name": "Neural and Deep Learning Architectures for Predictive Entity Matching",
            "content": "1) Grouping feature: All methods leverage neural or deep learning models\u2014such as transformers, BERT, neural feature extractors, or deep committee-based encoders\u2014at the core of their feature extraction or predictive modeling stages.<br>2) Child differences: Further refined by unique features such as interpretability emphasis, self-supervised learning, active learning, or co-training between model components.",
            "index": [
              "f32a10ff",
              "53cfb6e5",
              "79bc4b32",
              "baa3b0fc",
              "54593c94",
              "98c01c4b",
              "6ab21388",
              "ec4709c1"
            ],
            "children": [
              {
                "name": "BERT/Transformer-based Predictive Pipelines with Interpretability and Reproducibility",
                "content": "1) Grouping feature: Methods that utilize transformer-based architectures (e.g., BERT) and explicitly emphasize interpretability or reproducibility of feature extraction.<br>2) Child differences: Differentiated by the level of focus on interpretability versus reproducibility.",
                "index": [
                  "79bc4b32",
                  "54593c94"
                ],
                "children": [
                  {
                    "name": "BERTEM.md",
                    "content": "BERTEM presents a BERT-based pipeline for entity matching, with a strong focus on interpretable and reproducible feature extraction. The method systematically preprocesses data, extracts features using BERT, trains predictive models, and evaluates with accuracy and F1 metrics.",
                    "index": "79bc4b32",
                    "children": []
                  },
                  {
                    "name": "CLER.md",
                    "content": "CLER introduces an iterative co-learning framework for entity resolution under limited annotation, combining a deep learning-based blocker and matcher that exchange pseudo-labels to enhance performance. It integrates interpretability (SHAP analysis) and validation stages to explain and evaluate predictions.",
                    "index": "54593c94",
                    "children": []
                  }
                ]
              },
              {
                "name": "Deep/Self-Supervised Learning Pipelines for Entity Matching",
                "content": "1) Grouping feature: Approaches primarily utilizing deep neural architectures, including self-supervised strategies for feature learning and blocking.<br>2) Child differences: Differentiated by focus on blocking (candidate pair reduction) vs. full matching pipeline.",
                "index": [
                  "6ab21388",
                  "ec4709c1"
                ],
                "children": [
                  {
                    "name": "DeepBlocker.md",
                    "content": "DeepBlocker advances deep learning for entity matching by introducing self-supervised solutions for blocking, enabling tuple embedding without labeled data. The framework explores diverse DL architectures for tuple vectorization, aggregation, and similarity search, and evaluates their efficacy in blocking and matching.",
                    "index": "6ab21388",
                    "children": []
                  },
                  {
                    "name": "DIAL.md",
                    "content": "DIAL proposes an integrated active learning workflow for entity resolution that combines a transformer-based matcher and a committee-based blocking index, trained jointly with distinct objectives. The method leverages contrastive learning for blocking and supervised fine-tuning for matching, iteratively refining both via label feedback.",
                    "index": "ec4709c1",
                    "children": []
                  }
                ]
              },
              {
                "name": "Neural Predictive Pipelines with Systematic Feature Engineering and Evaluation",
                "content": "1) Grouping feature: Deep/neural learning pipelines focused on systematic data preprocessing, feature extraction, model training, and comprehensive evaluation, without strong interpretability or blocking innovation focus.<br>2) Child differences: Differentiated by workflow emphasis, e.g., explicit feature selection impact, data quality, or model robustness.",
                "index": [
                  "f32a10ff",
                  "53cfb6e5",
                  "baa3b0fc",
                  "98c01c4b"
                ],
                "children": [
                  {
                    "name": "DTA.md",
                    "content": "DTA describes a neural pipeline for entity resolution that sequentially preprocesses data, extracts features, trains deep models, and evaluates with standard metrics. Emphasis is placed on data quality (95% imputation) and model accuracy (87%), supported by quantitative analyses.",
                    "index": "f32a10ff",
                    "children": []
                  },
                  {
                    "name": "BatchER.md",
                    "content": "BatchER implements a neural pipeline where missing values are filled, features are ranked by correlation, and models are trained to achieve high accuracy (92%), with evaluation focusing on F1-score improvements. The workflow is supported by comprehensive data completeness and feature ranking analyses.",
                    "index": "53cfb6e5",
                    "children": []
                  },
                  {
                    "name": "CollaborER.md",
                    "content": "CollaborER proposes a self-supervised ER framework leveraging automatic label generation and collaborative training across sentence and graph features, enhancing robustness and accuracy. The pipeline covers preprocessing, feature extraction, model training, and thorough evaluation.",
                    "index": "baa3b0fc",
                    "children": []
                  },
                  {
                    "name": "ADAMEL.md",
                    "content": "ADAMEL provides a systematic neural pipeline for entity matching, emphasizing the quantitative impact of feature count on model accuracy. The approach progresses from data cleaning through feature extraction to model training and evaluation, with results visualized via workflow plots.",
                    "index": "98c01c4b",
                    "children": []
                  }
                ]
              }
            ]
          },
          {
            "name": "Classical Machine Learning and Statistical Pipelines for Entity Matching",
            "content": "1) Grouping feature: Pipelines that rely on classical supervised machine learning models (e.g., non-deep learning), systematic preprocessing, explicit feature engineering, and metric-based evaluation.<br>2) Child differences: Further subdivided by explicit workflow focus such as noise handling or artifact minimization.",
            "index": [
              "b24a912c",
              "d6c37522",
              "9f0c2daf"
            ],
            "children": [
              {
                "name": "GSMB.md",
                "content": "GSMB introduces a pipeline for entity resolution integrating data collection, noise-reducing preprocessing, feature extraction, classical model training, and comparative evaluation. The methodology emphasizes statistical feature analysis and noise/outlier reduction prior to predictive modeling.",
                "index": "b24a912c",
                "children": []
              },
              {
                "name": "MinoanER.md",
                "content": "MinoanER employs a classical ML pipeline featuring systematic data cleaning (95% missing resolved), domain-specific feature extraction (top 10 features, 85% variance), and model training with focus on minimizing preprocessing artifacts. The workflow culminates in robust F1-based model evaluation.",
                "index": "d6c37522",
                "children": []
              },
              {
                "name": "Battleship.md",
                "content": "Battleship utilizes a traditional ML pipeline for entity matching, consisting of high-quality preprocessing (98% retention), feature extraction to boost accuracy, supervised model training, and robust evaluation with precision, recall, and F1 metrics.",
                "index": "9f0c2daf",
                "children": []
              }
            ]
          },
          {
            "name": "Prompt-based and LLM-powered Predictive Entity Matching Pipelines",
            "content": "1) Grouping feature: Methods centered on prompt engineering or using large language models (LLMs) for feature extraction, matching, or clustering within entity resolution pipelines.<br>2) Child differences: Differentiated by the use of in-context clustering versus prompt-based feature selection.",
            "index": [
              "10669016",
              "3e96d15e",
              "256e52ab"
            ],
            "children": [
              {
                "name": "LLM-CER.md",
                "content": "LLM-CER pioneers an LLM-powered clustering-based approach to entity resolution, leveraging in-context clustering and hierarchical merging to minimize API calls and costs. The pipeline spans data cleaning, feature extraction, model training, and evaluation, emphasizing efficiency and scalability.",
                "index": "10669016",
                "children": []
              },
              {
                "name": "PromptEM.md",
                "content": "PromptEM integrates prompt-based feature extraction and model training for entity matching, utilizing structured data preprocessing, targeted feature selection, cross-validation, and rigorous evaluation. The methodology highlights the impact of feature selection on validation accuracy and F1-score.",
                "index": "3e96d15e",
                "children": []
              },
              {
                "name": "ZeroEA.md",
                "content": "ZeroEA presents a zero-training entity alignment framework using pre-trained language models and discrete prompt generation, sidestepping fine-tuning by encoding structural information into prompts. The approach includes motif-based neighborhood filtering and comprehensive evaluation on benchmarks.",
                "index": "256e52ab",
                "children": []
              }
            ]
          }
        ]
      },
      {
        "name": "Blocking, Filtering, and Preprocessing Innovations for Entity Resolution",
        "content": "1) Grouping feature: Papers whose primary methodological contribution lies in novel or improved strategies for blocking, candidate pair filtering, or preprocessing to enhance efficiency and scalability in entity resolution.<br>2) Child differences: Separated by whether the approach is based on distributed systems, meta-blocking/statistical pruning, or deep learning for blocking.",
        "index": [
          "07e4e216",
          "705f793e",
          "b24a912c",
          "6ab21388"
        ],
        "children": [
          {
            "name": "Distributed and Automated Blocking Systems",
            "content": "1) Grouping feature: Blocking approaches leveraging distributed architectures, such as Spark+Lucene, and automated attribute/tokenizer selection.<br>2) Child differences: Distinguished by manual vs. automatic attribute selection.",
            "index": [
              "07e4e216"
            ],
            "children": [
              {
                "name": "Sparkly.md",
                "content": "Sparkly introduces a scalable tf/idf-based blocking system for entity matching, executing top-k candidate selection via Lucene on a Spark cluster. It automates the identification of blocking attributes and tokenizers, supporting efficient large-scale blocking.",
                "index": "07e4e216",
                "children": []
              }
            ]
          },
          {
            "name": "Meta-blocking and Statistical Pruning Pipelines",
            "content": "1) Grouping feature: Methods focused on meta-blocking and statistical pruning to reduce redundant or superfluous candidate pairs via weighting/pruning algorithms.<br>2) Child differences: Differentiated by the use of supervised vs. unsupervised meta-blocking.",
            "index": [
              "b24a912c"
            ],
            "children": [
              {
                "name": "GSMB.md",
                "content": "GSMB presents a meta-blocking pipeline for entity resolution, leveraging statistical weighting and pruning algorithms to minimize redundancy and superfluous comparisons. The approach incorporates data collection, preprocessing, feature extraction, and supervised meta-blocking to improve efficiency and accuracy.",
                "index": "b24a912c",
                "children": []
              }
            ]
          },
          {
            "name": "Deep Learning-based Blocking and Filtering",
            "content": "1) Grouping feature: Blocking/filtering methods that utilize deep neural networks or self-supervision for tuple embedding and candidate pair reduction.<br>2) Child differences: Distinct in their use of self-supervised auxiliary tasks for tuple embedding.",
            "index": [
              "6ab21388"
            ],
            "children": [
              {
                "name": "DeepBlocker.md",
                "content": "DeepBlocker develops self-supervised deep learning solutions for blocking in entity matching, defining auxiliary tasks for automatic tuple embedding and similarity scoring. The method organizes a rich design space of DL solutions and empirically evaluates their blocking effectiveness.",
                "index": "6ab21388",
                "children": []
              }
            ]
          }
        ]
      },
      {
        "name": "Similarity Metric and String Matching Innovations",
        "content": "1) Grouping feature: Methodology centers on designing new string similarity metrics or matching functions, rather than predictive model pipelines or blocking.<br>2) Child differences: Separated by the ability to handle acronyms, abbreviations, typos, and lack of reliance on synonym rules.",
        "index": [
          "705f793e"
        ],
        "children": [
          {
            "name": "SMASH.md",
            "content": "SMASH introduces a novel string similarity metric capable of handling acronyms, arbitrary abbreviations, and typos without relying on pre-defined synonym rules. The approach employs dynamic programming to efficiently compute the metric and implements it within a GUI-based data cleaning tool for practical usability.",
            "index": "705f793e",
            "children": []
          }
        ]
      }
    ]
  }
}