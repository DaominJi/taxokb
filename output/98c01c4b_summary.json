{
    "Introduction": {
        "content": "Entity linkage (EL), also known as entity resolution, record linkage, entity matching, is a fundamental task in data mining, database, and knowledge integration with numerous applications, including deduplication, data cleaning, user stitching, and more. The key idea is to identify records across different data sources (e.g., databases, websites, knowledge base, etc.) that represent the same real-world\n\n[^0]Danai Koutra\nUniversity of Michigan\ndkoutra@umich.edu\n\n![img-0.jpeg](img-0.jpeg)\n\nFigure 1: Well-labeled data sources (e.g., blue tables) are generally outnumbered by massive unlabeled data in real-world knowledge integration scenarios. Entity linkage models trained only on welllabeled samples fail to handle new sources with different contexts or formats (e.g., red tables). Our proposed framework, AdaMEL automatically learns the attribute importance that adapts to the massive unlabeled data from different sources during training, and then uses it as the transferable knowledge to perform matching.\nentity. For example, some music websites record the song \"Hey Jude\" by Paul McCartney with the name abbreviation (i.e., \"P.M.\") while others with the band name (i.e., \"The Beatles\"). As newlygenerated data surge over time, accurately consolidating the same entities across semi-structured web sources becomes increasingly important, especially in areas such as knowledge base establishment $[7,13]$ and personalization [16].\n\nMethods for solving the entity linkage problem across data sources include rule reasoning [9, 32], computation of similarity between attributes or schemas [2], and active learning [30]. In particular, recent deep learning approaches that are based on heterogeneous schema matching or word matching [23, 26, 27] have been widely studied. Their promising performance mainly comes from the sophisticated word-level operations such as RNN and Attention [11, 26] to represent token sequences under attributes as the summarization, or the usage of pretrained language models [20] to better learn the word semantics. However, these approaches implicitly assume that the \"matching/non-matching\" information for training records is available (e.g., the music records in source 1 and 2 shown in the two blue tables of Fig. 1) and can be queried through the learning process, which does not always hold in practice. In real-world knowledge integration scenarios, new data come incrementally with only a few records being labeled (e.g., through manual confirmation), and most of them are unlabeled. While the existing frameworks can handle high-quality labeled data, they\n\n\n[^0]:    This work is licensed under the Creative Commons BY-NC-ND 4.0 International License. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of this license. For any use beyond those covered by this license, obtain permission by emailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights licensed to the VLDB Endowment.\n    Proceedings of the VLDB Endowment, Vol. 15, No. 3 ISSN 2150-8097. doi:10.14778/3494124.3494131\n\ncannot deal with the massive volume of unlabeled and previously unseen data or missing values. As the example shown in Fig. 1, a model trained on the high-quality labeled data (blue tables) would fail to generalize to the new data sources (red tables) with missing and different attribute values (i.e., \"Artist\"), as well as new attributes or attributes that are rarely seen (i.e., \"Gender\").\n\nMotivated by real-world knowledge integration settings, we consider three key challenges in the multi-data source scenario: (C1) missing attribute values from unseen data sources; (C2) new attributes from unseen data sources; and (C3) different value distribution in unseen data sources. Based on these challenges, we seek to tackle the following MEL (multi-source entity linkage) problem: Given labeled data from a limited set of sources, what knowledge can be learned and how can it be transferred to automatically handle multiple unseen data sources with different value distribution, missing values and new attributes? To solve this task, human experts typically rely on prior domain knowledge to learn the attribute importance in the seen data sources, and then transfer it to match the unseen records based on the similarity of attribute values. As challenges (C1-C3) lead to different attribute importance, human experts would adapt their knowledge learned from the seen data sources for the unseen ones. For example in Figure 1, when trying to link entities in the red table, the importance of \"Artist\" learned in the seen data sources (blue table) is down-weighted due to the fact that name abbreviation is less informative. On the other hand, even though it is a rarely-seen attribute in the seen sources, \"Gender\" would be more important because the gender difference between artists naturally leads to non-matching of music records regardless of the fact that entity pairs could share the same \"Title\" (\"Hello\") and very similar \"Artist\" values (\"A. A.\" and \"A. W.\"). This process, however, is tedious and does not scale to massive unlabeled data involved in real-world entity linkage problems, where large volumes of new data sources continuously arrive.\n\nFollowing this intuition, we propose ADAMEL, a deep transfer learning framework that leverages both the labeled and massive unlabeled data to train the model for multi-source entity linkage while addressing the aforementioned challenges (C1-C3). We define the attribute importance in entity linkage as the high-level transferable knowledge and automatically learn it through a proposed attribute-level attention mechanism (what to transfer). In general, as transfer learning aims to transfers knowledge learned from the domain with abundant training data to a related target domain with limited data, the existing works rely on increasing the labeling volume by introducing the external data (e.g., public knowledge bases) [40] or reusing the seen training data [34]. On the contrary, ADAMEL adopts domain adaptation (DA) to jointly update the attention scores for attributes in both the seen and unseen data as the basis for entity linkage (how to handle multiple sources), so that the knowledge is adaptive to the continuously incoming data sources. In addition, the insightful feature importance as transferrable knowledge is explicitly defined by ADAMEL to benefit both human interpretation and the performance of learning tasks, which is also different from methods that incorporates the knowledge into pretrained models like \"black-boxes\", such as the contextual word/character embeddings. While the widely-adopted NLP-based attribute summarization in existing works [11, 23, 27] could accurately capture the word-level semantics using pretrained language\nmodels or domain knowledge for all attributes, they are computationally expensive in practice. On the contrary, the feature-level attention is much faster to obtain and we claim that the impact of word-level similarity of some attributes is limited and even harmful for model performance if those attributes are not important.\n\nADAMEL follows the real-world scenario and assumes that new data sources come from the same or neighboring domains in batches (e.g., music from different websites). Transferring knowledge between irrelevant domains (e.g., celebrities and products) does not produce meaningful outputs and is out of the scope of this paper. We also propose a series of ADAMEL variants for different learning scenarios in practice. Our contributions are summarized as follows.\n\n- We formulate the problem of MEL in real-world knowledge integration where the incoming data of unseen data sources are associated with missing values, unseen attributes and different value distributions.\n- We propose a transfer-learning framework that learns the attribute-level importance as the high-level knowledge, and incorporates massive unlabeled data across multiple sources via domain adaptation to make it agnostic and transferable.\n- We apply ADAMEL to multi-source entity linkage over both industrial and public datasets, and show that it achieves at least $5.92 \\%$ improvement in terms of mean average precision compared to the state-of-the-art deep learning EL methods.\n",
        "title": [
            "## 1 INTRODUCTION"
        ],
        "summary": "This section introduces the entity linkage (EL) problem, highlighting its significance in knowledge integration and the challenges posed by limited labeled data and diverse, unlabeled sources. Existing deep learning methods often assume ample labeled data, hindering their effectiveness on new or heterogeneous data sources with missing or novel attributes. To address these issues, the authors propose ADAMEL, a deep transfer learning framework that automatically learns and transfers attribute importance using an attribute-level attention mechanism and domain adaptation, thus enabling robust multi-source entity linkage. Experiments demonstrate that ADAMEL outperforms prior methods, improving mean average precision by at least 5.92% on both industrial and public datasets."
    },
    "Problem Definition": {
        "content": "In this section, we first formally define the problem, and then provide several key notions relevant to our proposed solution. Symbols and notations used in this paper are listed in Table 1.\nAn entity record is collected from a specific data source such as a website or a database, and is identified by its attributes. For example, a song record $r=$ (\"Sweet Caroline\", \"Neil Diamond\", \"USA\") is specified by the attributes $\\mathcal{A}=\\{$ title, artist, country $\\}$. We start with the formal definition of entity linkage.\n\nProblem 1 (EL: Entity Linkage). Given two entity records $r$ and $r^{\\prime}$ associated with the same set of attributes $\\mathcal{A}$ (schema), entity linkage aims to predict if $r$ and $r^{\\prime}$ refers to the same real-world entity.\n\nTable 1: Summary of notation\n\n| Symbol | Definition |\n| :-- | :-- |\n| $\\mathcal{A}=\\left\\{A_{j}\\right\\}$ | a set of pre-defined textual attributes (data source schema) |\n| $r, r[A]$ | an entity record and the value (word tokens) of attribute $A$ |\n| $\\mathcal{D}_{S}, \\mathcal{D}_{T}$ | source and target domain, respectively |\n| $\\left(r, r^{\\prime}\\right)_{S / T}$ | an entity pair in the source and target domain, respectively |\n| $S, S^{\\prime}$ | set of data sources in general |\n| $r^{*}$ | the data source that record $r$ is sampled from |\n| $\\mathcal{D}^{*}$ | set of data sources in a domain, e.g., $\\mathcal{D}_{S}^{*}=\\left\\{r^{*}\\right\\}_{r \\in \\mathcal{D}_{S}}$ |\n| $F$ | the number of relational features, $F=\\mathbb{E} \\mathcal{A} \\mid$ |\n| $\\mathbf{x}, y$ | $H$-dim latent feature vector of an entity pair and its label |\n| $\\mathbf{h}_{j}$ | $D$-dim token embedding of feature $j$ |\n| $f$ | attention embedding function $\\mathbb{R}^{D \\times F} \\rightarrow \\mathbb{R}^{F}$ |\n\nIn this paper, we conduct analysis based on entity pairs $\\left(r, r^{\\prime}\\right)$ instead of individual entity records. We now define the MEL problem, which is related to heterogeneous entity matching ${ }^{1}[11,27]$.\n\nProblem 2 (MEL: Multi-Source Entity Linkage). Given the labeled entity pairs $\\left\\{\\left(r, r^{\\prime}\\right)\\right\\}_{\\text {seen }}$ from a limited set of data sources $\\mathcal{S}$ where each entity record $r$ is associated with attributes $\\mathcal{A}$, and previously unseen pairs $\\left\\{\\left(r, r^{\\prime}\\right)\\right\\}_{\\text {unseen }}$ from the new data sources $\\mathcal{S}^{\\prime}$ with attributes $\\mathcal{A}^{\\prime}$, multi-source entity linkage aims to predict if each pair in $\\left\\{\\left(r, r^{\\prime}\\right)\\right\\}_{\\text {unseen }}$ represents the same real-world entity, where $\\left(r, r^{\\prime}\\right)_{\\text {unseen }} \\in\\left(\\mathcal{S} \\times \\mathcal{S}^{\\prime}\\right) \\cup\\left(\\mathcal{S}^{\\prime} \\times \\mathcal{S}^{\\prime}\\right),\\left|\\mathcal{S}^{\\prime}\\right|>|\\mathcal{S}|$. Since $\\mathcal{S} \\neq \\mathcal{S}^{\\prime}$, certain attributes in $\\mathcal{A}^{\\prime}$ could be missing (C1), new (C2), or associated with values from different distributions (C3), and thus $\\mathcal{A} \\neq \\mathcal{A}^{\\prime}$.\n\nThe key notion in Problem 2 that is different from Problem 1 is that the linkage task is conducted on entity pairs sampled from a wider range of data sources than the labeled data used to train the model (ten or hundred orders of magnitude more in reality). Back to the example shown in Figure 1, while the trained model could make perfect prediction based on \"Artist\" only, it would fail to handle new records because the attribute \"Artist\" has missing or abbreviated values that contain less information. Moreover, the new data sources contain a rarely seen or unseen attribute (\"Gender\"). This issue can be addressed by aligning the union of ontology $\\mathcal{A} \\cup \\mathcal{A}^{\\prime}$ with blank \"dummy\" attributes. Based on our definition, a solution to MEL should be able to (G1) make use of the massive unlabeled data from the new sources, and (G2) further improve the linkage performance by leveraging a few labeled record pairs from the new sources, if available (i.e., an additional support set).\nHere we discuss the necessary terminology of our framework.\nDefinition 3.1 (Source \\& target domain). The source domain $\\mathcal{D}_{S}$ refers to a set of labeled entity pairs $\\left\\{\\left(r, r^{\\prime}\\right)\\right\\}$ sampled from limited data sources that the model is trained on. The target domain $\\mathcal{D}_{T}$ refers to the set of unlabeled pairs where each pair has at least one entity sampled from the data sources unseen in $\\mathcal{D}_{S}$.\n\nFor clarity, we use the superscript * to indicate the data source(s) of a record/domain. Following Definition 3.1, the seen and unseen\n\n[^0]\n[^0]:    ${ }^{1}$ In MEL, the entities come from different data sources, and thus there may be new or missing attributes. On the other hand, in heterogeneous entity matching, the schemas are heterogeneous (i.e., they have different attributes, which may not be aligned) and the entities do not necessarily come from different data sources.\n\n![img-1.jpeg](img-1.jpeg)\n\nFigure 2: Overview. AdaMEL first embeds attributes for records from both the source and target domain to derive the feature representations, and uses the feature attention function to get the attention scores (importance) as the transferable knowledge $\\mathcal{K}$. Then, depending on the availability of the labeled support set, AdaMEL uses $\\mathcal{K}$ and performs either the unsupervised or semi-supervised manner of domain adaptation for MEL.\nset of data sources in Problem 2 are formulated as $\\mathcal{S}=\\mathcal{D}_{S}^{*}$ and $\\mathcal{S}^{\\prime}=\\mathcal{D}_{T}^{*}$. Besides, given a pair in the target domain, it could either contain one entity sampled from the seen data sources in $\\mathcal{D}_{S}^{*}$ and the other one from the unseen, i.e., $\\left(r, r^{\\prime}\\right)_{T} \\in \\mathcal{D}_{S}^{*} \\times \\mathcal{D}_{T}^{*}$, or it has both entities sampled from the completely unseen data sources, i.e., $\\left(r, r^{\\prime}\\right)_{T} \\in \\mathcal{D}_{T}^{*} \\times \\mathcal{D}_{T}^{*}$. In both cases, achieving G1 requires data in $\\mathcal{D}_{T}$. To achieve G2, we introduce the support set.\n\nDefinition 3.2 (Support set). The support set $\\mathcal{S}_{U}$ refers to a small set of labeled entity pairs sampled from the same set of data sources as the target domain $\\mathcal{D}_{T}^{*}$. It has at least one data source that is not contained in $\\mathcal{D}_{S}^{*}$.\n\nThe support set corresponds to the real-world scenario that a few newly incoming entity pairs are well-labeled (e.g., on-the-fly human annotation). Thus, entity pairs in $\\mathcal{D}_{S}, \\mathcal{D}_{T}$, as well as $\\mathcal{S}_{U}$ are all required to achieve G2 for MEL.\n",
        "title": [
            "## 3 PRELIMINARIES",
            "### 3.1 Problem Definition",
            "### 3.2 Terminology"
        ],
        "summary": "This section formally defines the entity linkage (EL) and multi-source entity linkage (MEL) problems, emphasizing the challenges posed by schema heterogeneity and the presence of new or missing attributes in data from unseen sources. It introduces key terminology and notations, detailing the distinction between source and target domains, and highlighting the need for models to leverage both massive unlabeled data and small labeled support sets from new sources to improve linkage performance. The section also describes how the AdaMEL framework addresses these challenges through attribute embedding, feature attention, and domain adaptation techniques for effective MEL."
    },
    "Methodology": {
        "content": "We propose AdaMEL to address Problem 2, a deep framework that learns attribute importance as the transferable knowledge $\\mathcal{K}$ (Section 4.1), and adapt it to multiple data-sources via domain adaptation . ADAMEL first extracts the contrastive relational features of entity pairs to derive the embeddings (Section 4.2). Then, by using the proposed attention embedding function $f$, ADAMEL projects features from $\\mathcal{D}_{S}$ and $\\mathcal{D}_{T}$ into the same attention space (Section 4.3), and jointly learns the feature importance for data sources in both $\\mathcal{D}_{S}^{*}$ and $\\mathcal{D}_{T}^{*}$. This process is conducted in an unsupervised or supervised domain adaptation manner (Section4.4), depending on the real-world scenario. The overview is depicted in Figure 2.\nIn transfer learning, the generic transferable knowledge $\\mathcal{K}$ is key to adapt the model trained on the source domain to the target domain. We denote our domain adaptation solution to MEL as the following binary classification task.\n\n$$\ny=M\\left(\\mathcal{K},\\left(r, r^{\\prime}\\right)\\right) \\in\\{0,1\\}\n$$\n\nwhere $M$ represents the deep model that generates the binary prediction $y$ for the entity pair $\\left(r, r^{\\prime}\\right) \\in \\mathcal{D}_{T}$, where 1 and 0 indicate the matching and non-matching, respectively. As mentioned in\n\nSection 3.1, the key difference between $\\mathcal{D}_{S}$ and $\\mathcal{D}_{T}$ lies in the difference in data sources, therefore $\\mathcal{K}$ should be data-source agnostic to address (C1)-(C3). To ensure $\\mathcal{D}_{T}$ shares the same feature space as $\\mathcal{D}_{S}$ (the prerequisite for domain adaptation), ADAMEL first aligns the ontology so that data sources $\\mathcal{D}_{S}^{*}$ and $\\mathcal{D}_{T}^{*}$ share the same attribute schema, but the attribute values (word tokens) can vary significantly. By doing so, entity records reveal the following properties that correspond to the aforementioned challenges: (C1) entity records in the source/target domain contain missing values, i.e., $r[A]={ }^{\\circ \\circ}$ (empty string) for $r \\in \\mathcal{D}_{S} \\cup \\mathcal{D}_{T}$, (C2) certain attribute values are completely missing for records in $\\mathcal{D}_{S}$, i.e., $r\\left[A_{j}\\right]={ }^{\\circ \\circ}$ for $r \\in \\mathcal{D}_{S}$, but not in $\\mathcal{D}_{T}$, and (C3) rich texts under some attributes in $\\mathcal{D}_{S}$ but sparse in $\\mathcal{D}_{T}$ or vice versa.\nGiven entity pairs $\\left(r, r^{\\prime}\\right)$ with the aligned attributes $\\mathcal{A}$, ADAMEL leverages the attention mechanism to learn the importance of each textual attribute $A \\in \\mathcal{A}$ as the generic knowledge for transfer learning. However, instead of computing the attribute importance directly, ADAMEL parses each attribute $A$ into 2 contrastive relational features, which are word tokens shared by $r$ and $r^{\\prime}$, and word tokens that only appear in one record but not the other. This is because the similarity or uniqueness of attribute between $r$ and $r^{\\prime}$ gives independent and complementary evidence for linkage [37]. Taking the attribute $A=$ \"music version\" as an example, a pair of music recordings sharing the same word (i.e., \"original\" or \"remix\") is not as strong an identifier for matching as it would be for nonmatching if one recording is \"original\" while the other is \"remix\". In addition, looking into both the similarity and uniqueness in attribute $A$ between entities would enrich the feature space and facilitate training the deep model. We describe the 2 contrastive relational features of an attribute $A$ as follows.\n\n$$\n\\begin{cases}\\operatorname{sim}(A) & =\\{w\\} \\text { for } w \\in\\{r[A] \\cap r^{\\prime}[A]\\} \\\\ \\operatorname{uni}(A) & =\\{w\\} \\text { for } w \\in\\{r[A] \\cup r^{\\prime}[A]-r[A] \\cap r^{\\prime}[A]\\}\\end{cases}\n$$\n\nwhere $w$ is the word token in attribute $r[A]$. For clarity, we uniformly denote shared/unique tokens $\\operatorname{sim}(A) / \\operatorname{uni}(A)$ as \"features\" that contribute independently to entity linkage. Clearly, there are $F=2 \\mid \\mathcal{A} \\mid$ features for a pair of entities. To summarize the feature representation, ADAMEL sums up the embeddings of the cropped word tokens $[18,26,35]$ without using more sophisticated operations. The embeddings can be obtained using any pretraining language model, such as BERT [20] or Fasttext [18]. For clarity, we use $i$ as the index of entity pairs and $j$ as the index of features. Thus, the token embedding vector of an entity pair $\\left(r, r^{\\prime}\\right)$ is denoted as:\n\n$$\n\\begin{aligned}\n\\mathbf{h} & =\\left[\\mathbf{h}_{1}, \\mathbf{h}_{2}, \\cdots, \\mathbf{h}_{F}\\right] \\\\\n& =\\left[\\operatorname{emb}\\left(\\operatorname{sim}\\left(A_{j}\\right)\\right), \\operatorname{emb}\\left(\\operatorname{uni}\\left(A_{j}\\right)\\right)\\right] \\text { for } j=1, \\cdots,|\\mathcal{A}|\n\\end{aligned}\n$$\n\nBy doing so, we denote the entity pairs $\\left(r, r^{\\prime}\\right)$ as $F$ textual embedding features $(F=2 \\mid \\mathcal{A})$ ) for matching. The complete process is depicted in Figure 3. Besides, ADAMEL leverages per-feature nonlinear affine transformation to project the word embeddings to get the latent feature $\\mathbf{x}$ :\n\n$$\n\\mathbf{x}=\\left[\\mathbf{x}_{1}, \\mathbf{x}_{2}, \\cdots, \\mathbf{x}_{F}\\right]=\\left[\\sigma\\left(\\mathbf{V}_{j} \\mathbf{h}_{j}+\\mathbf{b}_{j}\\right)\\right] \\text { for } j=1, \\cdots, F\n$$\n\n![img-2.jpeg](img-2.jpeg)\n\nFigure 3: AdaMEL processes 1 attribute $A$ as 2 relational features (i.e., $\\operatorname{sim}(A)$ and $\\operatorname{uni}(A)$ ). In this example, $F=4$ features are generated from $|\\mathcal{A}|=2$ attributes (i.e., \"Title\" and \"Artist\"). The empty word tokens are embedded as the fixed normalized non-zero vector to form $h$ (red dashed box). The feature embedding $x$ is obtained through non-linear affine transformation of $h$ (Equation (4)). Each feature assumes to contribute independently to predict the linkage.\nwhere $\\mathbf{V}_{j}^{H \\times D}$ is the learnable weight matrix, $\\mathbf{b}_{j}^{H}$ is the learnable bias vector, and $\\sigma$ denotes the non-linear activation function (e.g., Relu). With this representation, Equation (1) can be rewritten as: $y=M(\\mathcal{K}, \\mathbf{x}) \\in\\{0,1\\}$. Next we discuss how AdaMEL learns feature importance ${ }^{2}$ as the transferable knowledge $\\mathcal{K}$.\nGiven a pair of entities denoted through $F$ features, ADAMEL defines the energy score of feature $j$ as $e_{j}=a\\left(\\mathbf{W} \\mathbf{x}_{j}\\right)$, where $\\mathbf{x}_{j}$ is the $H$ dimensional representation of latent feature $j, \\mathbf{W}^{H^{\\prime} \\times H}$ is a shared linear transformation, and $a$ represents the attention mechanism $\\mathbb{R}^{H^{\\prime}} \\rightarrow \\mathbb{R}$, as a single-layer neural network (parameterized with a). ADAMEL allows each feature to attend to the label $y$ independently and computes coefficients using the softmax function such that the normalized scores are comparable across all features. Formula in Equation (5) computes the attention score of feature $j$ :\n\n$$\ng\\left(\\mathbf{x}_{j}\\right)=\\operatorname{softmax}_{j}\\left(e_{j}\\right)=\\frac{\\exp \\left(\\mathbf{a}^{T} \\tanh \\left(\\mathbf{W} \\mathbf{x}_{j}\\right)\\right)}{\\sum_{k=1}^{F} \\exp \\left(\\mathbf{a}^{T} \\tanh \\left(\\mathbf{W} \\mathbf{x}_{k}\\right)\\right)}\n$$\n\nNote that Equation (5) only generates the scalar attention score of feature $j$ for an input vector $\\mathbf{x}$. To compute the scores of all features, we introduce the attention embedding function $f$ that learns attention scores of all $F$ features as follows.\n\n$$\nf(\\mathbf{x})=f\\left(\\left[\\mathbf{x}_{1}, \\mathbf{x}_{2}, \\cdots, \\mathbf{x}_{F}\\right]\\right)=\\left[g\\left(\\mathbf{x}_{1}\\right), g\\left(\\mathbf{x}_{2}\\right), \\cdots, g\\left(\\mathbf{x}_{F}\\right)\\right]\n$$\n\nIn Equation (6), all features share the same $\\mathbf{W}$ and a to compute the attention scores. We denote $f(\\mathbf{x})_{j}=g\\left(\\mathbf{x}_{j}\\right)$, and $\\sum_{j=1}^{F} f(\\mathbf{x})_{j}=1$. ADAMEL takes the generated feature importance vector $f(\\mathbf{x})$ as the transferable knowledge $\\mathcal{K}$ for the entity pair $\\left(r, r^{\\prime}\\right)$, i.e., $\\mathcal{K}=f(\\mathbf{x})$.\n\nIn the learning process, ADAMEL feeds the feature representation coupled with its attention score to a 2-layer feed-forward neural network $\\Theta$ to perform the binary classification task:\n\n$$\n\\hat{y}=\\Theta(\\sigma(f(\\mathbf{x}) \\odot \\mathbf{x}))=\\Theta\\left(\\left[\\sigma\\left(g\\left(\\mathbf{x}_{1}\\right) \\cdot \\mathbf{x}_{1}\\right), \\cdots, \\sigma\\left(g\\left(\\mathbf{x}_{F}\\right) \\cdot \\mathbf{x}_{F}\\right)\\right]\\right)\n$$\n\n[^0]![img-3.jpeg](img-3.jpeg)\n\nFigure 4: AdaMEL-base architecture that updates $f$ via labeled data in $\\mathcal{D}_{S}$. AdaMEL-base first computes the attention vector $f\\left(\\mathbf{x}_{i}\\right)$ for the $i$-th entity pair (dashed line), and then compose it with the feature embeddings (solid line) as the input to the neural network $\\Theta$.\nwhere $\\odot$ denotes the element-wise multiplication, $\\sigma$ denotes the non-linear activation (e.g., Relu) and $\\hat{y}$ denotes the inference score for matching. ADAMEL uses the same attention mechanism to handle all records in the training and leverages the cross-entropy loss to update the shared parameters $\\mathbf{W}, \\mathbf{a}$, as well as the learnable $\\mathbf{V}, \\mathbf{b}$ through back-propagation:\n\n$$\nL_{\\text {base }}=-\\frac{1}{N} \\sum_{i=1}^{N} y_{i} \\log \\hat{y}_{i}+\\left(1-y_{i}\\right) \\log \\left(1-\\hat{y}_{i}\\right)\n$$\n\nwhere $y_{i}$ denotes the label $\\{0,1\\}$. To ensure that all learnable parameters can be updated correctly, ADAMEL initializes the missing attribute values (incurred by challenge $\\mathbf{C 1}, \\mathbf{C 2}$ ) with a fixed normalized non-zero vector.\n\nWe name this solution AdaMEL-base as it learns $f$ through the labeled data in $\\mathcal{D}_{S}$ and illustrate the architecture in Figure 4. The attribute importance learned under the supervision of labeled data in $\\mathcal{D}_{S}$ will be carried over to the unseen data sources and may not generalize well as there is always new data from seen or unseen sources with different distributions (C3) in MEL. Next we discuss how ADAMEL adopts $\\mathcal{D}_{T}$ sampled from multiple data sources to alleviate this issue and make $\\mathcal{K}$ data-source agnostic.\nBased on ADAMEL-base, we propose three variants that leverage domain adaptation to handle different learning scenarios.\n4.4.1 Unsupervised Domain Adaptation. Our first idea is to adjust the learned attribute importance according to new distribution of unlabeled data. In Equation (6), the attention embedding function $f$ contains the shared attention mechanism a parameterized by weight vector a and the shared transformation matrix $\\mathbf{W}$. It only takes the feature embeddings $\\mathbf{x}$ as input to compute the attention scores. Since $\\mathbf{W}$ and a are shared across the input data, the attention score vector $f(\\mathbf{x})$ can be seen as projecting the input feature embeddings $\\mathbf{x}$ into a hyper-plane that is parameterized by $\\mathbf{W}$ and a. Without introducing extra information such as entity pair labeling, we can project data from $\\mathcal{D}_{T}$ into the same space as $\\mathcal{D}_{S}$, and it holds as long as the ontology of the unlabeled data aligns with the labeled data, i.e., identical attribute schema between $\\mathcal{D}_{S}$ and $\\mathcal{D}_{T}$.\n\nTherefore, ADAMEL uses the KL divergence to measure the attention score distribution difference between $\\mathcal{D}_{S}$ and $\\mathcal{D}_{T}$ as the\n\n\n[^0]:    ${ }^{2}$ In this paper, we compute the feature attention as the transferable knowledge, feature importance.\n\n![img-4.jpeg](img-4.jpeg)\n\nFigure 5: AdaMEL-ZERO architecture that attempts to align the $i$-th entity pair $f\\left(\\mathbf{x}_{i}\\right)$ in $\\mathcal{D}_{S}$ (solid box) with the averaged $f\\left(\\mathbf{x}^{\\prime}\\right)$ (dashed box) in $\\mathcal{D}_{T} . \\mathbf{x}_{j}$ and $\\mathbf{x}_{j}^{\\prime}(j=1, \\cdots, F)$ denote the $j$-th feature in general from $\\mathcal{D}_{S}$ and $\\mathcal{D}_{T}$, respectively.\nregularization term to train the model. The loss is defined in Equation (9). At a specific iteration in the training, AdaMEL uses the up-to-date $f$ to project data from both $\\mathcal{D}_{S}$ and $\\mathcal{D}_{T}$ into the same feature attention space. Then, ADAMEL updates $\\mathbf{W}$ and a so that both the cross-entropy loss introduced in Equation (8) and the KL divergence between feature attention distributions for $\\mathcal{D}_{S}$ and $\\mathcal{D}_{T}$ are minimized. In this way, feature importance for entity records in $\\mathcal{D}_{S}$ is jointly updated with records sampled from a wider range of data sources in $\\mathcal{D}_{T}$, and thus being agnostic to previously unseen data sources with significantly different value distributions (C3).\n\n$$\nL_{\\text {un }}=(1-\\lambda) L_{\\text {base }}+\\lambda L_{\\text {target }}\n$$\n\nwhere $\\lambda$ is the hyperparameter that balances between $L_{\\text {base }}$ and $L_{\\text {target }}$. $\\lambda$ also measures the amount of adaptation to the target domain $\\mathcal{D}_{T} . L_{\\text {target }}$ is given as follows.\n\n$$\nL_{\\text {target }}=\\operatorname{KL}\\left(f(\\mathbf{x}), \\hat{f}\\left(\\mathbf{x}^{\\prime}\\right)\\right)=\\sum_{i=1}^{\\left|\\mathcal{D}_{S}\\right|} \\sum_{j=1}^{F} \\hat{f}\\left(\\mathbf{x}^{\\prime}\\right)_{j} \\log \\left(\\frac{\\hat{f}\\left(\\mathbf{x}^{\\prime}\\right)_{j}}{\\hat{f}\\left(\\mathbf{x}_{i}\\right)_{j}}\\right)\n$$\n\nwhere $\\hat{f}\\left(\\mathbf{x}^{\\prime}\\right)_{j}=\\frac{1}{\\left|\\mathcal{D}_{T}\\right|} \\sum_{\\mathbf{x}_{i}^{\\prime} \\in \\mathcal{D}_{T}} f\\left(\\mathbf{x}_{i}^{\\prime}\\right)_{j}$, which represents the attention score for feature $j$ averaged over the unlabeled data. $\\mathbf{x}$ and $\\mathbf{x}^{\\prime}$ denote the feature vector in the source and target domain, respectively, and $f\\left(\\mathbf{x}_{i}\\right)_{j}$ denotes the importance of the $j$-th feature in the $i$-th entity pair. In practice, ADAMEL adopts batch learning to improve the training efficiency, i.e., minimizing the loss per batch instead of iterating through all records in the data. The unlabeled data could also come in batches, which makes $\\hat{f}\\left(\\mathbf{x}^{\\prime}\\right)$ be the attention vector averaged over the batched unlabeled data instead of all in the target domain. By default, the batches are sampled randomly.\n\nWe name this solution AdaMEL-ZERO as it is based on unsupervised domain adaption without using any labeled data in $\\mathcal{D}_{T}$ and performs linkage in the zero-shot manner. This model also follows the design pattern in [12]. Figure 5 depicts the architecture and the algorithm is given in Algorithm 1. Line 3-4 project the affine transformation of entity pairs from both $\\mathcal{D}_{S}$ and $\\mathcal{D}_{T}$. Line 5 computes $\\hat{f}\\left(\\mathbf{x}^{\\prime}\\right)$, the attention vector averaged over entity pairs in the target domain. Line 8-10 computes each attention vector in the sampled batch $f\\left(\\mathbf{x}_{i}\\right)$ and adapt it to $\\hat{f}\\left(\\mathbf{x}^{\\prime}\\right)$ to compute the loss $L_{\\text {target }}$. ADAMEL minimizes both the inference loss $L_{\\text {base }}$ and $L_{\\text {target }}$ to train the parameters in $f$ and form the transferable knowledge $\\mathcal{K}=\\hat{f}\\left(\\mathbf{x}_{i}\\right)$ for $\\mathbf{x}_{i} \\in \\mathcal{D}_{T}$ (Line 12). Line 14- 15 denote the inference.\nInput: $\\mathcal{D}_{S}=\\left\\{\\left(\\mathbf{h}_{i}, y_{i}\\right)\\right\\}, \\mathcal{D}_{T}=\\left\\{\\mathbf{h}_{i}\\right\\}, \\lambda$, batch size $B$\nOutput: Predicted $\\hat{y}_{i}$ for $\\mathbf{h}_{i} \\in \\mathcal{D}_{T}$, updated $\\mathbf{a}, \\mathbf{W}$\n1: Initialize $\\mathbf{a}, \\mathbf{W}$ and $\\mathbf{V}, \\mathbf{b}$\n2: loop training epochs\nfor $\\mathbf{h} \\in \\mathcal{D}_{S} \\cup \\mathcal{D}_{T}$ do\n$\\begin{array}{ll}\\text { Form } \\mathbf{x} \\text { with } \\mathbf{V}, \\mathbf{b} & \\triangleright \\text { Eq. (4) } \\\\ \\hat{f}\\left(\\mathbf{x}^{\\prime}\\right) \\leftarrow \\frac{1}{\\left|\\mathcal{D}_{T}\\right|} \\sum_{\\mathbf{x}_{i} \\in \\mathcal{D}_{T}} f\\left(\\mathbf{x}_{i}\\right) & \\triangleright \\text { Initialize loss }\\end{array}$\n$J \\leftarrow 0$\n$\\triangleright$ D\n$\\mathcal{S}_{\\text {batch }} \\leftarrow$ RANDOMSAMPLE $\\left(\\mathcal{D}_{S}, B\\right)$\nfor $(\\mathbf{x}, y) \\in \\mathcal{S}_{\\text {batch }}$ do\n$L_{\\text {un }} \\leftarrow(1-\\lambda) L_{\\text {base }}+\\lambda L_{\\text {target }}$\n$\\triangleright$ Eq. (9)\n$J \\leftarrow J+\\nabla L_{\\text {un }}$\n$\\triangleright$ Update a, W, V, b\nend loop\nForm $\\mathbf{x}, f$ with updated $\\mathbf{a}, \\mathbf{W}, \\mathbf{V}, \\mathbf{b}$\n$\\triangleright$ Eq. (6)\n$\\hat{\\mathbf{y}} \\leftarrow \\emptyset$\nfor $\\mathbf{x}_{i} \\in \\mathcal{D}_{T}$ do\n$\\hat{\\mathbf{y}}_{i} \\leftarrow \\Theta\\left(\\sigma\\left(f\\left(\\mathbf{x}_{i}\\right) \\odot \\mathbf{x}_{i}\\right)\\right)$\nreturn $\\hat{\\mathbf{y}}, \\mathbf{a}, \\mathbf{W}$\n4.4.2 Semi-supervised Domain Adaptation. In practice, a small number of labels may be available for the entity pairs coming from the target domain (e.g., through on-the-fly human annotation). Entity pairs in this support set $\\mathcal{S}_{U}$ are sampled from the wide range of data sources and provide clues about the data characteristics of the target domain. To leverage this set of labeled data (G2), ADAMEL updates the attention embedding function $f$ under the supervision of $\\mathcal{S}_{U}$ so that the projected feature attention vectors of entity pairs in $\\mathcal{D}_{S}$ could match to those in $\\mathcal{S}_{U}$. For this purpose, ADAMEL computes the centroid of the positive entity pairs in $\\mathcal{D}_{S}$ as follows:\n\n$$\n\\mathbf{c}_{\\mathcal{D}_{S}}^{+}=\\frac{1}{\\left|\\mathcal{D}_{S}\\right|} \\sum_{\\left(\\mathbf{x}_{i}^{*}, y_{i}^{*}\\right) \\in \\mathcal{D}_{S}} f\\left(\\mathbf{x}_{i}\\right)\n$$\n\nThe centroid of the negative pairs can be computed in a similar way with negative samples. Intuitively, entity pairs from the data sources unseen in $\\mathcal{D}_{S}^{+}$are more important in adaptation than those from the seen sources, and should be highlighted. ADAMEL measures such difference through the Euclidean-distance between $f(\\mathbf{x})$ and $\\mathbf{c}_{\\mathcal{D}_{S}}$, as the deviating attention vectors are more likely to correspond to unseen data sources in the projected space. In the loss function shown in Equation (12), we compare the distance $d\\left(f(\\mathbf{x}), \\mathbf{c}_{\\mathcal{D}_{S}}\\right)$ with the \"mean distance to cluster centroids\" to give higher weights to entity pairs in $\\mathcal{S}_{U}$ that are deviating from seen data sources.\n$L_{\\text {support }}=\\sum_{y_{i}=1} \\frac{d\\left(f\\left(\\mathbf{x}_{i}^{*}\\right), \\mathbf{c}_{\\mathcal{D}_{S}}^{*}\\right)}{\\overline{d_{\\mathcal{D}_{S}}^{}} \\text { } \\log \\hat{y_{i}}}+\\sum_{y_{i}=0} \\frac{d\\left(f\\left(\\mathbf{x}_{i}^{-}\\right), \\mathbf{c}_{\\mathcal{D}_{S}}^{-}\\right)}{\\overline{d_{\\mathcal{D}_{S}}} \\operatorname{log}\\left(1-\\hat{y_{i}}\\right)}$\nwhere $d$ denotes the Euclidean distance, $\\overline{d^{+/-}}$represents the mean distance for all positive/negative pairs in $\\mathcal{D}_{S}$ to the corresponding centroid. Thus, by integrating $L_{\\text {support }}$ with $L_{\\text {base }}$, the updated loss of ADAMEL in the supervised setting is denoted as follows:\n\n$$\nL_{\\text {sd }}=L_{\\text {base }}+\\phi L_{\\text {support }}\n$$\n\nwhere $\\phi \\in(0,1]$ is a hyperparameter that controls the impact of the labeled support set. The training process updates not only parameters in the neural network $\\Theta$ for better classification performance, but also the attention embedding function $f$ so that the projected\n\nAlgorithm 2 AdaMEL-FEW\nInput: $\\mathcal{D}_{S}=\\left\\{\\left(\\mathbf{h}_{i}, y_{i}\\right)\\right\\}, \\mathcal{S}_{U}=\\left\\{\\left(\\mathbf{h}_{i}, y_{i}\\right)\\right\\}, \\mathcal{D}_{T}=\\left\\{\\mathbf{h}_{i}\\right\\}, \\phi, B$\nOutput: Predicted $\\hat{y}_{i}$ for $\\mathbf{h}_{i} \\in \\mathcal{D}_{T}$, updated $\\mathbf{a}, \\mathbf{W}$\n1: Initialize $\\mathbf{a}, \\mathbf{W}$ and $\\mathbf{V}, \\mathbf{b}$\n2: loop training epochs\n3: for $\\mathbf{h} \\in \\mathcal{D}_{S} \\cup \\mathcal{D}_{T}$ do\n4: Form $\\mathbf{x}$ with $\\mathbf{V}, \\mathbf{b}$\n5: $J \\leftarrow 0$\n6: $\\mathcal{S}_{\\text {batch }} \\leftarrow$ RANDOMSAMPLE $\\left(\\mathcal{D}_{S}, B\\right)$\n7: for $(\\mathbf{x}, y) \\in \\mathcal{S}_{\\text {batch }}$ do\n$J \\leftarrow J+\\nabla L_{\\text {base }}$\n8: Form $f$ with updated $\\mathbf{a}, \\mathbf{W}$\n9: Compute $\\mathcal{D}_{S}^{*}, \\mathcal{D}_{S}^{-}, d_{\\mathcal{D}_{S}}^{*}, d_{\\mathcal{D}_{S}}^{-}$\n$L_{\\text {ssl }} \\leftarrow L_{\\text {base }}+\\phi L_{\\text {support }}$\n$J \\leftarrow J+\\nabla L_{\\text {ssl }}$\n10: end loop\n11: Infer $\\hat{y}$\n12: return $\\hat{y}, \\mathbf{a}, \\mathbf{W}$\n\n* Same as Line 13- 15 of Algorithm 1\npositive and negative feature attentions are matched closer. In this process, feature importance from the new data sources unseen in $\\mathcal{D}_{S}^{*}$ can be incorporated to update the centroids $\\mathbf{c}^{+/-}$in the supervised manner. We name this solution AdaMEL-FEW as it uses a few labeled data in $\\mathcal{D}_{T}$, and depicts the process in Algorithm 2. Particularly, line 7- 8 denote the training process of $f$ to minimize the loss $L_{\\text {base }}$, and line 10- 11 denote the process of further training under the supervision of labeled samples in $\\mathcal{S}_{U}$.\n\n4.4.3 Hybrid Model. We further propose a hybrid model that incorporates both the labeled support set as well as the unlabeled data in the target domain in the training process. It can be seen as the composition of AdaMEL-ZERO and AdaMEL-FEW. The loss function is as follows.\n\n$$\nL_{\\text {hybrid }}=(1-\\lambda) L_{\\text {base }}+\\lambda L_{\\text {target }}+\\phi L_{\\text {support }}\n$$\n\nThis variant uses the loss $L_{\\text {target }}$ defined in Equation (10) and $L_{\\text {support }}$ defined in Equation (12). We name this hybrid solution as AdaMEL-hyR. The algorithm is similar to Algo. 2, the main difference is to incorporate $L_{\\text {un }}$ i.e., Equation (9) into the training process (line 7-8) to learn the parameters simultaneously.\nWe measure the parameter complexity of AdaMEL in terms of the numbers of learnable parameters that comes from three parts: (i) perfeature non-linear affine operations that transform the word token embeddings to the latent feature vectors, (ii) the shared feature attention embedding function $f$, which includes learning $\\mathbf{W}$ and a, and (iii) the multilayer perceptron (MLP) $\\Theta$ with 1 hidden layer for classification. For (i), there are totally $F$ features, each feature is associated with $\\mathbf{V}^{H \\times D}$ and $\\mathbf{b}$, thus leading to $O(F D H)$ learnable parameters. For (ii), as $\\mathbf{W}^{H^{\\prime} \\times H}$ and $\\mathbf{a}^{H^{\\prime}}$ are shared across all features, there are totally $O\\left(H H^{\\prime}\\right)$ parameters. The neural network $\\Theta$ in (iii) takes the concatenated $F H^{\\prime}$-dim features as input with one $H_{\\text {hidden }^{-}}$ dim hidden layer. Therefore, ADAMEL has totally $O\\left(F D H+H H^{\\prime}+\\right.$ $F H^{\\prime} H_{\\text {hidden }}$ ) parameters to learn. We discuss the setup values of $H$, $H^{\\prime}$ and $H_{\\text {hidden }}$ in the configuration of Section 5, and empirically estimate the parameter number in Section 5.6.\n\nTable 2: Data statistics and the train / test splits used in the experiments. The support set is set to $\\left|\\mathcal{S}_{U}\\right|=100$ for all cases.\n\n| Data | $|\\mathcal{A}|$ | $\\left\\|\\mathcal{D}_{S}^{*}\\right\\| /\\left|\\mathcal{D}_{T}^{*}\\right\\|$ | Entity_type | Train $\\left\\|\\mathcal{D}_{S}\\right\\|$ | Test $\\left\\|\\mathcal{D}_{T}\\right\\|$ |\n| :--: | :--: | :--: | :--: | :--: | :--: |\n| Music-3K | 9 | $3 / 7$ | Artist | 374 | 541 |\n|  |  |  | Album | 490 | 509 |\n|  |  |  | Track | 314 | 542 |\n| Music-1M | 9 | $3 / 7$ | Artist | 298566 | 541 |\n|  |  |  | Album | 697739 | 509 |\n| Monitor | 13 | $5 / 24$ | Monitor | 17766 | 1432 |\nDi Jin<br>University of Michigan<br>dijin@umich.edu\n\nBunyamin Sisman<br>Amazon<br>bunyamis@amazon.com<br>Hao Wei<br>Amazon<br>wehao@amazon.com\nDi Jin, Bunyamin Sisman, Hao Wei, Xin Luna Dong, and Danai Koutra. Deep Transfer Learning for Multi-source Entity Linkage via Domain Adaptation. PVLDB, 15(3): 465 - 477, 2022.\ndoi:10.14778/3494124.3494131\nAmazon\nlunadong@amazon.com\n",
        "title": [
            "## 4 PROPOSED FRAMEWORK",
            "### 4.1 Formulation",
            "### 4.2 Feature Representation",
            "### 4.3 Feature Attention Embedding",
            "### 4.4 Domain Adaptation-based Variants",
            "## Algorithm 1 ADAMEL-ZERO",
            "### 4.5 Parameter Complexity",
            "# Deep Transfer Learning for Multi-source Entity Linkage via Domain Adaptation",
            "## PVLDB Reference Format_",
            "## Xin Luna Dong"
        ],
        "summary": "This section introduces AdaMEL, a deep learning framework designed for multi-source entity linkage through domain adaptation. AdaMEL learns transferable attribute importance using an attention-based mechanism on contrastive relational features between entity pairs, aligning source and target domains within a unified feature space. The framework offers three variants\u2014AdaMEL-base (supervised), AdaMEL-ZERO (unsupervised), and AdaMEL-FEW (semi-supervised)\u2014as well as a hybrid model, each leveraging different combinations of labeled and unlabeled data from multiple sources to adaptively update feature importance and improve generalization. The section details the architecture, loss functions, training algorithms, and parameter complexity, establishing AdaMEL's capability to address schema heterogeneity, missing data, and distribution shifts in real-world transfer learning scenarios."
    },
    "Related Work": {
        "content": "Entity Linkage (EL). Early works in EL are mostly based on the similarity between entity attributes $[6,9,13,22]$ through resolving the data conflicts [7], linking relevant attributes through semantic matching or rule reasoning [32]. Techniques such as blocking or hashing are normally applied to merge the candidate entities [4]. The major drawback of these methods is the dependence on prior knowledge as the useful attributes are normally selected through human efforts. Recently, EL models based on deep neural networks [17, 26] have been widely studied due to their capability in automatically deriving latent features and promising results in fields such as CV and NLP [1, 10, 24]. For example, DeepER [17] and DeepMatcher [26] propose to leverage RNN to compose the pre-trained word embeddings of tokens within all attribute values, and use them as features to conduct EL as the binary classification task. CorDel [37] proposes to compare and contrast the pairwise input records before getting the embeddings so that small but critical differences between attributes can be modeled effectively. There are also recent works that formulate entity linkage across different data sources as heterogeneous entity or schema matching [11, 23, 27], for example, EntityMatcher [11] proposes a hierarchical matching network that jointly match entities in the token, attribute, and entity level. Ditto [23] proposes to leverage the pretrained language model [17, 20, 23] such as BERT or DistilBERT, as well as domain knowledge and data augmentation to improve the matching quality. The attention mechanisms $[24,35,36]$ are generally adopted by these deep models, where the goal is to improve the linkage performance by highlighting valuable embeddings, e.g., word embeddings within the attributes. The basis of these above deep models is to accurately summarize the attribute words through advanced NLP\n\ntechniques such as word token-level RNN (with attention) or pretrained language models. On the contrary, AdaMEL focuses on the impact of important attributes in matching and explicitly models their importance as the transferable knowledge. Such attributelevel importance is agnostic to specific data sources and generalizes better than individual words in the transfer learning paradigm.\nTransfer Learning. In the transfer learning scenario, models are trained on a source domain and applied to a related target domain to handle the same or a different task [14, 29]. The specific transferable knowledge that bridges the source and target domain has significant impact to model performance [39]. A popular approach is to adapt the pre-trained model for the new task through fine-tuning [20], or by adding new functions to specific tasks such as object detection [15]. In terms of EL, TLER [34] is a non-deep method that reuses and adopts seen data from the source domain to train models for the new domain. Auto-EM [40] proposes to pre-train models for both attribute-type (i.e., schema) and attribute value matching based on word- and character-level similarity. It assumes the typed entities are from a single data source and the attributes are seen during training, and thus cannot handle the multi-source scenario with unseen attributes. A specific type of transductive transfer learning that is most relevant to our work is known as Domain Adaptation, where the source and target domain share the same feature space with different distributions [33], and models are trained on the same task [38]. Many well-designed algorithms propose to map the original feature spaces to a shared latent feature space between domains [3, 8]. DeepMatcher+ [19] extends DeepMatcher by combining transfer learning and active learning to achieve comparable performance with fewer samples. This work mainly focuses on dataset adaptation rather than attribute matching. Another line of works proposes to pre-train models on the source and target domain (if labeling available) and then combine them through specific weighting schemes [31]. The process of applying the trained model to handle previously unseen data is also known as zero-shot learning [28]. Unlike the above approaches, ADaMEL explicitly learns feature importance by adapting to the massive unlabeled data from unseen sources as the transferable knowledge for the multi-source EL task.\n",
        "title": [
            "## 2 RELATED WORK"
        ],
        "summary": "This section reviews methods for entity linkage (EL), contrasting traditional attribute similarity and rule-based techniques\u2014which rely heavily on manual feature selection\u2014with recent deep learning approaches that leverage neural networks, pretrained language models, and attention mechanisms to automatically derive effective features. It highlights how current deep models focus on summarizing attribute words, whereas the proposed AdaMEL framework instead models attribute-level importance as transferable knowledge, enhancing generalization across data sources. The discussion then shifts to transfer learning, outlining strategies such as fine-tuning, domain adaptation, and zero-shot learning, and points out the limitations of existing methods in handling unseen attributes or multi-source scenarios. AdaMEL is distinguished by its ability to adaptively learn feature importance from large amounts of unlabeled data, enabling robust multi-source entity linkage."
    },
    "Experiment": {
        "content": "To evaluate the properties and performance of AdaMEL, we aim to answer the following research questions: (Q1) Does AdaMEL effectively handle MEL with the data challenges (C1-C3) under the transfer learning paradigm? (Q2) How well does AdaMEL adapt feature importance in the target domain and how does it affect the linkage results? (Q3) Are generated feature attention values meaningful? (Q4) How stable is AdaMEL in handling different data sources? (Q5) How does the size of support set $\\mathcal{S}_{U}$ impact the performance of AdaMEL? We conclude with the model justification (ablation study and limitation). The supplementary material and the code for splitting the public data are given in the repository ${ }^{3}$.\nData In accordance with (Q1-Q5), we use both the public benchmark dataset from the Data Integration to Knowledge Graphs (DI2KG) challenge [5] and two real-world datasets in different scales from an online-sales company. Both datasets are in the tabular form and the entities are associated with descriptive textual features. The data statistics, source description and splittings are given in Table 2. Specifically, Musci-1M shares the same testing set as Musci-3K. Monitor adopts all positive and randomly selected 1000 negative pairs to form the testing set (see repository for details).\n\n- Music-1M is a weakly labeled corpus crawled from 7 public music websites. We name them website 1-7 for confidentiality. There are 2 entity types: artists and albums. Entity pairs are labeled following the hyperlinks in pages, so there might be mixed-type errors such as matching an artist to her album.\n- Music-3K is a manually labeled corpus containing the same data sources as Music-1M. It has three types: artist, album and tracks. The manual annotation is based on 9 attributes such as the artist name and album title. Errors such as mixedtype matching are carefully corrected.\n- Monitor contains monitor data from 24 sales websites such as ebay.com and shopmania.com. We filter out attributes with $>60 \\%$ empty records, and get totally 13 attributes such as product description, manufacturer, condition status, etc.\nComparing with the public benchmark datasets [26], the above datasets are collected from wider ranges of real-world data sources with heterogeneous schemas, and their attribute values are generally longer with more diverse characters. For example, for Music-3K, artist type, the averaged attribute length is 25.75 word tokens, and for Monitor, the averaged attribute length is 11.73 word tokens. On the contrary, this number is 6.26 and 5.21 word tokens for the benchmark \"dirty\" and \"heterogeneous\" Walmart-Amazon dataset [11],\n\n[^0]\n[^0]:    ${ }^{3}$ https://github.com/DerekDijin/AdaMEL-supplementary\n\n![img-5.jpeg](img-5.jpeg)\n\nFigure 6: The challenges of missing values (C1) and new attributes (C2) between $\\mathcal{D}_{S}$ and $\\mathcal{D}_{T}$ for Monitor: per attribute, we give the percentage of entity pairs without missing values. For 11 out of 13 attributes (excluding 'page title' and 'source'), the majority of entity pairs have at least 1 entity with missing values. Among these 11 attributes, 5 have non-missing entity pairs mostly in the target domain, and 6 of them have significantly different percentages between $\\mathcal{D}_{S}$ and $\\mathcal{D}_{T}$.\nrespectively. In terms of the Music datasets, as the music works come from different countries, many entities are recorded with nonEnglish characters \\& phrases for attributes such as the title, album and artist names. Unlike Music-1M that labels entity pairs through website hyperlinks, Music-3K further inspects whether the pair of music works indicate the same physical copy (i.e., \"Album\"), or the same digital copy in formats such as remix or cover (i.e., \"Track\").\n\nThe Monitor dataset is highly imbalanced with more than $99 \\%$ unmatched entity pairs. To illustrate the data challenges, we show the difference between the source and target domains in terms of missing attribute values (C1) and new attributes (C2). As the attributes are associated with entity pairs, in Figure 6 we plot the percentage of pairs without missing values per attribute, i.e., $\\left(r[A], r^{\\prime}[A]\\right)$ where $r[A] \\neq \\emptyset, r^{\\prime}[A] \\neq \\emptyset$ for $A \\in \\mathcal{A}$ in both the source and target domain. Ideally, all the percentage bars should be close to 1 for data in both domains, which would indicate few missing values and not significantly different attributes (we observe this pattern in the benchmark datasets [26]). For the Monitor dataset, however, the pattern is different. We observe that only two attributes (i.e., \"page_title\" and \"source\") are close-to-1, while for the remaining 11 attributes, less than $50 \\%$ entity pairs have complete attribute values. Such data sparsity reflects challenge (C1). In addition, we observe that the percentages of pairs without missing values are significantly different for the source domain and the target domain. Particularly, we find that 5 out of 13 attributes have non-missing entity pairs only in the target domain, which can be seen as new attributes (C2). We illustrate the different attribute value distributions (C3) in the repository.\nBaselines. The following baselines are used in this work.\n\n- TLER [34] is a non-deep transfer learning framework that defines a standard feature space and reuses the seen data to train models for the new domain.\n- DeepMatcher [26] is a deep learning framework that consists of 3 modules: attribute embedding, attribute similarity\nrepresentation, and classification. The public implementation uses Fasttext to embed attribute words and uses attentative RNN to summarize attributes. We report results using the best-performing variant, DeepMatcher-hybrid.\n- EntityMatcher [11] is a hierarchical deep framework for heterogeneous schema matching. It jointly matches entities at the level of token, attribute, and entity. The token-level matching strategy allows EntityMatcher to perform crossattribute alignment. Fasttext is used to embed word tokens.\n- Ditto [23] uses fine-tuned and pre-trained Transformerbased language models (i.e., BERT, DistilBERT) with optimization including domain knowledge injection, text summarization, and data augmentation with difficult samples.\n- CorDel [37] adopts an alternative deep model to the widelyused \"twin architecture\". It compares and contrasts word tokens to filter out minor deviations between attribute values before embedding using Fasttext. Out of the variants, CorDelAttention is reported to perform the best on dirty EL datasets.\nWe consider these baselines since they are reported to achieve state-of-the-art EL performance and outperform methods such as Seq2SeqMatcher [27] and DeepMatcher+ [19]. Most of them are particularly proposed to handle heterogeneous entity linkage.\nConfiguration. In our experiments, we follow the original paper and fine-tune the baselines for optimal performance. For DeepMatcher, we use its hybrid variant (bi-directional RNN with attention) to summarize attributes with 2-layer highway neural network ((hidden dim $=300)$ ). The training epoches is set to 40 with batch size $=16$. For EntityMatcher, we use the full matching model that uses bi-GRU (hidden size $=150$ ) to embed attribute word sequences with cross-attribute token-level alignment. The training epoch is set to 20 with batch size $=16$. For CorDel, we use the attention-based variant that learns the word importance within the same attribute to validate the effectiveness of our attribute-level attention module. Moreover, CorDel-Attention was shown to perform best on long textual attribute values, which matches the property of our input data. All these 3 baselines use the pretrained FastText [18] to derive the 300-dimensional embeddings for word tokens in each attribute. We set the cropping size $=20$ and sum the embeddings of word tokens as the feature embeddings for CorDel. The training epoch is set to 20 with learning rate $=10^{-4}$ and batch size $=16$. For Ditto, we tested its optimization strategies and adopted the \"token span deletion\" for data augmentation, \"general\" domain knowledge and retaining high TF-IDF tokens to summarize the input sequences. We also tested all pretrained language models, i.e., bert, distilbert, and albert, and ended up using bert. The training epoch is set to 40 with batch size $=64$ and learning rate $=3 \\times 10^{-5}$.\n\nTo evaluate the effectiveness of our proposed framework, we configure AdaMEL with consistent setup as the baselines. Specifically, we use the 300-dim Fasttext to embed word tokens for fairness because 3 of the 4 baselines also use it, even though AdaMEL supports any word embedding techniques such as Bert embedding [20]. We set the cropping size $=20$ as CorDel. The hyparameters of AdaMEL are given as follows: the dimension of the projected embeddings per feature is $H=64$, the dimension of the hidden layer in $f$ is $H^{\\prime}=256$, and the dimension of the hidden layer in $\\Theta$ is $H_{\\text {hidden }}=256$. The activation $\\sigma$ is set to be Relu. We set $\\lambda=0.98$ and $\\phi=1.0$ in\n\n![img-6.jpeg](img-6.jpeg)\n\nFigure 7: MEL performance (PRAUC) comparison between AdaMEL variants and baselines. AdaMEL variants outperform baseline heterogeneous entity matching methods in almost all cases. Particularly, AdaMEL-HYb performs the best on all entity types and datasets.\n![img-7.jpeg](img-7.jpeg)\n\nFigure 8: MEL performance on Music-1M. Same legend as in Fig. 7a.\nEquation (9), (13) and (14) for AdaMEL variants unless otherwise addressed. To train the AdaMEL, we use Adam optimizer [21] for 100 epoches with learning rate $=10^{-4}$ and batch size $=16$. We conduct all experiments 3 times and report the mean and std. We run these experiments on the Linux platform with 2.5 GHz Intel Core i7, 256GB memory and 8 NVIDIA K80 GPUs.\nEvaluation Metric. We evaluate the model performance using PRAUC as it measures the precision-recall relation globally and handles data imbalance. We use the python Sklearn library to compute PRAUC based on the open-source implementation of all baselines.\nOur first experiment is to verify the effectiveness of AdaMEL variants on the task of MEL (Q1). We simulate two real-world scenarios: (S1) data in the target domain $\\left(\\mathcal{D}_{T}\\right)$ shares common data sources with the source domain $\\left(\\mathcal{D}_{S}\\right)\\left(\\right.$ i.e., $\\left.\\left(r, r^{\\prime}\\right)_{T} \\in \\mathcal{D}_{S}^{*} \\times \\mathcal{D}_{T}^{*}\\right)$, and (S2) data sources in the target domain are disjointed from the source domain (i.e., $\\left.\\left(r, r^{\\prime}\\right)_{T} \\in \\mathcal{D}_{T}^{*} \\times \\mathcal{D}_{T}^{*}\\right)$.\nSetup. For the Music data, we use three data sources (i.e., $\\mathcal{D}_{S}^{*}=$ \\{website 1, website 2, website 3$\\}$ ) to train our model and test on all 7 sources (overlapping scenario $\\mathbf{S 1}$ ) or only the 4 remaining sources (disjoint scenario $\\mathbf{S 2}$ ) as the target domain $\\mathcal{D}_{T}$. In either scenario, we collect 100 samples ( 50 positive and 50 negative) from the corresponding $\\mathcal{D}_{T}$ as support set $\\mathcal{S}_{U}$. For the public Monitor data, we use entity pairs from 5 sources (i.e., $\\mathcal{D}_{S}^{*}=$ \\{ebay.com, catalog.com, best-deal-items.com, cleverboxes.com, ca.pcpartpicker.com\\}) to train the models. We use data in all 24 sources as $\\mathcal{D}_{T}$ for $\\mathbf{S 1}$, and the rest 19 data sources for $\\mathbf{S 2}$, respectively. 100 samples are collected as $\\mathcal{S}_{U}$ in the same way as Music. We also randomly picked different sources to form $\\mathcal{D}_{S}$ and $\\mathcal{D}_{T}$ to eliminate the randomness, and found similar patterns in the results.\n\nResults. We report the results in Figure 7 and Figure 8. Our first observation is that all AdaMEL variants tend to outperform the baselines and our base model without adaptation, AdaMEL-base. The heterogeneous entity matching baselines do not perform well on these datasets under the supervision of labeled data only. This is likely because of the long and noisy word sequences in the data and the difference in attribute value distribution across data sources that is unseen during model training. AdaMEL highlights the impact of important features, and only represent the sequences by summing the token embeddings. This confirms our conjecture that learning the attribute-level attention as the transferable knowledge is more effective in handling the MEL task than refining the word-level sequence representation. Also, we observe that out of all variants, AdaMEL-HYb performs the best in all cases with $0.64 \\% \\sim 5.50 \\%$ improvement in PRAUC than the second-best (ADAMEL-ZERO in most cases), which demonstrates its effectiveness in integrating information from both the labeled support set $\\mathcal{S}_{U}$ and the unlabeled target domain $\\mathcal{D}_{T}$. AdaMEL-ZERO performs better than AdaMEL-FEW on the \"Artist\" and \"Album\" type, while AdaMEL-FEW performs better on the \"Track\" type. This is likely due to the fact that the track records are more diverse than the other types as the digital-format music tracks can be remixed or covered by other artists. Thus, the high-quality labeled samples from $\\mathcal{S}_{U}$ is of higher value. On the contrary, since the records are more consistent for \"Artist\" and \"Album\", incorporating more records in $\\mathcal{D}_{T}$ leads to higher improvement in MEL performance. Note Figure 8 shows that AdaMEL-FEW performs slightly worse than AdaMEL-base because the labeled samples from $\\mathcal{S}_{U}$ only overfits to the trained model on the source domain, that deviates the actual feature importance for the massive unlabeled samples in $\\mathcal{D}_{T}$. To summarize, the improvement of AdaMEL-ZERO, -FEW and -HYB over the baselines indicates the effectiveness of domain adaptation in incorporating data in $\\mathcal{D}_{T}$.\n\nOverall, ADAMEL variants achieve better performance on the overlapping scenario (S1) than the disjoint scenario (S2). This is as expected as the disjoint scenario represents an extreme case where data sources in $\\mathcal{D}_{T}$ are less likely or even entirely not used in training the model if the support set is unavailable. Besides, the performance of all approaches running on Music-1M is lower than Music-3K. The main reason is that the data is weakly labeled as it simply follows the hyperlinks from the websites, and does not distinguish the actual media of the music work (i.e., the physical or digital copy). As the models are tested on the same well-labeled\n\n![img-8.jpeg](img-8.jpeg)\n(a) No adaptation $(\\lambda=0)$\n![img-9.jpeg](img-9.jpeg)\n(b) With adaptation $(\\lambda=0.98)$\n\nFigure 9: Source and target domain feature attention vectors are better aligned with high value of $\\lambda$ for AdaMEL-hyb (visualized with TSNE, dim-2). Plots for AdaMEL-ZERO are given in the repository.\nset, training on Music-1M could be vulnerable to cases such as mixed-type matching. Nevertheless, we observe that AdaMEL still achieves promising results in both hard cases of transfer learning for MEL, i.e., unseen data sources in the target domain and training on weakly labeled data, which further demonstrates the advantage of AdaMEL. The experimental results on Monitor are shown in Figure 7b. Similarly, AdaMEL variants tend to outperform the baselines and AdaMEL-hyb performs the best with at least $0.51 \\%$ improvement in PRAUC over the second best, EntityMatcher. On average, AdaMEL-hyb outperforms the baseline by $9.39 \\%$ improvement in the overlapping scenario and $11.55 \\%$ improvement in the disjoint scenario. These results also validates our findings above.\nAdaptation is the key component to our proposed method. To evaluate how well AdaMEL learns feature importance adapted to the target domain (Q2), we study the effectiveness of $\\lambda$ adopted in AdaMEL-ZERO and AdaMEL-HYB as it controls the weight of adapting to unlabeled data in training (larger $\\lambda$ leads to more adaptation). Setup. We run both variants of AdaMEL on the Music-3K dataset and report the performance on MEL. As discussed in Section 4.4.1, records from both the source and target domains are projected into the same space using the shared attention embedding function, and AdaMEL attempts to adapt the model to match these feature importance distribution. Intuitively, with sufficient adaptation, feature importance vectors from both domains should align well, and further benefit the linkage task. To validate this conjecture, we visualize the learned feature attention vectors using AdaMEL-ZERO and AdaMEL-Hyb with different values of $\\lambda$ by projecting them into 2-dimensional space using TSNE [25]. We also study the linkage performance of AdaMEL-ZERO and AdaMEL-HYB with different $\\lambda$ values on the \"artist\" and \"album\" type of the Music-3K dataset.\nResults. In Figure 9, we observe that for AdaMEL-HYB, the feature attention vectors from $\\mathcal{D}_{S}$ and $\\mathcal{D}_{T}$ align better when $\\lambda$ is 0.98 than 0 , which confirms the effectiveness of adaptation. Also, for the same $\\lambda$ values, AdaMEL-Hyb (Figure 9b) generates better adapted results than AdaMEL-ZERO as the projected records from $\\mathcal{D}_{S}$ and $\\mathcal{D}_{T}$ are almost indistinguishable; this is expected since the labeled support set is used. We give the plots for AdaMEL-ZERO in the repository.\n\nTo evaluate the impact of adaptation to the linkage results, in Figure 10 we show the performance of our variants with different $\\lambda$ values. We observe that as $\\lambda$ approaches (but not equals)\n![img-10.jpeg](img-10.jpeg)\n(a) AdaMEL performance change on Music-3K, artist type\n![img-11.jpeg](img-11.jpeg)\n(b) AdaMEL performance change on Music-3K, album type\n\nFigure 10: AdaMEL-ZERO and AdaMEL-HYb performance improve with increasing $\\lambda$ from 0 to 0.98 (fitted with linear regression). The performance drops when $\\lambda=1$ as no labeled data in $\\mathcal{D}_{S}$ is used.\nto 1 , the general performance in terms of PRAUC improves for both AdaMEL-ZERO ( 0.8014 - 0.9091) and AdaMEL-HYB ( 0.8242 - 0.9201), which again demonstrates the effectiveness of adaptation. It is worth noting that when $\\lambda=1$, both AdaMEL-ZERO and AdaMEL-HYb perform worse without giving meaningful results. This is because at this point, AdaMEL-ZERO is trained without supervision of the labeling in $\\mathcal{D}_{S}$, and the only term in the loss function is the regularization. AdaMEL-HYb is better as labeling in $\\mathcal{S}_{\\mathrm{U}}$ is still used, but the overall performance deteriorates due to the lack of labeling from $\\mathcal{D}_{S}$. As a result, the parameters trained (i.e., $\\mathbf{a}, \\mathbf{W}$ ) would tend to only \"match\" the feature distribution between $\\mathcal{D}_{S}$ and $\\mathcal{D}_{T}$ that are not tailored to classification.\nSetup. To testify whether AdaMEL learns meaningful feature attention values (Q3), we showcase the learned feature importance through the attention scores produced by AdaMEL on two datasets: Music-3K and Monitor. We only report the artist type and omit the other two types for brevity. AdaMEL-Hyb is configured with the best performance $(\\lambda=0.98, \\phi=1.0)$ in the previous experiments.\nResult. For the Monitor dataset in Table 3, we observe the long \"tail distribution\" of feature importance, i.e., the most important feature is \"Page_title_shared\" with significantly high scores, while the other features are with roughly the same low scores. On the other hand, we observe the more uniform distribution for the artist type in Music-3K dataset, which makes sense as all top features are related to the artist names. The learned attention scores on both datasets imply that the task of MEL could be addressed with some of the most remarkable features (importance inequality).\n\nWe further run AdaMEL-HYb on these selected important features only and compare the performance with the result using the other features, as well as all the features. For Monitor, we use 3 attributes (i.e., \"Page_title\", \"Source\" and \"Manufacturer\"). For the\n\nTable 3: AdaMEL learned importance of top-5 features for Monitor and Music-3K, artist type.\n\n| Monitor |  | Music-3K, artist |  |\n| :-- | :-- | :-- | :-- |\n| Feature | Score | Feature | Score |\n| Page_title_shared | 0.1635 | Main_performer_shared | 0.0739 |\n| Page_title_unique | 0.0595 | Name_unique | 0.0697 |\n| Source_shared | 0.0535 | Name_shared | 0.0628 |\n| Manufacturer_unique | 0.0473 | Source_unique | 0.0597 |\n| Manufacturer_shared | 0.0416 | Name_Native_Language_shared | 0.0583 |\n\nartist type of Music-3K, we use the 3 name-related attributes (i.e., \"Main_performer\", \"Name\", Name_Native_Language), and \"Source\". Similarly for the other two types, we use their corresponding top important attributes, and report the results in Table 4. We observe that by using the selected important features only, AdaMEL achieves comparable and even slightly better performance than using all features with $2.21 \\%, 0.87 \\%$ and $2.92 \\%$ improvement in PRAUC on Monitor, Music-3K (artist) and Music-3K (album), respectively. For Music-3K (track), using the top attributes only performs slightly worse than using all attributes, which is likely due to the diversity of track records. Nevertheless, these experimental results show that model training can further benefit from feature importance as using all the possible attributes could introduce irrelevant or noisy input to the model (e.g., using album-related features when inferring the artist type). Also, they shows the effectiveness of the attention module of AdaMEL in learning reasonable feature importance.\n\nTable 4: Performance (PRAUC) comparison using the selected important features vs. the other features and all features. Numbers in the parenthesis denote the counts of features.\n\n| Dataset | Top Attributes ( $\\#$ ) | Other Attributes ( $\\#$ ) | All Attributes ( $\\#$ ) |\n| :-- | :--: | :--: | :--: |\n| Monitor | $\\mathbf{0 . 9 4 7 9} \\pm \\mathbf{0 . 0 0 0 7}(3)$ | $0.4276 \\pm 0.0015(10)$ | $0.9258 \\pm 0.0025(13)$ |\n| Music-3K, artist | $\\mathbf{0 . 9 2 9 8} \\pm \\mathbf{0 . 0 0 3 6}(4)$ | $0.7966 \\pm 0.0005(5)$ | $0.9211 \\pm 0.0040(9)$ |\n| Music-3K, album | $\\mathbf{0 . 8 1 2 5} \\pm \\mathbf{0 . 0 0 1 1}(4)$ | $0.4692 \\pm 0.0009(5)$ | $0.7833 \\pm 0.0031(9)$ |\n| Music-3K, track | $0.8398 \\pm 0.0004(3)$ | $0.7026 \\pm 0.0006(6)$ | $\\mathbf{0 . 8 4 5 4} \\pm \\mathbf{0 . 0 0 4 0}(9)$ |\nWe now simulate the real-world knowledge integration, where new data sources often arrive one by one incrementally (e.g., in batches from neighboring data sources), and show the stability of AdaMEL in handling the various data sources under this scenario (Q4) .\nSetup. We use the public Monitor dataset and compare AdaMELHYB with the optimal configuration $(\\lambda=0.98, \\phi=1.0$ as shown in Section 5.2 and Section 5.3) with the best-performing baseline approach, EntityMatcher, and the fastest baseline approach, CorDel-Attention. In this experiment, we use 1500 entity pairs from the same 5 data sources as mentioned in Section 5.2 to train the models (i.e., $\\mathcal{D}_{S}^{*}=\\{$ ebay.com, catalog.com, best-deal-items.com, cleverboxes.com, pcpartpicker.com\\}). To test the performance on MEL, we first randomly select 200 entity pairs from each of 7 data sources (the same 5 data sources as $\\mathcal{D}_{S}^{*}$ and 2 unseen ones, i.e., $\\mathcal{D}_{T}^{*}=\\mathcal{D}_{S}^{*} \\cup\\{$ yikus.com, getprice.com $\\}$ ) and form totally 1400 pairs to create the target domain. Then, we incrementally add up to 200 entity pairs from 2 new sources $\\left(\\Delta \\mathcal{D}_{T}^{*}\\right)$ to $\\mathcal{D}_{T}^{*}$, such that $\\mathcal{D}_{T}^{*}=\\mathcal{D}_{T}^{*} \\cup \\Delta \\mathcal{D}_{T}^{*}$. Each of the newly added pairs $\\left\\{\\left(r, r^{\\prime}\\right)\\right\\}$ contains at least one record from $\\Delta \\mathcal{D}_{T}$ to ensure new data sources are introduced to the target domain. As AdaMEL-HYB requires a small set of labeled entity pairs from $\\mathcal{D}_{T}^{*}$, we randomly select 100 labeled samples from all data sources $\\left(\\mathcal{D}_{S}^{*} \\cup \\mathcal{D}_{T}^{*}\\right)$. This small set simulates the on-the-fly manual labeling in the real-world, and we fix it throughout each run of the experiment to ensure the impact of $\\mathcal{S}_{U}$ is consistent. We also record the average runtime over all runs as an empirical study of the model efficiency.\nResults. We report the performance of AdaMEL-HYB and the two baselines on MEL in Figure 11, as well as their empirical runtime.\n![img-12.jpeg](img-12.jpeg)\n\nFigure 11: AdaMEL-HYB performs more stably ( $0.9750 \\sim 0.9219$ in PRAUC) as $\\#$ data sources increases in $\\mathcal{D}_{T}$ with less runtime.\n\nAs shown in the figure, AdaMEL-HYB is more stable than both EntityMatcher and CorDel-Attention with significantly higher performance in handling the incrementally incoming data sources. This is due to the fact that AdaMEL-HYB continuously updates parameters in the attention embedding function $f$ to adapt to new data sources in $\\mathcal{D}_{T}$. Comparing with CorDel-Attention, EntityMatcher performs better and could occasionally compete with AdaMELHYB under some scenarios $\\left(\\left|\\mathcal{D}_{T}^{*}\\right|=17,21\\right)$, but it is not stable as the performance fluctuates. Moreover, based on the runtime table, AdaMEL-HYB takes much less time to train than CorDel-Attention and EntityMatcher. The empirical runtime comparison corresponds to our analysis in Section 4.5 as AdaMEL-HYB does not require sophisticated operations on word-level embeddings and thus having relatively less parameters to train. In practice, the number of parameters to train for AdaMEL-HYB is $\\sim 2219520$, which is much less than the number given by EntityMatcher: $\\sim 123119104$. These findings demonstrate the capability of AdaMEL in consistently handing MEL with a variety of incoming data sources, while being more robust. In addition, they strengthen our claim that finding important features as the transferable knowledge in MEL could benefit the model performance with reduced computational complexity.\nSetup. To better understand the effectiveness of the labeled support set (Q5), we perform the sensitivity analysis with incrementally increasing numbers of labeled samples in the support set $\\mathcal{S}_{U}$. Following Section 5.2, we randomly select 200 additional samples from $\\mathcal{D}_{T}$ of the public Monitor dataset and create the support set with totally 300 labeled samples. We run two AdaMEL variants that leverage the support set, AdaMEL-FEW $(\\phi=1.0)$ and AdaMEL-HYB $(\\lambda=0.98, \\phi=1.0)$ in this experiment with $\\left|\\mathcal{S}_{U}\\right|$ ranging from 1 to 300 with step size $=20$ (specifically, we \"zoom in\" the smaller values and have $\\left|\\mathcal{S}_{U}\\right|=\\{1,5,10,20,40,60, \\cdots, 300\\}$ ). In each run, the samples in $\\mathcal{S}_{U}$ are randomly selected.\nResult. The experimental result is shown in Figure 12. Our first observation is that at the initial stage of the experiment, the performance of both AdaMEL-FEW and AdaMEL-HYB improves as the number of used labeled samples from $\\mathcal{S}_{U}$ increases. Particularly, we observe $\\sim 1 \\%$ performance improvement from $\\left|\\mathcal{S}_{U}\\right|=1$ to $\\left|\\mathcal{S}_{U}\\right|=140$ for AdaMEL-FEW and $2 \\% \\sim 3 \\%$ improvement for AdaMEL-Hyb. This overall performance improvement is as expected since an increasing amounts of labeled samples from $\\mathcal{D}_{T}$ are used to supervise the learning process. In the late stage $\\left(\\left|\\mathcal{S}_{U}\\right|>140\\right)$, we observe that the performance fluctuates in each run and the\n\n![img-13.jpeg](img-13.jpeg)\n\nFigure 12: Sensitivity analysis of the size of support set $\\left|\\mathcal{S}_{U}\\right|$ fitted with order-2 polynomial regression on AdaMEL-FEW and AdaMELHYB. As more labeled samples are included in $\\mathcal{S}_{U}$, the model performance (PRAUC) increases initially and then flattens out.\noverall performance saturates. This indicates that the feature importance learned by AdaMEL has sufficiently adapted and does not significantly change as more labeled data are collected in $\\mathcal{S}_{U}$. Moreover, comparing with AdaMEL-FEW, ADAMEL-HYB performs similarly when the size of support set is small $\\left(\\left|\\mathcal{S}_{U}\\right| \\leq 60\\right)$, and it consistently outperforms when $\\left|\\mathcal{S}_{U}\\right|>60$. This is likely due to the bias of feature importance brought by particular labeled samples selected when $\\left|\\mathcal{S}_{U}\\right|$ is small. When $\\mathcal{S}_{U}$ contains more samples, the learned feature importance becomes stable and sufficiently adapted to $\\mathcal{S}_{U}$, and the outperformance given by AdaMEL-HYB over AdaMEL-FEW comes from the unlabeled samples from $\\mathcal{D}_{T}$. As a rule of thumb, Figure 12 indicates that a small support set with $\\left|\\mathcal{S}_{U}\\right|=100 \\sim 200$ labeled samples from $\\mathcal{D}_{T}$ is beneficial to learn feature importance and to improve the MEL performance of AdaMEL. Too few samples would incur bias to the trained model, while too many samples would be expensive to obtain in practice, and does not necessarily help improve the model.\nAblation Study. We perform the ablation study of AdaMEL that uses the shared and unique contrastive features, as well as using both of them as the default setting. Table 5 shows that including the shared and unique attribute values capture different perspectives of the data and thus enriches the feature space. Including both achieves the highest performance with $0.41 \\%-6.72 \\%$ improvement over using one feature.\nPerformance on Single Domain. Here we compare AdaMELzero and -HYB with DeepMatcher on the benchmark datasets to justify their performance on well-labeled data from the same seen domain without the 3 challenges ( $\\mathbf{C 1}$ - C3). From Table 6, we observe that AdaMEL-ZERO does not perform as well as DeepMatcher on these benchmark datasets of one single domain. This shows the limitation of AdaMEL in handling data with no missing values\n\nTable 5: Ablation study: AdaMEL contrastive features on Music-3K, artist and album type. AdaMEL-ZERO and -FEw perform similarly.\n\n| Dataset | Method | Shared | Unique | Shared \\& Unique |\n| :-- | :-- | :-- | :-- | :-- |\n| Music-3K, <br> artist | AdaMEL-base <br> AdaMEL-HYB | $0.7868 \\pm 0.0045$ <br> $0.8559 \\pm 0.0026$ | $0.7170 \\pm 0.0132$ <br> $0.8069 \\pm 0.0112$ | $\\mathbf{0 . 8 5 4 5} \\pm \\mathbf{0 . 0 1 4 3}$ <br> $\\mathbf{0 . 9 2 1 1} \\pm \\mathbf{0 . 0 0 4 0}$ |\n| Music-3K, <br> album | AdaMEL-base <br> AdaMEL-HYB | $0.7163 \\pm 0.0048$ <br> $0.7504 \\pm 0.0059$ | $0.5520 \\pm 0.0044$ <br> $0.5879 \\pm 0.0028$ | $\\mathbf{0 . 7 2 0 4} \\pm \\mathbf{0 . 0 0 3 3}$ <br> $\\mathbf{0 . 7 8 3 3} \\pm \\mathbf{0 . 0 0 3 1}$ |\n\nor schema difference. The reason is likely due to the simplicity of ADAMEL architecture, as it aims to learn the data-source-level feature importance instead of improving the token-level embeddings as DeepMatcher or its variants. In the real-world knowledge integration process where data distributions are highly heterogeneous, transferring these token-level contextualized embeddings brings extra computation and does not always generalize well, as shown in Section 5.2. Nevertheless, even though ADAMEL is designed to handle data challenges in practice ( $\\mathbf{C 1 - C 3}$ ), we observe that ADAMEL-HYB performs comparably as DeepMatcher with reduced model complexity, which shows its effectiveness of adaptation.\n\nTable 6: Entity linkage performance (F1) of DeepMatcher, AdaMELzero and -HYB on the benchmark datasets, single domain scenario. AdaMEL-HYB performs comparably as DeepMatcher.\n\n| Type | Datasets | Domain | DeepMatcher | AdaMEL-ZERO | AdaMEL-HYB |\n| :--: | :--: | :--: | :--: | :--: | :--: |\n| Structured | Amazon-Google <br> Beer | Software <br> Product | 69.3 <br> 78.8 | 60.2 <br> 78.6 | 65.1 <br> 82.8 |\n|  | DBLP-ACM <br> DBLP-Google | Citation <br> Citation | 98.4 <br> 94.7 | 98.7 <br> 93.1 | 98.9 <br> 93.5 |\n|  | Fodors-Zagats <br> iTunes-Amazon | Restaurant <br> Music | 100 <br> 91.2 | 90.0 <br> 91.2 | 99.8 <br> 98.7 |\n|  | Walmart-Amazon | Electronics | 71.9 | 57.8 | 66.7 |\n| Dirty | DBLP-ACM <br> DBLP-Google | Citation <br> Citation | 98.1 <br> 95.8 | 95.7 <br> 89.7 | 97.7 <br> 91.5 |\n|  | iTunes-Amazon | Music | 79.4 | 79.3 | 80.7 |\n|  | Walmart-Amazon | Electronics | 55.8 | 48.2 | 52.2 |\n",
        "title": [
            "## 5 EXPERIMENTS",
            "### 5.1 Experimental Setup",
            "### 5.2 Transfer Learning for MEL",
            "### 5.3 Effectiveness of Adaptation",
            "### 5.4 Attention Analysis",
            "### 5.5 Data Sources Analysis",
            "### 5.6 Effectiveness of Support Set",
            "### 5.7 Model Justification"
        ],
        "summary": "This section presents a comprehensive experimental evaluation of AdaMEL, addressing its effectiveness in multi-source entity linkage (MEL) under real-world data challenges such as missing values, new attributes, and distribution shifts. Using both public benchmark and proprietary datasets, the authors compare AdaMEL variants to state-of-the-art baselines, demonstrating that AdaMEL consistently outperforms competitors, especially in transfer learning scenarios involving heterogeneous and noisy data sources. The study further shows that AdaMEL\u2019s adaptive feature attention mechanism not only yields meaningful and interpretable feature importance scores but also enhances stability and efficiency as new data sources are incrementally integrated. Sensitivity analyses confirm that AdaMEL benefits from moderate-sized labeled support sets, and ablation studies highlight the value of combining shared and unique features; however, AdaMEL is less effective than some baselines in single-domain, well-labeled settings without data heterogeneity."
    }
}