{"datasets": [{"name": "amazon_google", "description": "Product dataset; structured dataset; from DeepMatcher, ER Benchmark, and Magellan data repository", "usage_count": 10, "recommended": true}, {"name": "walmart_amazon", "description": "Product dataset; structured dataset; from DeepMatcher, ER Benchmark, and Magellan data repository", "usage_count": 8, "recommended": true}, {"name": "dblp_acm", "description": "Citation dataset; structured dataset; from DeepMatcher, ER Benchmark, and Magellan data repository", "usage_count": 7, "recommended": true}, {"name": "abt_buy", "description": "Product dataset; textual dataset; from DeepMatcher, ER Benchmark, and Magellan data repository", "usage_count": 6, "recommended": false}], "metrics": [{"name": "f1", "description": "Harmonic mean of precision and recall.", "usage_count": 11, "recommended": true}, {"name": "recall", "description": "Proportion of ground truth matches that are correctly predicted.", "usage_count": 10, "recommended": true}, {"name": "precision", "description": "Proportion of predicted matches that are correct.", "usage_count": 7, "recommended": true}], "baselines": [{"name": "ditto", "description": "Supervised ER model; fine-tunes a pre-trained language model for sentence-pair classification.", "usage_count": 5, "recommended": true}, {"name": "dm+", "description": "DeepMatcher+, a reference DL-based EM approach that does not rely on a transformer architecture.", "usage_count": 2, "recommended": true}, {"name": "sentencebert", "description": "Finetunes TPLM and a SentenceBERT-like classifier on labeled data to obtain embeddings for similarity search; RoBERTa transformer used in this work", "usage_count": 2, "recommended": true}, {"name": "decision tree", "description": "Non-deep learning baseline from Magellan package.", "usage_count": 2, "recommended": false}]}